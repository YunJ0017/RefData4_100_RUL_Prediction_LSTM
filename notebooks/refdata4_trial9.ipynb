{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4731faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "[SEED 9819123] device=cuda\n",
      "[SEED 9819123] out=./Trial9\\seed_9819123\n",
      "==============================\n",
      "[SEED 9819123] [001/300] train_mse_norm=0.039145 | val_rmse_norm=0.183075 | val_mae_cycles=2469.684 | best_val_rmse_norm=0.183075\n",
      "[SEED 9819123] [010/300] train_mse_norm=0.022222 | val_rmse_norm=0.175944 | val_mae_cycles=2320.628 | best_val_rmse_norm=0.175944\n",
      "[SEED 9819123] [020/300] train_mse_norm=0.022063 | val_rmse_norm=0.177891 | val_mae_cycles=2375.640 | best_val_rmse_norm=0.175610\n",
      "[SEED 9819123] [030/300] train_mse_norm=0.022391 | val_rmse_norm=0.180574 | val_mae_cycles=2414.558 | best_val_rmse_norm=0.174620\n",
      "[SEED 9819123] [040/300] train_mse_norm=0.018988 | val_rmse_norm=0.172718 | val_mae_cycles=2394.593 | best_val_rmse_norm=0.171338\n",
      "[SEED 9819123] [050/300] train_mse_norm=0.017346 | val_rmse_norm=0.161976 | val_mae_cycles=2219.903 | best_val_rmse_norm=0.160002\n",
      "[SEED 9819123] [060/300] train_mse_norm=0.016897 | val_rmse_norm=0.162507 | val_mae_cycles=2232.026 | best_val_rmse_norm=0.160002\n",
      "[SEED 9819123] [070/300] train_mse_norm=0.016744 | val_rmse_norm=0.159809 | val_mae_cycles=2170.922 | best_val_rmse_norm=0.158299\n",
      "[SEED 9819123] [080/300] train_mse_norm=0.016557 | val_rmse_norm=0.161801 | val_mae_cycles=2246.980 | best_val_rmse_norm=0.158299\n",
      "[SEED 9819123] [090/300] train_mse_norm=0.016150 | val_rmse_norm=0.159763 | val_mae_cycles=2202.179 | best_val_rmse_norm=0.158299\n",
      "[SEED 9819123] Early stopping at epoch 93.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_51840\\1971476606.py:799: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 9819123] best_by_val_norm: TEST mae_cycles=1257.080 | rmse_cycles=1725.640 | rmse_norm=0.117662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_51840\\1971476606.py:799: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 9819123] last_epoch: TEST mae_cycles=1077.731 | rmse_cycles=1570.975 | rmse_norm=0.111232\n",
      "\n",
      "==============================\n",
      "[SEED 111] device=cuda\n",
      "[SEED 111] out=./Trial9\\seed_111\n",
      "==============================\n",
      "[SEED 111] [001/300] train_mse_norm=0.044484 | val_rmse_norm=0.143394 | val_mae_cycles=2566.213 | best_val_rmse_norm=0.143394\n",
      "[SEED 111] [010/300] train_mse_norm=0.023225 | val_rmse_norm=0.149974 | val_mae_cycles=2777.918 | best_val_rmse_norm=0.142034\n",
      "[SEED 111] [020/300] train_mse_norm=0.019778 | val_rmse_norm=0.140678 | val_mae_cycles=2676.454 | best_val_rmse_norm=0.140678\n",
      "[SEED 111] [030/300] train_mse_norm=0.017083 | val_rmse_norm=0.156172 | val_mae_cycles=2974.168 | best_val_rmse_norm=0.139458\n",
      "[SEED 111] [040/300] train_mse_norm=0.001749 | val_rmse_norm=0.167299 | val_mae_cycles=3244.137 | best_val_rmse_norm=0.139458\n",
      "[SEED 111] [050/300] train_mse_norm=0.000318 | val_rmse_norm=0.163323 | val_mae_cycles=3110.225 | best_val_rmse_norm=0.139458\n",
      "[SEED 111] Early stopping at epoch 58.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_51840\\1971476606.py:799: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 111] best_by_val_norm: TEST mae_cycles=1925.653 | rmse_cycles=3041.797 | rmse_norm=0.162593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_51840\\1971476606.py:799: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 111] last_epoch: TEST mae_cycles=2168.028 | rmse_cycles=3533.994 | rmse_norm=0.187885\n",
      "\n",
      "==============================\n",
      "[SEED 222] device=cuda\n",
      "[SEED 222] out=./Trial9\\seed_222\n",
      "==============================\n",
      "[SEED 222] [001/300] train_mse_norm=0.037178 | val_rmse_norm=0.160059 | val_mae_cycles=1744.797 | best_val_rmse_norm=0.160059\n",
      "[SEED 222] [010/300] train_mse_norm=0.023973 | val_rmse_norm=0.159513 | val_mae_cycles=1735.737 | best_val_rmse_norm=0.158481\n",
      "[SEED 222] [020/300] train_mse_norm=0.023518 | val_rmse_norm=0.157483 | val_mae_cycles=1707.516 | best_val_rmse_norm=0.157483\n",
      "[SEED 222] [030/300] train_mse_norm=0.019415 | val_rmse_norm=0.142038 | val_mae_cycles=1585.173 | best_val_rmse_norm=0.142038\n",
      "[SEED 222] [040/300] train_mse_norm=0.018422 | val_rmse_norm=0.142673 | val_mae_cycles=1634.983 | best_val_rmse_norm=0.141286\n",
      "[SEED 222] [050/300] train_mse_norm=0.018164 | val_rmse_norm=0.141276 | val_mae_cycles=1606.496 | best_val_rmse_norm=0.140070\n",
      "[SEED 222] [060/300] train_mse_norm=0.017369 | val_rmse_norm=0.147364 | val_mae_cycles=1624.150 | best_val_rmse_norm=0.140070\n",
      "[SEED 222] [070/300] train_mse_norm=0.014317 | val_rmse_norm=0.157630 | val_mae_cycles=1652.198 | best_val_rmse_norm=0.140070\n",
      "[SEED 222] Early stopping at epoch 75.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_51840\\1971476606.py:799: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 222] best_by_val_norm: TEST mae_cycles=2131.104 | rmse_cycles=3212.260 | rmse_norm=0.129446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_51840\\1971476606.py:799: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 222] last_epoch: TEST mae_cycles=2344.069 | rmse_cycles=3703.386 | rmse_norm=0.153732\n",
      "\n",
      "==============================\n",
      "[SEED 333] device=cuda\n",
      "[SEED 333] out=./Trial9\\seed_333\n",
      "==============================\n",
      "[SEED 333] [001/300] train_mse_norm=0.041296 | val_rmse_norm=0.142262 | val_mae_cycles=1627.567 | best_val_rmse_norm=0.142262\n",
      "[SEED 333] [010/300] train_mse_norm=0.024162 | val_rmse_norm=0.142741 | val_mae_cycles=1635.359 | best_val_rmse_norm=0.139307\n",
      "[SEED 333] [020/300] train_mse_norm=0.021632 | val_rmse_norm=0.133861 | val_mae_cycles=1612.461 | best_val_rmse_norm=0.133861\n",
      "[SEED 333] [030/300] train_mse_norm=0.018213 | val_rmse_norm=0.136089 | val_mae_cycles=1594.288 | best_val_rmse_norm=0.128544\n",
      "[SEED 333] [040/300] train_mse_norm=0.001447 | val_rmse_norm=0.162272 | val_mae_cycles=1894.978 | best_val_rmse_norm=0.128544\n",
      "[SEED 333] [050/300] train_mse_norm=0.000293 | val_rmse_norm=0.158967 | val_mae_cycles=1875.254 | best_val_rmse_norm=0.128544\n",
      "[SEED 333] Early stopping at epoch 52.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_51840\\1971476606.py:799: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 333] best_by_val_norm: TEST mae_cycles=1077.414 | rmse_cycles=1746.859 | rmse_norm=0.159305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_51840\\1971476606.py:799: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 333] last_epoch: TEST mae_cycles=1070.591 | rmse_cycles=1764.352 | rmse_norm=0.173049\n",
      "\n",
      "==============================\n",
      "[SEED 444] device=cuda\n",
      "[SEED 444] out=./Trial9\\seed_444\n",
      "==============================\n",
      "[SEED 444] [001/300] train_mse_norm=0.039324 | val_rmse_norm=0.158877 | val_mae_cycles=2096.673 | best_val_rmse_norm=0.158877\n",
      "[SEED 444] [010/300] train_mse_norm=0.022518 | val_rmse_norm=0.162219 | val_mae_cycles=2202.080 | best_val_rmse_norm=0.157826\n",
      "[SEED 444] [020/300] train_mse_norm=0.022384 | val_rmse_norm=0.158541 | val_mae_cycles=2135.598 | best_val_rmse_norm=0.157826\n",
      "[SEED 444] [030/300] train_mse_norm=0.019706 | val_rmse_norm=0.153929 | val_mae_cycles=2289.575 | best_val_rmse_norm=0.149548\n",
      "[SEED 444] [040/300] train_mse_norm=0.008857 | val_rmse_norm=0.164545 | val_mae_cycles=2328.662 | best_val_rmse_norm=0.144265\n",
      "[SEED 444] [050/300] train_mse_norm=0.000574 | val_rmse_norm=0.169271 | val_mae_cycles=2376.328 | best_val_rmse_norm=0.144265\n",
      "[SEED 444] [060/300] train_mse_norm=0.000196 | val_rmse_norm=0.168427 | val_mae_cycles=2365.095 | best_val_rmse_norm=0.144265\n",
      "[SEED 444] Early stopping at epoch 64.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_51840\\1971476606.py:799: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 444] best_by_val_norm: TEST mae_cycles=1519.285 | rmse_cycles=2174.082 | rmse_norm=0.142564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_51840\\1971476606.py:799: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 444] last_epoch: TEST mae_cycles=1709.934 | rmse_cycles=2539.247 | rmse_norm=0.168767\n",
      "=== WIN-RATE SUMMARY (TEST; lower is better) ===\n",
      "- test_mae_cycles: last wins=2, best wins=3, ties=0 | mean(last-best)=91.963584, std(last-best)=161.476346\n",
      "- test_rmse_cycles: last wins=1, best wins=4, ties=0 | mean(last-best)=242.263165, std(last-best)=263.657933\n",
      "- test_mae_norm: last wins=1, best wins=4, ties=0 | mean(last-best)=0.008583, std(last-best)=0.011102\n",
      "- test_rmse_norm: last wins=1, best wins=4, ties=0 | mean(last-best)=0.016619, std(last-best)=0.012373\n",
      "\n",
      "=== MEAN ± STD across seeds (TEST) ===\n",
      "                 test_mae_cycles             test_rmse_cycles              \\\n",
      "                            mean         std             mean         std   \n",
      "checkpoint                                                                  \n",
      "best_by_val_norm     1582.107133  442.641396      2380.127745  707.475984   \n",
      "last_epoch           1674.070716  594.555770      2622.390910  980.840022   \n",
      "\n",
      "                 test_mae_norm           test_rmse_norm            \n",
      "                          mean       std           mean       std  \n",
      "checkpoint                                                         \n",
      "best_by_val_norm      0.112219  0.013456       0.142314  0.019192  \n",
      "last_epoch            0.120801  0.022975       0.158933  0.029312  \n",
      "\n",
      "Saved:\n",
      " - ./Trial9\\summary_across_seeds.csv\n",
      " - ./Trial9\\win_rate_summary.csv\n",
      " - ./Trial9\\win_rate_summary.txt\n",
      "\n",
      "DONE. Check Trial9 folder:\n",
      " - per seed results: Trial9/seed_<seed>/...\n",
      " - figures (paper-style): seed_<seed>/<ckpt>/paper_figures/<split>/\n",
      " - cycle sequence mean CSV: <ckpt>/<split>_cycle_sequence_mean.csv\n",
      " - PH/α–λ metrics CSV: <ckpt>/<split>_prognostics_metrics_per_file.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0) Reproducibility\n",
    "# ============================================================\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Config (Trial9: + PH / α–λ / RA / convergence)\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    data_dir: str = r\"C:\\Users\\11HOME_AHCI\\Desktop\\PoF\\Winter 2026\\Ref Data4\\100\"\n",
    "    out_dir: str = r\"./Trial9\"\n",
    "\n",
    "    # seeds to sweep\n",
    "    seeds: Tuple[int, ...] = (9819123, 111, 222, 333, 444)\n",
    "\n",
    "    # sliding window\n",
    "    seq_len: int = 100\n",
    "    stride: int = 5\n",
    "    pred_horizon: int = 0\n",
    "\n",
    "    # split by FILE\n",
    "    train_ratio: float = 0.7\n",
    "    val_ratio: float = 0.2\n",
    "    test_ratio: float = 0.1\n",
    "\n",
    "    # training\n",
    "    batch_size: int = 512\n",
    "    epochs: int = 300\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 0.0\n",
    "    patience: int = 30\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # model\n",
    "    hidden_size: int = 512\n",
    "    num_layers: int = 2\n",
    "    dropout: float = 0.2\n",
    "\n",
    "    # output controls\n",
    "    save_figures: bool = True\n",
    "    max_files_to_plot: Optional[int] = None  # None=all\n",
    "    num_workers: int = 0\n",
    "\n",
    "    # ===========================\n",
    "    # Trial9: Evaluation settings\n",
    "    # ===========================\n",
    "    alpha: float = 0.20                # relative error bound\n",
    "    ph_consecutive_m: int = 5          # PH: need M consecutive points within alpha\n",
    "    rep_method: str = \"mean\"           # cycle representative (fixed: mean)\n",
    "\n",
    "    # λ grid\n",
    "    lambdas: Tuple[float, ...] = (0.2, 0.4, 0.6, 0.8)\n",
    "\n",
    "    # For paper-style α–λ figure: pick ONE λ to visualize\n",
    "    lambda_to_plot: float = 0.6\n",
    "\n",
    "    # numeric stability for relative error\n",
    "    eps_rul: float = 1e-8\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Data utils\n",
    "# ============================================================\n",
    "def list_csv_files(data_dir: str) -> List[Path]:\n",
    "    p = Path(data_dir)\n",
    "    files = sorted([f for f in p.glob(\"*.csv\") if f.is_file()])\n",
    "    if len(files) == 0:\n",
    "        raise FileNotFoundError(f\"No CSV files found in: {data_dir}\")\n",
    "    return files\n",
    "\n",
    "\n",
    "def read_one_csv(csv_path: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(f\"{csv_path.name}: expected at least 2 columns, got {df.shape[1]}\")\n",
    "    vce = df.iloc[:, 0].astype(np.float32).to_numpy()\n",
    "    rul = df.iloc[:, 1].astype(np.float32).to_numpy()\n",
    "\n",
    "    if len(vce) != len(rul):\n",
    "        raise ValueError(f\"{csv_path.name}: length mismatch vce={len(vce)}, rul={len(rul)}\")\n",
    "    if len(vce) < 5:\n",
    "        raise ValueError(f\"{csv_path.name}: too short sequence length={len(vce)}\")\n",
    "    return vce, rul\n",
    "\n",
    "\n",
    "def split_files(\n",
    "    files: List[Path],\n",
    "    train_ratio: float,\n",
    "    val_ratio: float,\n",
    "    test_ratio: float,\n",
    "    seed: int\n",
    ") -> Dict[str, List[Path]]:\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-9\n",
    "\n",
    "    rng = random.Random(seed)\n",
    "    files_shuffled = files[:]\n",
    "    rng.shuffle(files_shuffled)\n",
    "\n",
    "    n = len(files_shuffled)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_val = int(n * val_ratio)\n",
    "\n",
    "    train_files = files_shuffled[:n_train]\n",
    "    val_files = files_shuffled[n_train:n_train + n_val]\n",
    "    test_files = files_shuffled[n_train + n_val:]\n",
    "\n",
    "    return {\"train\": train_files, \"val\": val_files, \"test\": test_files}\n",
    "\n",
    "\n",
    "def compute_dvce(vce: np.ndarray) -> np.ndarray:\n",
    "    dv = np.zeros_like(vce, dtype=np.float32)\n",
    "    dv[1:] = vce[1:] - vce[:-1]\n",
    "    return dv\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Dataset\n",
    "# ============================================================\n",
    "class WindowedRULDatasetNorm2F(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_list: List[Path],\n",
    "        seq_len: int,\n",
    "        stride: int,\n",
    "        pred_horizon: int,\n",
    "        scaler_x: StandardScaler = None,\n",
    "        fit_scaler: bool = False,\n",
    "    ):\n",
    "        self.file_list = file_list\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.scaler_x = scaler_x if scaler_x is not None else StandardScaler()\n",
    "\n",
    "        # store: (name, X2(T,2), rul(T,), rul0)\n",
    "        self.series: List[Tuple[str, np.ndarray, np.ndarray, float]] = []\n",
    "        for fp in self.file_list:\n",
    "            vce, rul = read_one_csv(fp)\n",
    "            rul0 = float(rul[0])\n",
    "            if rul0 <= 0:\n",
    "                raise ValueError(f\"{fp.name}: RUL0 must be > 0, got {rul0}\")\n",
    "\n",
    "            dv = compute_dvce(vce)\n",
    "            x2 = np.stack([vce, dv], axis=1).astype(np.float32)  # (T,2)\n",
    "            self.series.append((fp.name, x2, rul.astype(np.float32), rul0))\n",
    "\n",
    "        if fit_scaler:\n",
    "            all_x = np.concatenate([x2 for _, x2, _, _ in self.series], axis=0)\n",
    "            self.scaler_x.fit(all_x)\n",
    "\n",
    "        # window index\n",
    "        self.index: List[Tuple[int, int]] = []\n",
    "        for fi, (_name, x2, _rul, _rul0) in enumerate(self.series):\n",
    "            T = x2.shape[0]\n",
    "            last_start = T - (seq_len + pred_horizon)\n",
    "            if last_start < 0:\n",
    "                continue\n",
    "            for s in range(0, last_start + 1, stride):\n",
    "                self.index.append((fi, s))\n",
    "\n",
    "        if len(self.index) == 0:\n",
    "            raise ValueError(\"No windows were created. Check seq_len/pred_horizon vs file lengths.\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fi, s = self.index[idx]\n",
    "        name, x2, rul, rul0 = self.series[fi]\n",
    "\n",
    "        x = x2[s:s + self.seq_len, :]\n",
    "        y_idx = s + self.seq_len - 1 + self.pred_horizon\n",
    "        y_cycles = float(rul[y_idx])\n",
    "        y_norm = np.array([y_cycles / rul0], dtype=np.float32)\n",
    "\n",
    "        x = self.scaler_x.transform(x).astype(np.float32)\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x),\n",
    "            torch.from_numpy(y_norm),\n",
    "            name,\n",
    "            torch.tensor(s, dtype=torch.long),\n",
    "            torch.tensor(y_cycles, dtype=torch.float32),\n",
    "            torch.tensor(rul0, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Model\n",
    "# ============================================================\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.head(last)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Basic Eval + Save window-level predictions\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def evaluate_basic(model, loader, device) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "\n",
    "    mae_norm_list, mse_norm_list = [], []\n",
    "    mae_cyc_list, mse_cyc_list = [], []\n",
    "\n",
    "    for x, y_norm, _name, _s, y_cycles, rul0 in loader:\n",
    "        x = x.to(device)\n",
    "        y_norm = y_norm.to(device)\n",
    "        y_cycles = y_cycles.to(device).view(-1, 1)\n",
    "        rul0 = rul0.to(device).view(-1, 1)\n",
    "\n",
    "        pred_norm = model(x)\n",
    "\n",
    "        err_norm = pred_norm - y_norm\n",
    "        mae_norm_list.append(torch.mean(torch.abs(err_norm)).item())\n",
    "        mse_norm_list.append(torch.mean(err_norm ** 2).item())\n",
    "\n",
    "        pred_cycles = pred_norm * rul0\n",
    "        err_cyc = pred_cycles - y_cycles\n",
    "        mae_cyc_list.append(torch.mean(torch.abs(err_cyc)).item())\n",
    "        mse_cyc_list.append(torch.mean(err_cyc ** 2).item())\n",
    "\n",
    "    return {\n",
    "        \"mae_norm\": float(np.mean(mae_norm_list)) if mae_norm_list else float(\"nan\"),\n",
    "        \"rmse_norm\": float(np.sqrt(np.mean(mse_norm_list))) if mse_norm_list else float(\"nan\"),\n",
    "        \"mae_cycles\": float(np.mean(mae_cyc_list)) if mae_cyc_list else float(\"nan\"),\n",
    "        \"rmse_cycles\": float(np.sqrt(np.mean(mse_cyc_list))) if mse_cyc_list else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_predictions_windows_csv(model, loader, device, out_csv: str, seq_len: int) -> None:\n",
    "    \"\"\"\n",
    "    window-level 상세 CSV 생성:\n",
    "    file, start_idx, cycle(=target index), RUL_true, RUL_pred 등\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for x, y_norm, name, s, y_cycles, rul0 in loader:\n",
    "        x = x.to(device)\n",
    "        y_norm = y_norm.to(device)\n",
    "        y_cycles = y_cycles.to(device).view(-1, 1)\n",
    "        rul0 = rul0.to(device).view(-1, 1)\n",
    "\n",
    "        pred_norm = model(x)\n",
    "        pred_cycles = pred_norm * rul0\n",
    "\n",
    "        pred_norm_np = pred_norm.cpu().numpy().reshape(-1)\n",
    "        y_norm_np = y_norm.cpu().numpy().reshape(-1)\n",
    "        pred_cyc_np = pred_cycles.cpu().numpy().reshape(-1)\n",
    "        y_cyc_np = y_cycles.cpu().numpy().reshape(-1)\n",
    "\n",
    "        rul0_np = rul0.cpu().numpy().reshape(-1)\n",
    "        s_np = s.cpu().numpy().reshape(-1)\n",
    "        name_list = list(name)\n",
    "\n",
    "        for i in range(len(pred_norm_np)):\n",
    "            rows.append({\n",
    "                \"file\": name_list[i],\n",
    "                \"start_idx\": int(s_np[i]),\n",
    "                \"cycle\": int(s_np[i] + (seq_len - 1)),  # window target index\n",
    "                \"rul0\": float(rul0_np[i]),\n",
    "                \"RUL_true\": float(y_cyc_np[i]),\n",
    "                \"RUL_pred\": float(pred_cyc_np[i]),\n",
    "                \"RUL_true_norm\": float(y_norm_np[i]),\n",
    "                \"RUL_pred_norm\": float(pred_norm_np[i]),\n",
    "            })\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) Window -> Cycle sequence (mean representative)\n",
    "# ============================================================\n",
    "def windows_to_cycle_sequence_mean(windows_csv: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    같은 file, 같은 cycle에 대해 여러 window 예측이 존재할 수 있으므로,\n",
    "    cycle 단위로 mean 대표값을 생성한다.\n",
    "    결과: file, cycle, RUL_true, RUL_pred, rul0\n",
    "    \"\"\"\n",
    "    dfw = pd.read_csv(windows_csv)\n",
    "    if dfw.empty:\n",
    "        raise ValueError(f\"Empty windows csv: {windows_csv}\")\n",
    "\n",
    "    g = dfw.groupby([\"file\", \"cycle\"], as_index=False).agg(\n",
    "        rul0=(\"rul0\", \"first\"),\n",
    "        RUL_true=(\"RUL_true\", \"mean\"),   # 방어적 mean\n",
    "        RUL_pred=(\"RUL_pred\", \"mean\"),   # 대표화: mean\n",
    "        n_windows=(\"RUL_pred\", \"count\"),\n",
    "    )\n",
    "    return g\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) Prognostics metrics: Relative Error / RA / CRA / PH / α–λ / Convergence\n",
    "# ============================================================\n",
    "def compute_metrics_for_one_file(\n",
    "    df_seq_one_file: pd.DataFrame,\n",
    "    seq_len: int,\n",
    "    alpha: float,\n",
    "    ph_consecutive_m: int,\n",
    "    lambdas: Tuple[float, ...],\n",
    "    eps_rul: float,\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    입력 df_seq_one_file columns: cycle, RUL_true, RUL_pred, rul0\n",
    "\n",
    "    - t_s = seq_len - 1\n",
    "    - t_e = last cycle index\n",
    "    - relative error = |true - pred| / max(|true|, eps_rul)\n",
    "    - RA(t) = 1 - relative_error\n",
    "    - CRA = mean(RA(t)) over [t_s, t_e]\n",
    "    - PH: t_s 이후 처음으로 rel_err<=alpha 가 M연속 만족되는 시작 cycle = t_PH_start\n",
    "    - λ mapping: target RUL_true = (1-λ)*RUL0, t_λ = argmin |RUL_true - target|\n",
    "    - α–λ performance: at t_λ, rel_err<=alpha ? (0/1)\n",
    "    \"\"\"\n",
    "    df = df_seq_one_file.sort_values(\"cycle\").reset_index(drop=True).copy()\n",
    "\n",
    "    t_s = seq_len - 1\n",
    "    last_cycle = int(df[\"cycle\"].max())\n",
    "    EOL_true = last_cycle + 1\n",
    "    t_e = EOL_true - 1  # == last_cycle\n",
    "\n",
    "    df_eval = df[(df[\"cycle\"] >= t_s) & (df[\"cycle\"] <= t_e)].copy()\n",
    "    df_eval.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if df_eval.empty:\n",
    "        summary = {\n",
    "            \"t_s\": t_s, \"t_e\": t_e, \"EOL_true\": EOL_true,\n",
    "            \"PH\": np.nan, \"t_PH_start\": np.nan,\n",
    "            \"CRA\": np.nan, \"Convergence_cycles\": np.nan,\n",
    "        }\n",
    "        for lam in lambdas:\n",
    "            summary[f\"t_lambda_{lam:.2f}\"] = np.nan\n",
    "            summary[f\"alpha_lambda_ok_{lam:.2f}\"] = np.nan\n",
    "        return df_eval, summary\n",
    "\n",
    "    denom = np.maximum(np.abs(df_eval[\"RUL_true\"].values), eps_rul)\n",
    "    rel_err = np.abs(df_eval[\"RUL_true\"].values - df_eval[\"RUL_pred\"].values) / denom\n",
    "    RA = 1.0 - rel_err\n",
    "\n",
    "    df_eval[\"rel_err\"] = rel_err\n",
    "    df_eval[\"RA\"] = RA\n",
    "    df_eval[\"in_alpha\"] = df_eval[\"rel_err\"] <= alpha\n",
    "\n",
    "    CRA = float(np.mean(df_eval[\"RA\"].values))\n",
    "\n",
    "    # PH start: M consecutive in_alpha\n",
    "    flags = df_eval[\"in_alpha\"].values.astype(np.int32)\n",
    "    t_PH_start = np.nan\n",
    "    if len(flags) >= ph_consecutive_m:\n",
    "        run = 0\n",
    "        for i, ok in enumerate(flags):\n",
    "            if ok:\n",
    "                run += 1\n",
    "                if run >= ph_consecutive_m:\n",
    "                    start_i = i - ph_consecutive_m + 1\n",
    "                    t_PH_start = int(df_eval.loc[start_i, \"cycle\"])\n",
    "                    break\n",
    "            else:\n",
    "                run = 0\n",
    "\n",
    "    if np.isfinite(t_PH_start):\n",
    "        PH = float(EOL_true - t_PH_start)\n",
    "        Convergence_cycles = float(t_PH_start - t_s)\n",
    "    else:\n",
    "        PH = np.nan\n",
    "        Convergence_cycles = np.nan\n",
    "\n",
    "    # α–λ\n",
    "    rul0 = float(df_eval[\"rul0\"].iloc[0])\n",
    "    lam_results = {}\n",
    "    for lam in lambdas:\n",
    "        target_rul = (1.0 - float(lam)) * rul0\n",
    "        idx = int(np.argmin(np.abs(df_eval[\"RUL_true\"].values - target_rul)))\n",
    "        t_lam = int(df_eval.loc[idx, \"cycle\"])\n",
    "        ok = bool(df_eval.loc[idx, \"rel_err\"] <= alpha)\n",
    "\n",
    "        lam_results[f\"t_lambda_{lam:.2f}\"] = t_lam\n",
    "        lam_results[f\"alpha_lambda_ok_{lam:.2f}\"] = int(ok)\n",
    "\n",
    "    summary = {\n",
    "        \"t_s\": int(t_s),\n",
    "        \"t_e\": int(t_e),\n",
    "        \"EOL_true\": int(EOL_true),\n",
    "        \"alpha\": float(alpha),\n",
    "        \"ph_consecutive_m\": int(ph_consecutive_m),\n",
    "        \"CRA\": CRA,\n",
    "        \"t_PH_start\": t_PH_start if np.isfinite(t_PH_start) else np.nan,\n",
    "        \"PH\": PH,\n",
    "        \"Convergence_cycles\": Convergence_cycles,\n",
    "        **lam_results\n",
    "    }\n",
    "    return df_eval, summary\n",
    "\n",
    "\n",
    "def compute_metrics_from_windows_csv(\n",
    "    windows_csv: str,\n",
    "    seq_len: int,\n",
    "    alpha: float,\n",
    "    ph_consecutive_m: int,\n",
    "    lambdas: Tuple[float, ...],\n",
    "    eps_rul: float,\n",
    "    out_dir: str,\n",
    "    split_name: str,\n",
    ") -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    windows_csv -> cycle sequence(mean) -> file별 metrics 산출\n",
    "    저장:\n",
    "      - <out_dir>/<split_name>_cycle_sequence_mean.csv\n",
    "      - <out_dir>/<split_name>_prognostics_metrics_per_file.csv\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    df_seq = windows_to_cycle_sequence_mean(windows_csv)\n",
    "    seq_path = os.path.join(out_dir, f\"{split_name}_cycle_sequence_mean.csv\")\n",
    "    df_seq.to_csv(seq_path, index=False)\n",
    "\n",
    "    rows = []\n",
    "    for f in df_seq[\"file\"].unique():\n",
    "        sub = df_seq[df_seq[\"file\"] == f].copy()\n",
    "        _df_eval, summary = compute_metrics_for_one_file(\n",
    "            df_seq_one_file=sub,\n",
    "            seq_len=seq_len,\n",
    "            alpha=alpha,\n",
    "            ph_consecutive_m=ph_consecutive_m,\n",
    "            lambdas=lambdas,\n",
    "            eps_rul=eps_rul,\n",
    "        )\n",
    "        summary[\"file\"] = f\n",
    "        rows.append(summary)\n",
    "\n",
    "    dfm = pd.DataFrame(rows)\n",
    "    metrics_path = os.path.join(out_dir, f\"{split_name}_prognostics_metrics_per_file.csv\")\n",
    "    dfm.to_csv(metrics_path, index=False)\n",
    "\n",
    "    return seq_path, metrics_path\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) Plotters (2 figures)\n",
    "# ============================================================\n",
    "def _safe_name(s: str) -> str:\n",
    "    return s.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\")\n",
    "\n",
    "\n",
    "def plot_alpha_ph(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    title: str,\n",
    "    alpha: float,\n",
    "    PH_start: Optional[float],\n",
    "    out_path: str,\n",
    "    dpi: int = 200,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Fig 1 (paper-style):\n",
    "      - True\n",
    "      - Prediction\n",
    "      - alpha accuracy zone (±alpha, relative)\n",
    "      - PH start (vertical line)\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    # ✅ FIX #1: always create a new figure (prevents occasional overlay/accumulation)\n",
    "    plt.figure()\n",
    "\n",
    "    x = df_eval[\"cycle\"].values\n",
    "    y_true = df_eval[\"RUL_true\"].values\n",
    "    y_pred = df_eval[\"RUL_pred\"].values\n",
    "\n",
    "    upper = y_true * (1.0 + alpha)\n",
    "    lower = y_true * (1.0 - alpha)\n",
    "\n",
    "    plt.plot(x, y_true, color=\"k\", label=\"True (cycles)\")              # 검정\n",
    "    plt.plot(x, y_pred, color=\"r\", label=\"Prediction (cycles)\")        # 빨강\n",
    "\n",
    "    plt.plot(x, upper, color=\"b\", linestyle=\"--\",\n",
    "             label=f\"+{alpha:.2f} alpha accuracy zone\")                # 파랑\n",
    "    plt.plot(x, lower, color=\"b\", linestyle=\"--\",\n",
    "             label=f\"-{alpha:.2f} alpha accuracy zone\")                # 파랑\n",
    "\n",
    "    if PH_start is not None and np.isfinite(PH_start):\n",
    "        plt.axvline(int(PH_start), color=\"g\", linestyle=\"-.\", label=\"PH start\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title}\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_alpha_lambda(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    title: str,\n",
    "    alpha: float,\n",
    "    lambda_to_plot: float,\n",
    "    t_lambda: Optional[int],\n",
    "    out_path: str,\n",
    "    dpi: int = 200,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Fig 2 (paper-style):\n",
    "      - True\n",
    "      - Prediction\n",
    "      - alpha-lambda zone: show ±alpha band ONLY for t >= t_lambda\n",
    "      - t_lambda vertical line (single)\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    x = df_eval[\"cycle\"].values\n",
    "    y_true = df_eval[\"RUL_true\"].values\n",
    "    y_pred = df_eval[\"RUL_pred\"].values\n",
    "\n",
    "    upper = y_true * (1.0 + alpha)\n",
    "    lower = y_true * (1.0 - alpha)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y_true, color=\"k\", label=\"True (cycles)\")              # 검정\n",
    "    plt.plot(x, y_pred, color=\"r\", label=\"Prediction (cycles)\")        # 빨강\n",
    "\n",
    "    if t_lambda is not None and np.isfinite(t_lambda):\n",
    "        t_lambda = int(t_lambda)\n",
    "        plt.axvline(t_lambda, linestyle=\":\", color=\"g\", label=f\"t_λ (λ={lambda_to_plot:.2f})\")\n",
    "\n",
    "        mask = x >= t_lambda\n",
    "        if np.any(mask):\n",
    "            plt.plot(x[mask], upper[mask], color=\"b\", linestyle=\"--\",\n",
    "                     label=f\"+{alpha:.2f} alpha–lambda accuracy zone\")         # 파랑\n",
    "            plt.plot(x[mask], lower[mask], color=\"b\", linestyle=\"--\",\n",
    "                     label=f\"-{alpha:.2f} alpha–lambda accuracy zone\")         # 파랑\n",
    "        else:\n",
    "            # ✅ FIX #2a: fallback colors fixed (keep zone blue even here)\n",
    "            plt.plot(x, upper, color=\"b\", linestyle=\"--\", label=f\"+{alpha:.2f} α zone\")\n",
    "            plt.plot(x, lower, color=\"b\", linestyle=\"--\", label=f\"-{alpha:.2f} α zone\")\n",
    "    else:\n",
    "        # ✅ FIX #2b: fallback colors fixed (keep zone blue even when t_lambda is missing)\n",
    "        plt.plot(x, upper, color=\"b\", linestyle=\"--\", label=f\"+{alpha:.2f} α zone\")\n",
    "        plt.plot(x, lower, color=\"b\", linestyle=\"--\", label=f\"-{alpha:.2f} α zone\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title}\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def make_paper_figures_for_split(\n",
    "    cycle_seq_csv: str,\n",
    "    metrics_per_file_csv: str,\n",
    "    out_fig_dir: str,\n",
    "    title_prefix: str,\n",
    "    alpha: float,\n",
    "    lambda_to_plot: float,\n",
    "    max_files: Optional[int] = None,\n",
    "    dpi: int = 200,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    For each file -> generate TWO figures:\n",
    "      1) alpha+PH\n",
    "      2) alpha-lambda (single lambda)\n",
    "    \"\"\"\n",
    "    df_seq = pd.read_csv(cycle_seq_csv)\n",
    "    dfm = pd.read_csv(metrics_per_file_csv)\n",
    "\n",
    "    files = df_seq[\"file\"].unique().tolist()\n",
    "    if max_files is not None:\n",
    "        files = files[:max_files]\n",
    "\n",
    "    os.makedirs(out_fig_dir, exist_ok=True)\n",
    "\n",
    "    lam_key = f\"t_lambda_{lambda_to_plot:.2f}\"\n",
    "\n",
    "    for f in files:\n",
    "        sub = df_seq[df_seq[\"file\"] == f].sort_values(\"cycle\").copy()\n",
    "        mrow = dfm[dfm[\"file\"] == f]\n",
    "        if mrow.empty:\n",
    "            continue\n",
    "        mrow = mrow.iloc[0].to_dict()\n",
    "\n",
    "        t_s = int(mrow[\"t_s\"])\n",
    "        t_e = int(mrow[\"t_e\"])\n",
    "        PH_start = mrow.get(\"t_PH_start\", np.nan)\n",
    "\n",
    "        df_eval = sub[(sub[\"cycle\"] >= t_s) & (sub[\"cycle\"] <= t_e)].copy()\n",
    "        if df_eval.empty:\n",
    "            continue\n",
    "\n",
    "        t_lambda = None\n",
    "        if lam_key in mrow and np.isfinite(mrow[lam_key]):\n",
    "            t_lambda = int(mrow[lam_key])\n",
    "\n",
    "        safe = _safe_name(f)\n",
    "\n",
    "        # 1) alpha + PH\n",
    "        out1 = os.path.join(out_fig_dir, f\"FIG1_alpha_PH__{safe}.png\")\n",
    "        plot_alpha_ph(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            title=f\"{title_prefix} | α+PH\",\n",
    "            alpha=alpha,\n",
    "            PH_start=PH_start if np.isfinite(PH_start) else None,\n",
    "            out_path=out1,\n",
    "            dpi=dpi,\n",
    "        )\n",
    "\n",
    "        # 2) alpha-lambda (single)\n",
    "        out2 = os.path.join(out_fig_dir, f\"FIG2_alpha_lambda__lam{lambda_to_plot:.2f}__{safe}.png\")\n",
    "        plot_alpha_lambda(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            title=f\"{title_prefix} | α–λ (λ={lambda_to_plot:.2f})\",\n",
    "            alpha=alpha,\n",
    "            lambda_to_plot=lambda_to_plot,\n",
    "            t_lambda=t_lambda,\n",
    "            out_path=out2,\n",
    "            dpi=dpi,\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) One seed run (train + export best/last + Trial9 metrics)\n",
    "# ============================================================\n",
    "def run_one_seed(cfg: Config, seed: int) -> Dict[str, Any]:\n",
    "    set_seed(seed)\n",
    "\n",
    "    seed_dir = os.path.join(cfg.out_dir, f\"seed_{seed}\")\n",
    "    os.makedirs(seed_dir, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"[SEED {seed}] device={device}\")\n",
    "    print(f\"[SEED {seed}] out={seed_dir}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    # split\n",
    "    files = list_csv_files(cfg.data_dir)\n",
    "    splits = split_files(files, cfg.train_ratio, cfg.val_ratio, cfg.test_ratio, seed)\n",
    "\n",
    "    # save split lists\n",
    "    for k in [\"train\", \"val\", \"test\"]:\n",
    "        pd.Series([p.name for p in splits[k]]).to_csv(\n",
    "            os.path.join(seed_dir, f\"{k}_files.csv\"), index=False, header=False\n",
    "        )\n",
    "\n",
    "    # datasets (fit scaler on train only)\n",
    "    scaler_x = StandardScaler()\n",
    "    train_ds = WindowedRULDatasetNorm2F(\n",
    "        splits[\"train\"], cfg.seq_len, cfg.stride, cfg.pred_horizon,\n",
    "        scaler_x=scaler_x, fit_scaler=True\n",
    "    )\n",
    "    val_ds = WindowedRULDatasetNorm2F(\n",
    "        splits[\"val\"], cfg.seq_len, cfg.stride, cfg.pred_horizon,\n",
    "        scaler_x=train_ds.scaler_x, fit_scaler=False\n",
    "    )\n",
    "    test_ds = WindowedRULDatasetNorm2F(\n",
    "        splits[\"test\"], cfg.seq_len, cfg.stride, cfg.pred_horizon,\n",
    "        scaler_x=train_ds.scaler_x, fit_scaler=False\n",
    "    )\n",
    "\n",
    "    pd.DataFrame({\n",
    "        \"feature\": [\"min_vce\", \"d_min_vce\"],\n",
    "        \"mean\": train_ds.scaler_x.mean_.ravel(),\n",
    "        \"std\": np.sqrt(train_ds.scaler_x.var_).ravel(),\n",
    "    }).to_csv(os.path.join(seed_dir, \"scaler_x_mean_std.csv\"), index=False)\n",
    "\n",
    "    # loaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "\n",
    "    train_eval = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "    val_eval = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "    test_eval = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "\n",
    "    # model\n",
    "    model = LSTMRegressor(\n",
    "        input_size=2,\n",
    "        hidden_size=cfg.hidden_size,\n",
    "        num_layers=cfg.num_layers,\n",
    "        dropout=cfg.dropout,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    best_by_val_norm = float(\"inf\")\n",
    "    best_path = os.path.join(seed_dir, \"best_by_val_norm.pt\")\n",
    "    last_path = os.path.join(seed_dir, \"last_epoch.pt\")\n",
    "\n",
    "    history: List[Dict[str, Any]] = []\n",
    "    bad_epochs = 0\n",
    "\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        model.train()\n",
    "        losses = []\n",
    "\n",
    "        for x, y_norm, *_ in train_loader:\n",
    "            x = x.to(device)\n",
    "            y_norm = y_norm.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred_norm = model(x)\n",
    "            loss = criterion(pred_norm, y_norm)\n",
    "            loss.backward()\n",
    "\n",
    "            if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        train_mse_norm = float(np.mean(losses)) if losses else float(\"nan\")\n",
    "        val_metrics = evaluate_basic(model, val_loader, device)\n",
    "\n",
    "        history.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_mse_norm\": train_mse_norm,\n",
    "            \"val_rmse_norm\": val_metrics[\"rmse_norm\"],\n",
    "            \"val_mae_norm\": val_metrics[\"mae_norm\"],\n",
    "            \"val_rmse_cycles\": val_metrics[\"rmse_cycles\"],\n",
    "            \"val_mae_cycles\": val_metrics[\"mae_cycles\"],\n",
    "        })\n",
    "\n",
    "        # best 기준: val_rmse_norm\n",
    "        if val_metrics[\"rmse_norm\"] < best_by_val_norm:\n",
    "            best_by_val_norm = val_metrics[\"rmse_norm\"]\n",
    "            bad_epochs = 0\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(\n",
    "                f\"[SEED {seed}] [{epoch:03d}/{cfg.epochs}] \"\n",
    "                f\"train_mse_norm={train_mse_norm:.6f} | \"\n",
    "                f\"val_rmse_norm={val_metrics['rmse_norm']:.6f} | \"\n",
    "                f\"val_mae_cycles={val_metrics['mae_cycles']:.3f} | \"\n",
    "                f\"best_val_rmse_norm={best_by_val_norm:.6f}\"\n",
    "            )\n",
    "\n",
    "        if bad_epochs >= cfg.patience:\n",
    "            print(f\"[SEED {seed}] Early stopping at epoch {epoch}.\")\n",
    "            break\n",
    "\n",
    "    pd.DataFrame(history).to_csv(os.path.join(seed_dir, \"history.csv\"), index=False)\n",
    "    torch.save(model.state_dict(), last_path)\n",
    "\n",
    "    # export helper\n",
    "    def export_ckpt(tag: str, ckpt_path: str) -> Dict[str, Any]:\n",
    "        sub_dir = os.path.join(seed_dir, tag)\n",
    "        os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "        model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "        model.eval()\n",
    "\n",
    "        tr = evaluate_basic(model, train_eval, device)\n",
    "        va = evaluate_basic(model, val_eval, device)\n",
    "        te = evaluate_basic(model, test_eval, device)\n",
    "\n",
    "        for split_name, loader in [(\"train\", train_eval), (\"val\", val_eval), (\"test\", test_eval)]:\n",
    "            win_csv = os.path.join(sub_dir, f\"{split_name}_predictions_windows.csv\")\n",
    "            save_predictions_windows_csv(model, loader, device, win_csv, seq_len=cfg.seq_len)\n",
    "\n",
    "            seq_csv, metrics_csv = compute_metrics_from_windows_csv(\n",
    "                windows_csv=win_csv,\n",
    "                seq_len=cfg.seq_len,\n",
    "                alpha=cfg.alpha,\n",
    "                ph_consecutive_m=cfg.ph_consecutive_m,\n",
    "                lambdas=cfg.lambdas,\n",
    "                eps_rul=cfg.eps_rul,\n",
    "                out_dir=sub_dir,\n",
    "                split_name=split_name,\n",
    "            )\n",
    "\n",
    "            if cfg.save_figures:\n",
    "                fig_dir = os.path.join(sub_dir, \"paper_figures\", split_name)\n",
    "                make_paper_figures_for_split(\n",
    "                    cycle_seq_csv=seq_csv,\n",
    "                    metrics_per_file_csv=metrics_csv,\n",
    "                    out_fig_dir=fig_dir,\n",
    "                    title_prefix=f\"SEED {seed} | {tag.upper()} | {split_name}\",\n",
    "                    alpha=cfg.alpha,\n",
    "                    lambda_to_plot=cfg.lambda_to_plot,\n",
    "                    max_files=cfg.max_files_to_plot,\n",
    "                )\n",
    "\n",
    "        ms = {\n",
    "            \"seed\": seed,\n",
    "            \"checkpoint\": tag,\n",
    "            \"train_rmse_cycles\": tr[\"rmse_cycles\"],\n",
    "            \"train_mae_cycles\": tr[\"mae_cycles\"],\n",
    "            \"train_rmse_norm\": tr[\"rmse_norm\"],\n",
    "            \"train_mae_norm\": tr[\"mae_norm\"],\n",
    "            \"val_rmse_cycles\": va[\"rmse_cycles\"],\n",
    "            \"val_mae_cycles\": va[\"mae_cycles\"],\n",
    "            \"val_rmse_norm\": va[\"rmse_norm\"],\n",
    "            \"val_mae_norm\": va[\"mae_norm\"],\n",
    "            \"test_rmse_cycles\": te[\"rmse_cycles\"],\n",
    "            \"test_mae_cycles\": te[\"mae_cycles\"],\n",
    "            \"test_rmse_norm\": te[\"rmse_norm\"],\n",
    "            \"test_mae_norm\": te[\"mae_norm\"],\n",
    "            \"stopped_epoch\": history[-1][\"epoch\"] if len(history) else None,\n",
    "            \"best_val_rmse_norm\": best_by_val_norm,\n",
    "            \"alpha\": cfg.alpha,\n",
    "            \"ph_consecutive_m\": cfg.ph_consecutive_m,\n",
    "            \"rep_method\": cfg.rep_method,\n",
    "            \"lambdas\": str(cfg.lambdas),\n",
    "            \"lambda_to_plot\": cfg.lambda_to_plot,\n",
    "        }\n",
    "        pd.DataFrame([ms]).to_csv(os.path.join(sub_dir, \"metrics_summary.csv\"), index=False)\n",
    "\n",
    "        print(f\"[SEED {seed}] {tag}: TEST mae_cycles={te['mae_cycles']:.3f} | rmse_cycles={te['rmse_cycles']:.3f} | rmse_norm={te['rmse_norm']:.6f}\")\n",
    "        return ms\n",
    "\n",
    "    ms_best = export_ckpt(\"best_by_val_norm\", best_path)\n",
    "    ms_last = export_ckpt(\"last_epoch\", last_path)\n",
    "\n",
    "    return {\"seed\": seed, \"seed_dir\": seed_dir, \"best\": ms_best, \"last\": ms_last}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10) Seed sweep + global comparison\n",
    "# ============================================================\n",
    "def summarize_across_seeds(cfg: Config, results: List[Dict[str, Any]]) -> None:\n",
    "    rows = []\n",
    "    for r in results:\n",
    "        rows.append(r[\"best\"])\n",
    "        rows.append(r[\"last\"])\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(os.path.join(cfg.out_dir, \"summary_across_seeds.csv\"), index=False)\n",
    "\n",
    "    def _isfinite(x: Any) -> bool:\n",
    "        try:\n",
    "            return bool(np.isfinite(float(x)))\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def win_rate(metric: str) -> Dict[str, Any]:\n",
    "        wins_last = 0\n",
    "        wins_best = 0\n",
    "        ties = 0\n",
    "        diffs = []\n",
    "\n",
    "        for r in results:\n",
    "            b = r[\"best\"][metric]\n",
    "            l = r[\"last\"][metric]\n",
    "            if _isfinite(b) and _isfinite(l):\n",
    "                diffs.append(float(l) - float(b))\n",
    "                if float(l) < float(b):\n",
    "                    wins_last += 1\n",
    "                elif float(b) < float(l):\n",
    "                    wins_best += 1\n",
    "                else:\n",
    "                    ties += 1\n",
    "\n",
    "        return {\n",
    "            \"metric\": metric,\n",
    "            \"wins_last\": wins_last,\n",
    "            \"wins_best\": wins_best,\n",
    "            \"ties\": ties,\n",
    "            \"mean(last-best)\": float(np.mean(diffs)) if diffs else float(\"nan\"),\n",
    "            \"std(last-best)\": float(np.std(diffs)) if diffs else float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    metrics = [\"test_mae_cycles\", \"test_rmse_cycles\", \"test_mae_norm\", \"test_rmse_norm\"]\n",
    "    wr = [win_rate(m) for m in metrics]\n",
    "    pd.DataFrame(wr).to_csv(os.path.join(cfg.out_dir, \"win_rate_summary.csv\"), index=False)\n",
    "\n",
    "    lines = []\n",
    "    lines.append(\"=== WIN-RATE SUMMARY (TEST; lower is better) ===\")\n",
    "    for row in wr:\n",
    "        lines.append(\n",
    "            f\"- {row['metric']}: last wins={row['wins_last']}, best wins={row['wins_best']}, ties={row['ties']} | \"\n",
    "            f\"mean(last-best)={row['mean(last-best)']:.6f}, std(last-best)={row['std(last-best)']:.6f}\"\n",
    "        )\n",
    "\n",
    "    agg = df.groupby(\"checkpoint\")[metrics].agg([\"mean\", \"std\"])\n",
    "    lines.append(\"\\n=== MEAN ± STD across seeds (TEST) ===\")\n",
    "    lines.append(str(agg))\n",
    "\n",
    "    with open(os.path.join(cfg.out_dir, \"win_rate_summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nSaved:\")\n",
    "    print(\" -\", os.path.join(cfg.out_dir, \"summary_across_seeds.csv\"))\n",
    "    print(\" -\", os.path.join(cfg.out_dir, \"win_rate_summary.csv\"))\n",
    "    print(\" -\", os.path.join(cfg.out_dir, \"win_rate_summary.txt\"))\n",
    "\n",
    "\n",
    "def run_trial9_seed_sweep(cfg: Config) -> None:\n",
    "    os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "\n",
    "    results = []\n",
    "    for seed in cfg.seeds:\n",
    "        res = run_one_seed(cfg, seed)\n",
    "        results.append(res)\n",
    "\n",
    "    summarize_across_seeds(cfg, results)\n",
    "\n",
    "    print(\"\\nDONE. Check Trial9 folder:\")\n",
    "    print(\" - per seed results: Trial9/seed_<seed>/...\")\n",
    "    print(\" - figures (paper-style): seed_<seed>/<ckpt>/paper_figures/<split>/\")\n",
    "    print(\" - cycle sequence mean CSV: <ckpt>/<split>_cycle_sequence_mean.csv\")\n",
    "    print(\" - PH/α–λ metrics CSV: <ckpt>/<split>_prognostics_metrics_per_file.csv\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 11) Run\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = Config(\n",
    "        data_dir=r\"C:\\Users\\11HOME_AHCI\\Desktop\\PoF\\Winter 2026\\Ref Data4\\100\",\n",
    "        out_dir=r\"./Trial9\",\n",
    "\n",
    "        seeds=(9819123, 111, 222, 333, 444),\n",
    "\n",
    "        seq_len=100,\n",
    "        stride=5,\n",
    "        pred_horizon=0,\n",
    "\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.2,\n",
    "        test_ratio=0.1,\n",
    "\n",
    "        batch_size=512,\n",
    "        epochs=300,\n",
    "        lr=1e-3,\n",
    "        weight_decay=0.0,\n",
    "        patience=30,\n",
    "        grad_clip=1.0,\n",
    "\n",
    "        hidden_size=512,\n",
    "        num_layers=2,\n",
    "        dropout=0.2,\n",
    "\n",
    "        save_figures=True,\n",
    "        max_files_to_plot=None,\n",
    "        num_workers=0,\n",
    "\n",
    "        alpha=0.20,\n",
    "        ph_consecutive_m=5,\n",
    "        rep_method=\"mean\",\n",
    "        lambdas=(0.2, 0.4, 0.6, 0.8),\n",
    "\n",
    "        # pick ONE lambda to visualize in α–λ figure\n",
    "        lambda_to_plot=0.6,\n",
    "    )\n",
    "\n",
    "    run_trial9_seed_sweep(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb74354d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] DONE -> ./Trial9\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\\train\n",
      "[val] DONE -> ./Trial9\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\\val\n",
      "[test] DONE -> ./Trial9\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\\test\n",
      "\n",
      "ALL DONE.\n",
      "Saved under: ./Trial9\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================\n",
    "# USER CONFIG\n",
    "# ============================================================\n",
    "TRIAL_DIR = r\"./Trial9\"              # Trial9 폴더\n",
    "SEED = 333                           # seed 선택\n",
    "CKPT = \"best_by_val_norm\"            # \"best_by_val_norm\" or \"last_epoch\"\n",
    "\n",
    "SPLITS = [\"train\", \"val\", \"test\"]    # ✅ 여러 split 한 번에\n",
    "\n",
    "ALPHA = 0.20\n",
    "SEQ_LEN = 100                        # (참고) eval 구간에서 t_s=seq_len-1을 이미 metrics가 갖고 있어 직접 쓰진 않음\n",
    "LAMBDA_TO_PLOT = 0.60                # α–λ 그림에 표시할 λ\n",
    "\n",
    "MAX_FILES = None                     # None=모두, 아니면 예: 10\n",
    "\n",
    "# 저장 폴더 루트\n",
    "OUT_ROOT = os.path.join(TRIAL_DIR, f\"seed_{SEED}\", CKPT, \"paper_figures_bookstyle\")\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "def safe_name(s: str) -> str:\n",
    "    return s.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\")\n",
    "\n",
    "def load_cycle_seq_and_metrics(seed_dir: str, split: str):\n",
    "    \"\"\"\n",
    "    Trial9가 이미 만들어 둔 파일들:\n",
    "      - <split>_cycle_sequence_mean.csv\n",
    "      - <split>_prognostics_metrics_per_file.csv\n",
    "    \"\"\"\n",
    "    seq_csv = os.path.join(seed_dir, f\"{split}_cycle_sequence_mean.csv\")\n",
    "    met_csv = os.path.join(seed_dir, f\"{split}_prognostics_metrics_per_file.csv\")\n",
    "\n",
    "    if not os.path.exists(seq_csv):\n",
    "        raise FileNotFoundError(f\"Missing: {seq_csv}\")\n",
    "    if not os.path.exists(met_csv):\n",
    "        raise FileNotFoundError(f\"Missing: {met_csv}\")\n",
    "\n",
    "    df_seq = pd.read_csv(seq_csv)\n",
    "    df_met = pd.read_csv(met_csv)\n",
    "    return df_seq, df_met\n",
    "\n",
    "def get_eval_segment(df_one_file: pd.DataFrame, t_s: int, t_e: int) -> pd.DataFrame:\n",
    "    df = df_one_file.sort_values(\"cycle\").copy()\n",
    "    df = df[(df[\"cycle\"] >= t_s) & (df[\"cycle\"] <= t_e)].copy()\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "# ============================================================\n",
    "# Plotters\n",
    "# ============================================================\n",
    "def plot_ph_alpha_absolute_band(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    alpha: float,\n",
    "    out_path: str,\n",
    "    ph_start: Optional[float] = None,\n",
    "    title_prefix: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    BOOK Fig.2.9(a) 스타일: PH용 α-zone은 '절대 폭(평행 밴드)'\n",
    "      alphaZone = alpha * EOL_true\n",
    "      zone = RUL_true ± alphaZone\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    x = df_eval[\"cycle\"].to_numpy()\n",
    "    y_true = df_eval[\"RUL_true\"].to_numpy()\n",
    "    y_pred = df_eval[\"RUL_pred\"].to_numpy()\n",
    "\n",
    "    last_cycle = int(df_eval[\"cycle\"].max())\n",
    "    eol_true = last_cycle + 1\n",
    "\n",
    "    alpha_zone = alpha * float(eol_true)  # ✅ book-style 핵심 (평행 밴드)\n",
    "    upper = y_true + alpha_zone\n",
    "    lower = y_true - alpha_zone\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y_true, \"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, \"r\", label=\"Prediction (cycles)\")\n",
    "    plt.plot(x, upper, \"b--\", label=f\"+{alpha:.2f} α-zone (±α·EOL)\")\n",
    "    plt.plot(x, lower, \"b--\", label=f\"-{alpha:.2f} α-zone (±α·EOL)\")\n",
    "\n",
    "    if ph_start is not None and np.isfinite(ph_start):\n",
    "        plt.axvline(int(ph_start), color=\"g\", linestyle=\"-.\", label=\"PH start\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title_prefix} | BOOK-STYLE α+PH (absolute band)\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_alpha_lambda_relative_band(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    alpha: float,\n",
    "    lambda_to_plot: float,\n",
    "    t_lambda: Optional[int],\n",
    "    out_path: str,\n",
    "    title_prefix: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    BOOK Fig.2.9(b) 스타일: α–λ는 '상대 폭(수렴 밴드)'\n",
    "      zone = RUL_true*(1±alpha), 그리고 t >= t_lambda 구간만 표시\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    x = df_eval[\"cycle\"].to_numpy()\n",
    "    y_true = df_eval[\"RUL_true\"].to_numpy()\n",
    "    y_pred = df_eval[\"RUL_pred\"].to_numpy()\n",
    "\n",
    "    upper = y_true * (1.0 + alpha)\n",
    "    lower = y_true * (1.0 - alpha)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y_true, \"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, \"r\", label=\"Prediction (cycles)\")\n",
    "\n",
    "    if t_lambda is not None and np.isfinite(t_lambda):\n",
    "        t_lambda = int(t_lambda)\n",
    "        plt.axvline(t_lambda, color=\"g\", linestyle=\":\", label=f\"t_λ (λ={lambda_to_plot:.2f})\")\n",
    "\n",
    "        mask = x >= t_lambda\n",
    "        if np.any(mask):\n",
    "            plt.plot(x[mask], upper[mask], \"b--\", label=f\"+{alpha:.2f} α–λ zone\")\n",
    "            plt.plot(x[mask], lower[mask], \"b--\", label=f\"-{alpha:.2f} α–λ zone\")\n",
    "        else:\n",
    "            plt.plot(x, upper, \"b--\", label=f\"+{alpha:.2f} α zone\")\n",
    "            plt.plot(x, lower, \"b--\", label=f\"-{alpha:.2f} α zone\")\n",
    "    else:\n",
    "        plt.plot(x, upper, \"b--\", label=f\"+{alpha:.2f} α zone\")\n",
    "        plt.plot(x, lower, \"b--\", label=f\"-{alpha:.2f} α zone\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title_prefix} | BOOK-STYLE α–λ (relative band)\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# Main (multi-split)\n",
    "# ============================================================\n",
    "def run_for_one_split(seed_dir: str, split: str):\n",
    "    df_seq, df_met = load_cycle_seq_and_metrics(seed_dir, split)\n",
    "\n",
    "    files = df_seq[\"file\"].unique().tolist()\n",
    "    if MAX_FILES is not None:\n",
    "        files = files[:MAX_FILES]\n",
    "\n",
    "    out_dir = os.path.join(OUT_ROOT, split)  # ✅ split별 폴더\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    lam_key = f\"t_lambda_{LAMBDA_TO_PLOT:.2f}\"\n",
    "    title_prefix = f\"SEED {SEED} | {CKPT.upper()} | {split}\"\n",
    "\n",
    "    for f in files:\n",
    "        sub = df_seq[df_seq[\"file\"] == f].copy()\n",
    "        mrow = df_met[df_met[\"file\"] == f]\n",
    "        if mrow.empty:\n",
    "            continue\n",
    "        mrow = mrow.iloc[0].to_dict()\n",
    "\n",
    "        t_s = int(mrow[\"t_s\"])\n",
    "        t_e = int(mrow[\"t_e\"])\n",
    "        ph_start = mrow.get(\"t_PH_start\", np.nan)\n",
    "        t_lambda = mrow.get(lam_key, np.nan)\n",
    "\n",
    "        df_eval = get_eval_segment(sub, t_s, t_e)\n",
    "        if df_eval.empty:\n",
    "            continue\n",
    "\n",
    "        sname = safe_name(f)\n",
    "\n",
    "        out_a = os.path.join(out_dir, f\"FIG_A_BOOKSTYLE_alpha_PH__{sname}.png\")\n",
    "        plot_ph_alpha_absolute_band(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            alpha=ALPHA,\n",
    "            out_path=out_a,\n",
    "            ph_start=ph_start if np.isfinite(ph_start) else None,\n",
    "            title_prefix=title_prefix,\n",
    "        )\n",
    "\n",
    "        out_b = os.path.join(out_dir, f\"FIG_B_BOOKSTYLE_alpha_lambda__lam{LAMBDA_TO_PLOT:.2f}__{sname}.png\")\n",
    "        plot_alpha_lambda_relative_band(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            alpha=ALPHA,\n",
    "            lambda_to_plot=LAMBDA_TO_PLOT,\n",
    "            t_lambda=int(t_lambda) if np.isfinite(t_lambda) else None,\n",
    "            out_path=out_b,\n",
    "            title_prefix=title_prefix,\n",
    "        )\n",
    "\n",
    "    print(f\"[{split}] DONE -> {out_dir}\")\n",
    "\n",
    "def main():\n",
    "    seed_dir = os.path.join(TRIAL_DIR, f\"seed_{SEED}\", CKPT)\n",
    "\n",
    "    for split in SPLITS:\n",
    "        run_for_one_split(seed_dir, split)\n",
    "\n",
    "    print(\"\\nALL DONE.\")\n",
    "    print(\"Saved under:\", OUT_ROOT)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29d5cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       seed        checkpoint  split  n_files  alpha_lambda_rate_0.20  \\\n",
      "0       111  best_by_val_norm  train       70                0.642857   \n",
      "1       111  best_by_val_norm    val       20                0.650000   \n",
      "2       111  best_by_val_norm   test       10                0.300000   \n",
      "3       111        last_epoch  train       70                1.000000   \n",
      "4       111        last_epoch    val       20                0.550000   \n",
      "5       111        last_epoch   test       10                0.500000   \n",
      "6       222  best_by_val_norm  train       70                0.828571   \n",
      "7       222  best_by_val_norm    val       20                0.600000   \n",
      "8       222  best_by_val_norm   test       10                1.000000   \n",
      "9       222        last_epoch  train       70                0.928571   \n",
      "10      222        last_epoch    val       20                0.500000   \n",
      "11      222        last_epoch   test       10                0.900000   \n",
      "12      333  best_by_val_norm  train       70                0.728571   \n",
      "13      333  best_by_val_norm    val       20                0.700000   \n",
      "14      333  best_by_val_norm   test       10                0.700000   \n",
      "15      333        last_epoch  train       70                1.000000   \n",
      "16      333        last_epoch    val       20                0.650000   \n",
      "17      333        last_epoch   test       10                0.700000   \n",
      "18      444  best_by_val_norm  train       70                0.771429   \n",
      "19      444  best_by_val_norm    val       20                0.850000   \n",
      "20      444  best_by_val_norm   test       10                1.000000   \n",
      "21      444        last_epoch  train       70                1.000000   \n",
      "22      444        last_epoch    val       20                0.750000   \n",
      "23      444        last_epoch   test       10                0.800000   \n",
      "24  9819123  best_by_val_norm  train       70                0.785714   \n",
      "25  9819123  best_by_val_norm    val       20                0.700000   \n",
      "26  9819123  best_by_val_norm   test       10                0.900000   \n",
      "27  9819123        last_epoch  train       70                0.657143   \n",
      "28  9819123        last_epoch    val       20                0.550000   \n",
      "29  9819123        last_epoch   test       10                0.800000   \n",
      "\n",
      "    alpha_lambda_rate_0.40  alpha_lambda_rate_0.60  alpha_lambda_rate_0.80  \n",
      "0                 0.671429                0.628571                0.342857  \n",
      "1                 0.550000                0.450000                0.200000  \n",
      "2                 0.600000                0.500000                0.400000  \n",
      "3                 1.000000                1.000000                1.000000  \n",
      "4                 0.700000                0.400000                0.250000  \n",
      "5                 0.700000                0.300000                0.200000  \n",
      "6                 0.642857                0.585714                0.342857  \n",
      "7                 0.600000                0.450000                0.350000  \n",
      "8                 0.700000                0.600000                0.300000  \n",
      "9                 0.714286                0.728571                0.400000  \n",
      "10                0.300000                0.650000                0.400000  \n",
      "11                0.600000                0.600000                0.300000  \n",
      "12                0.457143                0.557143                0.314286  \n",
      "13                0.550000                0.350000                0.200000  \n",
      "14                0.500000                0.700000                0.400000  \n",
      "15                1.000000                1.000000                0.971429  \n",
      "16                0.550000                0.400000                0.150000  \n",
      "17                0.600000                0.400000                0.300000  \n",
      "18                0.671429                0.557143                0.357143  \n",
      "19                0.600000                0.500000                0.450000  \n",
      "20                0.600000                0.400000                0.400000  \n",
      "21                1.000000                1.000000                1.000000  \n",
      "22                0.700000                0.400000                0.400000  \n",
      "23                0.600000                0.300000                0.200000  \n",
      "24                0.785714                0.600000                0.300000  \n",
      "25                0.450000                0.500000                0.300000  \n",
      "26                0.800000                0.300000                0.300000  \n",
      "27                0.728571                0.700000                0.485714  \n",
      "28                0.400000                0.500000                0.400000  \n",
      "29                0.800000                0.800000                0.400000  \n",
      "[SAVE] ./Trial9\\alpha_lambda_rates_per_seed.csv\n",
      "                       alpha_lambda_rate_0.20            \\\n",
      "                                         mean       std   \n",
      "checkpoint       split                                    \n",
      "best_by_val_norm test                0.780000  0.294958   \n",
      "                 train               0.751429  0.070421   \n",
      "                 val                 0.700000  0.093541   \n",
      "last_epoch       test                0.740000  0.151658   \n",
      "                 train               0.917143  0.148599   \n",
      "                 val                 0.600000  0.100000   \n",
      "\n",
      "                       alpha_lambda_rate_0.40            \\\n",
      "                                         mean       std   \n",
      "checkpoint       split                                    \n",
      "best_by_val_norm test                0.640000  0.114018   \n",
      "                 train               0.645714  0.118838   \n",
      "                 val                 0.550000  0.061237   \n",
      "last_epoch       test                0.660000  0.089443   \n",
      "                 train               0.888571  0.152663   \n",
      "                 val                 0.530000  0.178885   \n",
      "\n",
      "                       alpha_lambda_rate_0.60            \\\n",
      "                                         mean       std   \n",
      "checkpoint       split                                    \n",
      "best_by_val_norm test                0.500000  0.158114   \n",
      "                 train               0.585714  0.030305   \n",
      "                 val                 0.450000  0.061237   \n",
      "last_epoch       test                0.480000  0.216795   \n",
      "                 train               0.885714  0.156818   \n",
      "                 val                 0.470000  0.109545   \n",
      "\n",
      "                       alpha_lambda_rate_0.80            \n",
      "                                         mean       std  \n",
      "checkpoint       split                                   \n",
      "best_by_val_norm test                0.360000  0.054772  \n",
      "                 train               0.331429  0.023474  \n",
      "                 val                 0.300000  0.106066  \n",
      "last_epoch       test                0.280000  0.083666  \n",
      "                 train               0.771429  0.301696  \n",
      "                 val                 0.320000  0.115109  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "TRIAL_DIR = r\"./Trial9\"  # 또는 네 Trial9 경로\n",
    "CKPTS = [\"best_by_val_norm\", \"last_epoch\"]\n",
    "SPLITS = [\"train\", \"val\", \"test\"]\n",
    "LAM_STRS = [\"0.20\", \"0.40\", \"0.60\", \"0.80\"]\n",
    "\n",
    "rows = []\n",
    "\n",
    "seed_dirs = sorted([d for d in os.listdir(TRIAL_DIR) if d.startswith(\"seed_\")])\n",
    "for seed_dir in seed_dirs:\n",
    "    seed = seed_dir.replace(\"seed_\", \"\")\n",
    "    for ckpt in CKPTS:\n",
    "        for split in SPLITS:\n",
    "            mpath = os.path.join(TRIAL_DIR, seed_dir, ckpt, f\"{split}_prognostics_metrics_per_file.csv\")\n",
    "            if not os.path.exists(mpath):\n",
    "                continue\n",
    "\n",
    "            df = pd.read_csv(mpath)\n",
    "\n",
    "            out = {\"seed\": seed, \"checkpoint\": ckpt, \"split\": split, \"n_files\": len(df)}\n",
    "            for ls in LAM_STRS:\n",
    "                col = f\"alpha_lambda_ok_{ls}\"\n",
    "                out[f\"alpha_lambda_rate_{ls}\"] = float(df[col].mean()) if col in df.columns else np.nan\n",
    "            rows.append(out)\n",
    "\n",
    "df_rates = pd.DataFrame(rows)\n",
    "print(df_rates)\n",
    "\n",
    "out_csv1 = os.path.join(TRIAL_DIR, \"alpha_lambda_rates_per_seed.csv\")\n",
    "df_rates.to_csv(out_csv1, index=False)\n",
    "print(f\"[SAVE] {out_csv1}\")\n",
    "\n",
    "# (선택) seed 평균±표준편차로 요약\n",
    "rate_cols = [c for c in df_rates.columns if c.startswith(\"alpha_lambda_rate_\")]\n",
    "summary = df_rates.groupby([\"checkpoint\", \"split\"])[rate_cols].agg([\"mean\", \"std\"])\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95a6a482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ BEST MODEL (Trial9) ================\n",
      "[SELECTED BY VAL]  (recommended for model selection)\n",
      "  Seed             : 333\n",
      "  Checkpoint       : best_by_val_norm\n",
      "  VAL  RMSE (cyc)   : 2208.480\n",
      "  VAL  MAE  (cyc)   : 1512.734\n",
      "  VAL  RMSE (norm)  : 0.128544\n",
      "  VAL  MAE  (norm)  : 0.101163\n",
      "  TEST RMSE (cyc)   : 1746.859\n",
      "  TEST MAE  (cyc)   : 1077.414\n",
      "  TEST RMSE (norm)  : 0.159305\n",
      "  TEST MAE  (norm)  : 0.126422\n",
      "\n",
      "[SELECTED BY TEST] (for reporting only; not for tuning)\n",
      "  Seed             : 9819123\n",
      "  Checkpoint       : last_epoch\n",
      "  TEST RMSE (cyc)   : 1570.975\n",
      "  TEST MAE  (cyc)   : 1077.731\n",
      "  TEST RMSE (norm)  : 0.111232\n",
      "  TEST MAE  (norm)  : 0.084752\n",
      "  VAL  RMSE (cyc)   : 2992.851\n",
      "  VAL  MAE  (cyc)   : 2167.899\n",
      "  VAL  RMSE (norm)  : 0.159813\n",
      "  VAL  MAE  (norm)  : 0.131243\n",
      "\n",
      "---------------- WIN-RATE (last_epoch vs best_by_val_norm) ----------------\n",
      "- val_rmse_cycles: last wins=1, best wins=4, ties=0 | mean(last-best)=367.693952\n",
      "- test_rmse_cycles: last wins=1, best wins=4, ties=0 | mean(last-best)=242.263165\n",
      "- val_rmse_norm: last wins=0, best wins=5, ties=0 | mean(last-best)=0.021389\n",
      "- test_rmse_norm: last wins=1, best wins=4, ties=0 | mean(last-best)=0.016619\n",
      "=====================================================\n",
      "\n",
      "Saved -> ./Trial9\\BEST_MODEL_BY_VAL.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================\n",
    "# Trial9 paths\n",
    "# ============================\n",
    "TRIAL9_DIR = \"./Trial9\"\n",
    "SUMMARY_CSV = os.path.join(TRIAL9_DIR, \"summary_across_seeds.csv\")\n",
    "\n",
    "BEST_TAG = \"best_by_val_norm\"\n",
    "LAST_TAG = \"last_epoch\"\n",
    "\n",
    "\n",
    "def _require_cols(df: pd.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in summary CSV: {missing}\")\n",
    "\n",
    "\n",
    "def pick_best_row(df: pd.DataFrame, metric_prefix: str = \"val\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    metric_prefix: \"val\" or \"test\"\n",
    "    Sort rule:\n",
    "      1) <prefix>_rmse_cycles ascending\n",
    "      2) <prefix>_mae_cycles  ascending\n",
    "    \"\"\"\n",
    "    rmse_col = f\"{metric_prefix}_rmse_cycles\"\n",
    "    mae_col = f\"{metric_prefix}_mae_cycles\"\n",
    "    _require_cols(df, [\"seed\", \"checkpoint\", rmse_col, mae_col])\n",
    "\n",
    "    df_sorted = df.sort_values(\n",
    "        by=[rmse_col, mae_col],\n",
    "        ascending=[True, True]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return df_sorted.iloc[0]\n",
    "\n",
    "\n",
    "def win_rate(df: pd.DataFrame, metric: str) -> dict:\n",
    "    \"\"\"\n",
    "    Compare BEST_TAG vs LAST_TAG within each seed on the given metric (lower is better).\n",
    "    Returns wins for last, wins for best, ties, and mean(last-best).\n",
    "    \"\"\"\n",
    "    _require_cols(df, [\"seed\", \"checkpoint\", metric])\n",
    "\n",
    "    wins_last = 0\n",
    "    wins_best = 0\n",
    "    ties = 0\n",
    "    diffs = []\n",
    "\n",
    "    for seed, g in df.groupby(\"seed\"):\n",
    "        ckpts = set(g[\"checkpoint\"].astype(str).values)\n",
    "        if not ({BEST_TAG, LAST_TAG} <= ckpts):\n",
    "            continue\n",
    "\n",
    "        b = float(g.loc[g[\"checkpoint\"] == BEST_TAG, metric].iloc[0])\n",
    "        l = float(g.loc[g[\"checkpoint\"] == LAST_TAG, metric].iloc[0])\n",
    "\n",
    "        if np.isfinite(b) and np.isfinite(l):\n",
    "            diffs.append(l - b)  # negative => last better\n",
    "            if l < b:\n",
    "                wins_last += 1\n",
    "            elif b < l:\n",
    "                wins_best += 1\n",
    "            else:\n",
    "                ties += 1\n",
    "\n",
    "    return {\n",
    "        \"metric\": metric,\n",
    "        \"wins_last\": wins_last,\n",
    "        \"wins_best\": wins_best,\n",
    "        \"ties\": ties,\n",
    "        \"mean(last-best)\": float(np.mean(diffs)) if diffs else float(\"nan\"),\n",
    "        \"std(last-best)\": float(np.std(diffs)) if diffs else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(SUMMARY_CSV):\n",
    "        raise FileNotFoundError(f\"Not found: {SUMMARY_CSV}\")\n",
    "\n",
    "    df = pd.read_csv(SUMMARY_CSV)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) VAL 기준 best (권장)\n",
    "    # -----------------------------\n",
    "    best_val = pick_best_row(df, metric_prefix=\"val\")\n",
    "    best_val_seed = int(best_val[\"seed\"])\n",
    "    best_val_ckpt = str(best_val[\"checkpoint\"])\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) TEST 기준 best (참고)\n",
    "    # -----------------------------\n",
    "    best_test = pick_best_row(df, metric_prefix=\"test\")\n",
    "    best_test_seed = int(best_test[\"seed\"])\n",
    "    best_test_ckpt = str(best_test[\"checkpoint\"])\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) win-rate (seed별 last vs best 비교)\n",
    "    # -----------------------------\n",
    "    wr_val_rmse = win_rate(df, \"val_rmse_cycles\")\n",
    "    wr_test_rmse = win_rate(df, \"test_rmse_cycles\")\n",
    "\n",
    "    # Trial9에서는 norm 지표도 같이 보는 경우가 많아서 추가\n",
    "    wr_val_rmse_norm = win_rate(df, \"val_rmse_norm\")\n",
    "    wr_test_rmse_norm = win_rate(df, \"test_rmse_norm\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4) 출력\n",
    "    # -----------------------------\n",
    "    print(\"\\n================ BEST MODEL (Trial9) ================\")\n",
    "    print(\"[SELECTED BY VAL]  (recommended for model selection)\")\n",
    "    print(f\"  Seed             : {best_val_seed}\")\n",
    "    print(f\"  Checkpoint       : {best_val_ckpt}\")\n",
    "    print(f\"  VAL  RMSE (cyc)   : {best_val['val_rmse_cycles']:.3f}\")\n",
    "    print(f\"  VAL  MAE  (cyc)   : {best_val['val_mae_cycles']:.3f}\")\n",
    "    print(f\"  VAL  RMSE (norm)  : {best_val['val_rmse_norm']:.6f}\")\n",
    "    print(f\"  VAL  MAE  (norm)  : {best_val['val_mae_norm']:.6f}\")\n",
    "    print(f\"  TEST RMSE (cyc)   : {best_val['test_rmse_cycles']:.3f}\")\n",
    "    print(f\"  TEST MAE  (cyc)   : {best_val['test_mae_cycles']:.3f}\")\n",
    "    print(f\"  TEST RMSE (norm)  : {best_val['test_rmse_norm']:.6f}\")\n",
    "    print(f\"  TEST MAE  (norm)  : {best_val['test_mae_norm']:.6f}\")\n",
    "\n",
    "    print(\"\\n[SELECTED BY TEST] (for reporting only; not for tuning)\")\n",
    "    print(f\"  Seed             : {best_test_seed}\")\n",
    "    print(f\"  Checkpoint       : {best_test_ckpt}\")\n",
    "    print(f\"  TEST RMSE (cyc)   : {best_test['test_rmse_cycles']:.3f}\")\n",
    "    print(f\"  TEST MAE  (cyc)   : {best_test['test_mae_cycles']:.3f}\")\n",
    "    print(f\"  TEST RMSE (norm)  : {best_test['test_rmse_norm']:.6f}\")\n",
    "    print(f\"  TEST MAE  (norm)  : {best_test['test_mae_norm']:.6f}\")\n",
    "    print(f\"  VAL  RMSE (cyc)   : {best_test['val_rmse_cycles']:.3f}\")\n",
    "    print(f\"  VAL  MAE  (cyc)   : {best_test['val_mae_cycles']:.3f}\")\n",
    "    print(f\"  VAL  RMSE (norm)  : {best_test['val_rmse_norm']:.6f}\")\n",
    "    print(f\"  VAL  MAE  (norm)  : {best_test['val_mae_norm']:.6f}\")\n",
    "\n",
    "    print(\"\\n---------------- WIN-RATE (last_epoch vs best_by_val_norm) ----------------\")\n",
    "    print(f\"- {wr_val_rmse['metric']}: last wins={wr_val_rmse['wins_last']}, \"\n",
    "          f\"best wins={wr_val_rmse['wins_best']}, ties={wr_val_rmse['ties']} | \"\n",
    "          f\"mean(last-best)={wr_val_rmse['mean(last-best)']:.6f}\")\n",
    "    print(f\"- {wr_test_rmse['metric']}: last wins={wr_test_rmse['wins_last']}, \"\n",
    "          f\"best wins={wr_test_rmse['wins_best']}, ties={wr_test_rmse['ties']} | \"\n",
    "          f\"mean(last-best)={wr_test_rmse['mean(last-best)']:.6f}\")\n",
    "    print(f\"- {wr_val_rmse_norm['metric']}: last wins={wr_val_rmse_norm['wins_last']}, \"\n",
    "          f\"best wins={wr_val_rmse_norm['wins_best']}, ties={wr_val_rmse_norm['ties']} | \"\n",
    "          f\"mean(last-best)={wr_val_rmse_norm['mean(last-best)']:.6f}\")\n",
    "    print(f\"- {wr_test_rmse_norm['metric']}: last wins={wr_test_rmse_norm['wins_last']}, \"\n",
    "          f\"best wins={wr_test_rmse_norm['wins_best']}, ties={wr_test_rmse_norm['ties']} | \"\n",
    "          f\"mean(last-best)={wr_test_rmse_norm['mean(last-best)']:.6f}\")\n",
    "    print(\"=====================================================\\n\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5) 기록 저장 (VAL 기준 best)\n",
    "    # -----------------------------\n",
    "    out_txt = os.path.join(TRIAL9_DIR, \"BEST_MODEL_BY_VAL.txt\")\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"BEST MODEL (Trial9) - Selected by VAL\\n\")\n",
    "        f.write(f\"seed={best_val_seed}\\n\")\n",
    "        f.write(f\"checkpoint={best_val_ckpt}\\n\")\n",
    "        f.write(f\"val_rmse_cycles={best_val['val_rmse_cycles']}\\n\")\n",
    "        f.write(f\"val_mae_cycles={best_val['val_mae_cycles']}\\n\")\n",
    "        f.write(f\"val_rmse_norm={best_val['val_rmse_norm']}\\n\")\n",
    "        f.write(f\"val_mae_norm={best_val['val_mae_norm']}\\n\")\n",
    "        f.write(f\"test_rmse_cycles={best_val['test_rmse_cycles']}\\n\")\n",
    "        f.write(f\"test_mae_cycles={best_val['test_mae_cycles']}\\n\")\n",
    "        f.write(f\"test_rmse_norm={best_val['test_rmse_norm']}\\n\")\n",
    "        f.write(f\"test_mae_norm={best_val['test_mae_norm']}\\n\")\n",
    "\n",
    "    print(f\"Saved -> {out_txt}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfaea0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] train: λ=0.20:0.729, λ=0.40:0.457, λ=0.60:0.557, λ=0.80:0.314\n",
      "[OK] val: λ=0.20:0.700, λ=0.40:0.550, λ=0.60:0.350, λ=0.80:0.200\n",
      "[OK] test: λ=0.20:0.700, λ=0.40:0.500, λ=0.60:0.700, λ=0.80:0.400\n",
      "\n",
      "==================== DONE ====================\n",
      "Saved:\n",
      " - ./Trial9\\seed_333\\best_by_val_norm\\alpha_lambda_eval\\alpha_lambda_summary_seed333.csv\n",
      " - ./Trial9\\seed_333\\best_by_val_norm\\alpha_lambda_eval\\alpha_lambda_per_file_seed333.csv\n",
      "==============================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# USER CONFIG\n",
    "# =========================\n",
    "TRIAL_DIR = r\"./Trial9\"           # Trial9 루트 폴더\n",
    "SEED = 333                        # 선택된 seed\n",
    "CKPT = \"best_by_val_norm\"         # 선택된 checkpoint (\"best_by_val_norm\" or \"last_epoch\")\n",
    "SPLITS = [\"train\", \"val\", \"test\"] # 평가할 split\n",
    "LAM_STRS = [\"0.20\", \"0.40\", \"0.60\", \"0.80\"]\n",
    "\n",
    "# (선택) 후반을 더 중요하게 보고 싶으면 가중치 사용 (기본 None이면 단순 평균)\n",
    "# 예: λ=0.2,0.4,0.6,0.8 가중치 = 1,1,2,3\n",
    "LAMBDA_WEIGHTS = None  # 또는 {\"0.20\":1, \"0.40\":1, \"0.60\":2, \"0.80\":3}\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def _require_cols(df: pd.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "def compute_alpha_lambda_rates(dfm: pd.DataFrame, lam_strs, weights=None) -> dict:\n",
    "    \"\"\"\n",
    "    dfm: <split>_prognostics_metrics_per_file.csv\n",
    "    Returns:\n",
    "      - per-lambda success rate (mean of alpha_lambda_ok_{lam})\n",
    "      - overall mean rate (simple mean or weighted mean)\n",
    "    \"\"\"\n",
    "    rates = {}\n",
    "    ok_cols = []\n",
    "    for ls in lam_strs:\n",
    "        col = f\"alpha_lambda_ok_{ls}\"\n",
    "        if col in dfm.columns:\n",
    "            rates[f\"rate_{ls}\"] = float(dfm[col].mean())\n",
    "            ok_cols.append(col)\n",
    "        else:\n",
    "            rates[f\"rate_{ls}\"] = np.nan\n",
    "\n",
    "    # overall score\n",
    "    if weights is None:\n",
    "        # simple mean across available lambdas\n",
    "        vals = [rates[f\"rate_{ls}\"] for ls in lam_strs if np.isfinite(rates[f\"rate_{ls}\"])]\n",
    "        rates[\"rate_mean_all\"] = float(np.mean(vals)) if vals else np.nan\n",
    "    else:\n",
    "        # weighted mean\n",
    "        num = 0.0\n",
    "        den = 0.0\n",
    "        for ls in lam_strs:\n",
    "            v = rates[f\"rate_{ls}\"]\n",
    "            w = float(weights.get(ls, 0.0))\n",
    "            if np.isfinite(v) and w > 0:\n",
    "                num += w * v\n",
    "                den += w\n",
    "        rates[\"rate_weighted_all\"] = (num / den) if den > 0 else np.nan\n",
    "\n",
    "    return rates\n",
    "\n",
    "def main():\n",
    "    seed_dir = os.path.join(TRIAL_DIR, f\"seed_{SEED}\", CKPT)\n",
    "    if not os.path.isdir(seed_dir):\n",
    "        raise FileNotFoundError(f\"Not found: {seed_dir}\")\n",
    "\n",
    "    out_dir = os.path.join(seed_dir, \"alpha_lambda_eval\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    summary_rows = []\n",
    "    per_file_rows = []  # (선택) 파일별 ok 값도 모아서 저장\n",
    "\n",
    "    for split in SPLITS:\n",
    "        mpath = os.path.join(seed_dir, f\"{split}_prognostics_metrics_per_file.csv\")\n",
    "        if not os.path.exists(mpath):\n",
    "            print(f\"[SKIP] Missing: {mpath}\")\n",
    "            continue\n",
    "\n",
    "        dfm = pd.read_csv(mpath)\n",
    "\n",
    "        # 파일별 컬럼 존재 체크 (있으면 per-file도 저장 가능)\n",
    "        need_cols = [\"file\"] + [f\"alpha_lambda_ok_{ls}\" for ls in LAM_STRS if f\"alpha_lambda_ok_{ls}\" in dfm.columns]\n",
    "        _require_cols(dfm, [\"file\"])  # 최소 file은 있어야 함\n",
    "\n",
    "        # split 요약(성공률)\n",
    "        rates = compute_alpha_lambda_rates(dfm, LAM_STRS, weights=LAMBDA_WEIGHTS)\n",
    "        row = {\n",
    "            \"seed\": SEED,\n",
    "            \"checkpoint\": CKPT,\n",
    "            \"split\": split,\n",
    "            \"n_files\": int(len(dfm)),\n",
    "            **rates\n",
    "        }\n",
    "        summary_rows.append(row)\n",
    "\n",
    "        # (선택) 파일별 pass/fail 저장 (분석할 때 유용)\n",
    "        keep_cols = [\"file\"]\n",
    "        for ls in LAM_STRS:\n",
    "            c_ok = f\"alpha_lambda_ok_{ls}\"\n",
    "            c_tl = f\"t_lambda_{ls}\"\n",
    "            if c_ok in dfm.columns:\n",
    "                keep_cols.append(c_ok)\n",
    "            if c_tl in dfm.columns:\n",
    "                keep_cols.append(c_tl)\n",
    "        sub = dfm[keep_cols].copy()\n",
    "        sub.insert(0, \"split\", split)\n",
    "        sub.insert(0, \"checkpoint\", CKPT)\n",
    "        sub.insert(0, \"seed\", SEED)\n",
    "        per_file_rows.append(sub)\n",
    "\n",
    "        print(f\"[OK] {split}: \" +\n",
    "              \", \".join([f\"λ={ls}:{row.get('rate_'+ls, np.nan):.3f}\" for ls in LAM_STRS if np.isfinite(row.get('rate_'+ls, np.nan))]))\n",
    "\n",
    "    # 저장: split별 요약\n",
    "    df_summary = pd.DataFrame(summary_rows)\n",
    "    out_summary = os.path.join(out_dir, \"alpha_lambda_summary_seed333.csv\")\n",
    "    df_summary.to_csv(out_summary, index=False)\n",
    "\n",
    "    # 저장: 파일별 pass/fail (선택)\n",
    "    if per_file_rows:\n",
    "        df_pf = pd.concat(per_file_rows, axis=0, ignore_index=True)\n",
    "        out_pf = os.path.join(out_dir, \"alpha_lambda_per_file_seed333.csv\")\n",
    "        df_pf.to_csv(out_pf, index=False)\n",
    "    else:\n",
    "        out_pf = None\n",
    "\n",
    "    print(\"\\n==================== DONE ====================\")\n",
    "    print(\"Saved:\")\n",
    "    print(\" -\", out_summary)\n",
    "    if out_pf:\n",
    "        print(\" -\", out_pf)\n",
    "    print(\"==============================================\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5813b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] ./alpha_lambda_curve_seed333.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SUMMARY_CSV = r\"C:\\Users\\11HOME_AHCI\\Desktop\\PoF\\Winter 2026\\Ref Data4\\Trial9\\seed_333\\best_by_val_norm\\alpha_lambda_eval\\alpha_lambda_summary_seed333.csv\"\n",
    "OUT_PNG = r\"./alpha_lambda_curve_seed333.png\"\n",
    "\n",
    "LAM_STRS = [\"0.20\", \"0.40\", \"0.60\", \"0.80\"]\n",
    "LAM = [float(x) for x in LAM_STRS]\n",
    "SPLITS_ORDER = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "df = pd.read_csv(SUMMARY_CSV)\n",
    "\n",
    "plt.figure()\n",
    "for split in SPLITS_ORDER:\n",
    "    sub = df[df[\"split\"] == split]\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    row = sub.iloc[0]\n",
    "    rates = [float(row.get(f\"rate_{ls}\", np.nan)) for ls in LAM_STRS]\n",
    "    plt.plot(LAM, rates, marker=\"o\", label=split)\n",
    "\n",
    "plt.xticks(LAM, [f\"{x:.2f}\" for x in LAM])\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xlabel(\"λ (life fraction)\")\n",
    "plt.ylabel(\"α–λ success rate\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(OUT_PNG, dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"[SAVE] {OUT_PNG}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5667f7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE. saved figures -> ./Trial9/seed_333/best_by_val_norm/paper_figures/test_manual\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def _safe_name(s: str) -> str:\n",
    "    return s.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\")\n",
    "\n",
    "\n",
    "def plot_alpha_ph(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    title: str,\n",
    "    alpha: float,\n",
    "    PH_start: Optional[float],\n",
    "    out_path: str,\n",
    "    dpi: int = 200,\n",
    ") -> None:\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    plt.figure()\n",
    "\n",
    "    x = df_eval[\"cycle\"].values\n",
    "    y_true = df_eval[\"RUL_true\"].values\n",
    "    y_pred = df_eval[\"RUL_pred\"].values\n",
    "\n",
    "    upper = y_true * (1.0 + alpha)\n",
    "    lower = y_true * (1.0 - alpha)\n",
    "\n",
    "    plt.plot(x, y_true, color=\"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, color=\"r\", label=\"Prediction (cycles)\")\n",
    "    plt.plot(x, upper, color=\"b\", linestyle=\"--\", label=f\"+{alpha:.2f} alpha accuracy zone\")\n",
    "    plt.plot(x, lower, color=\"b\", linestyle=\"--\", label=f\"-{alpha:.2f} alpha accuracy zone\")\n",
    "\n",
    "    if PH_start is not None and np.isfinite(PH_start):\n",
    "        plt.axvline(int(PH_start), color=\"g\", linestyle=\"-.\", label=\"PH start\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title}\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_alpha_lambda(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    title: str,\n",
    "    alpha: float,\n",
    "    lambda_to_plot: float,\n",
    "    t_lambda: Optional[int],\n",
    "    out_path: str,\n",
    "    dpi: int = 200,\n",
    ") -> None:\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    x = df_eval[\"cycle\"].values\n",
    "    y_true = df_eval[\"RUL_true\"].values\n",
    "    y_pred = df_eval[\"RUL_pred\"].values\n",
    "    upper = y_true * (1.0 + alpha)\n",
    "    lower = y_true * (1.0 - alpha)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y_true, color=\"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, color=\"r\", label=\"Prediction (cycles)\")\n",
    "\n",
    "    if t_lambda is not None and np.isfinite(t_lambda):\n",
    "        t_lambda = int(t_lambda)\n",
    "        plt.axvline(t_lambda, linestyle=\":\", color=\"g\", label=f\"t_λ (λ={lambda_to_plot:.2f})\")\n",
    "        mask = x >= t_lambda\n",
    "        if np.any(mask):\n",
    "            plt.plot(x[mask], upper[mask], color=\"b\", linestyle=\"--\",\n",
    "                     label=f\"+{alpha:.2f} alpha–lambda accuracy zone\")\n",
    "            plt.plot(x[mask], lower[mask], color=\"b\", linestyle=\"--\",\n",
    "                     label=f\"-{alpha:.2f} alpha–lambda accuracy zone\")\n",
    "        else:\n",
    "            plt.plot(x, upper, color=\"b\", linestyle=\"--\", label=f\"+{alpha:.2f} α zone\")\n",
    "            plt.plot(x, lower, color=\"b\", linestyle=\"--\", label=f\"-{alpha:.2f} α zone\")\n",
    "    else:\n",
    "        plt.plot(x, upper, color=\"b\", linestyle=\"--\", label=f\"+{alpha:.2f} α zone\")\n",
    "        plt.plot(x, lower, color=\"b\", linestyle=\"--\", label=f\"-{alpha:.2f} α zone\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title}\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def make_paper_figures_for_split(\n",
    "    cycle_seq_csv: str,\n",
    "    metrics_per_file_csv: str,\n",
    "    out_fig_dir: str,\n",
    "    title_prefix: str,\n",
    "    alpha: float,\n",
    "    lambda_to_plot: float,\n",
    "    max_files: Optional[int] = None,\n",
    "    dpi: int = 200,\n",
    ") -> None:\n",
    "    df_seq = pd.read_csv(cycle_seq_csv)\n",
    "    dfm = pd.read_csv(metrics_per_file_csv)\n",
    "\n",
    "    files = df_seq[\"file\"].unique().tolist()\n",
    "    if max_files is not None:\n",
    "        files = files[:max_files]\n",
    "\n",
    "    os.makedirs(out_fig_dir, exist_ok=True)\n",
    "\n",
    "    lam_key = f\"t_lambda_{lambda_to_plot:.2f}\"\n",
    "\n",
    "    for f in files:\n",
    "        sub = df_seq[df_seq[\"file\"] == f].sort_values(\"cycle\").copy()\n",
    "        mrow = dfm[dfm[\"file\"] == f]\n",
    "        if mrow.empty:\n",
    "            print(f\"[WARN] metrics row missing for file={f}. skip.\")\n",
    "            continue\n",
    "        mrow = mrow.iloc[0].to_dict()\n",
    "\n",
    "        # evaluation interval\n",
    "        t_s = int(mrow.get(\"t_s\", sub[\"cycle\"].min()))\n",
    "        t_e = int(mrow.get(\"t_e\", sub[\"cycle\"].max()))\n",
    "        PH_start = mrow.get(\"t_PH_start\", np.nan)\n",
    "\n",
    "        df_eval = sub[(sub[\"cycle\"] >= t_s) & (sub[\"cycle\"] <= t_e)].copy()\n",
    "        if df_eval.empty:\n",
    "            print(f\"[WARN] empty df_eval for file={f}. skip.\")\n",
    "            continue\n",
    "\n",
    "        # t_lambda\n",
    "        t_lambda = None\n",
    "        if lam_key in mrow and np.isfinite(mrow[lam_key]):\n",
    "            t_lambda = int(mrow[lam_key])\n",
    "\n",
    "        safe = _safe_name(f)\n",
    "\n",
    "        out1 = os.path.join(out_fig_dir, f\"FIG1_alpha_PH__{safe}.png\")\n",
    "        plot_alpha_ph(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            title=f\"{title_prefix} | α+PH\",\n",
    "            alpha=alpha,\n",
    "            PH_start=PH_start if np.isfinite(PH_start) else None,\n",
    "            out_path=out1,\n",
    "            dpi=dpi,\n",
    "        )\n",
    "\n",
    "        out2 = os.path.join(out_fig_dir, f\"FIG2_alpha_lambda__lam{lambda_to_plot:.2f}__{safe}.png\")\n",
    "        plot_alpha_lambda(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            title=f\"{title_prefix} | α–λ (λ={lambda_to_plot:.2f})\",\n",
    "            alpha=alpha,\n",
    "            lambda_to_plot=lambda_to_plot,\n",
    "            t_lambda=t_lambda,\n",
    "            out_path=out2,\n",
    "            dpi=dpi,\n",
    "        )\n",
    "\n",
    "    print(f\"DONE. saved figures -> {out_fig_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ✅ 너 환경에 맞게 여기만 수정하면 끝\n",
    "    cycle_seq_csv = r\"./Trial9/seed_333/best_by_val_norm/test_cycle_sequence_mean.csv\"\n",
    "    metrics_csv   = r\"./Trial9/seed_333/best_by_val_norm/test_prognostics_metrics_per_file.csv\"\n",
    "    out_fig_dir   = r\"./Trial9/seed_333/best_by_val_norm/paper_figures/test_manual\"\n",
    "\n",
    "    make_paper_figures_for_split(\n",
    "        cycle_seq_csv=cycle_seq_csv,\n",
    "        metrics_per_file_csv=metrics_csv,\n",
    "        out_fig_dir=out_fig_dir,\n",
    "        title_prefix=\"SEED 333 | BEST | test\",\n",
    "        alpha=0.20,\n",
    "        lambda_to_plot=0.60,\n",
    "        max_files=1,   # 1개만 만들기 (원하면 None으로)\n",
    "        dpi=200,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a64f4f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting captum\n",
      "  Downloading captum-0.8.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from captum) (3.10.7)\n",
      "Collecting numpy<2.0 (from captum)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from captum) (25.0)\n",
      "Requirement already satisfied: torch>=1.10 in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from captum) (2.5.1+cu121)\n",
      "Collecting tqdm (from captum)\n",
      "  Downloading tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from torch>=1.10->captum) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from torch>=1.10->captum) (4.15.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from torch>=1.10->captum) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from torch>=1.10->captum) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from torch>=1.10->captum) (2025.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from torch>=1.10->captum) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from sympy==1.13.1->torch>=1.10->captum) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from jinja2->torch>=1.10->captum) (3.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from matplotlib->captum) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from matplotlib->captum) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from matplotlib->captum) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from matplotlib->captum) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from matplotlib->captum) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from matplotlib->captum) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from matplotlib->captum) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\11home_ahci\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages (from tqdm->captum) (0.4.6)\n",
      "Downloading captum-0.8.0-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 14.6 MB/s  0:00:00\n",
      "Downloading numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 2.1/15.8 MB 14.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 5.5/15.8 MB 15.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.7/15.8 MB 15.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.6/15.8 MB 16.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.6/15.8 MB 16.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 13.8 MB/s  0:00:01\n",
      "Downloading tqdm-4.67.3-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, numpy, captum\n",
      "\n",
      "  Attempting uninstall: numpy\n",
      "\n",
      "    Found existing installation: numpy 2.2.5\n",
      "\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "    Uninstalling numpy-2.2.5:\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "      Successfully uninstalled numpy-2.2.5\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   -------------------------- ------------- 2/3 [captum]\n",
      "   -------------------------- ------------- 2/3 [captum]\n",
      "   -------------------------- ------------- 2/3 [captum]\n",
      "   ---------------------------------------- 3/3 [captum]\n",
      "\n",
      "Successfully installed captum-0.8.0 numpy-1.26.4 tqdm-4.67.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install captum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898e5da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "OUT_DIR: ./Trial9\\seed_333\\best_by_val_norm\\xai_ig_global_vA\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_45064\\3727629794.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used windows: 4096\n",
      "[DONE] Saved XAI results to: ./Trial9\\seed_333\\best_by_val_norm\\xai_ig_global_vA\\test\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# XAI (Integrated Gradients) - Version A (RECOMMENDED)\n",
    "# Fix: cuDNN RNN backward can only be called in training mode\n",
    "#   -> switch model.train() temporarily + set LSTM dropout=0.0\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# !pip install captum\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# USER CONFIG (edit here only)\n",
    "# ----------------------------\n",
    "DATA_DIR   = r\"C:\\Users\\11HOME_AHCI\\Desktop\\PoF\\Winter 2026\\Ref Data4\\100\"\n",
    "TRIAL_DIR  = r\"./Trial9\"\n",
    "SEED       = 333\n",
    "\n",
    "CKPT_TAG   = \"best_by_val_norm\"     # \"best_by_val_norm\" or \"last_epoch\"\n",
    "SPLIT      = \"test\"                 # \"train\" / \"val\" / \"test\"\n",
    "\n",
    "SEQ_LEN      = 100\n",
    "STRIDE       = 5\n",
    "PRED_HORIZON = 0\n",
    "\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS  = 2\n",
    "DROPOUT     = 0.2\n",
    "\n",
    "BATCH_SIZE  = 256\n",
    "NUM_SAMPLES = 4000   # 2k~10k 추천\n",
    "BASELINE    = \"zero\" # standardized input이면 mean=0과 동일\n",
    "\n",
    "OUT_DIR = os.path.join(TRIAL_DIR, f\"seed_{SEED}\", CKPT_TAG, \"xai_ig_global_vA\", SPLIT)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def load_split_files(seed_dir: str, split: str) -> List[Path]:\n",
    "    fpath = os.path.join(seed_dir, f\"{split}_files.csv\")\n",
    "    if not os.path.exists(fpath):\n",
    "        raise FileNotFoundError(f\"Missing split list: {fpath}\")\n",
    "\n",
    "    names = pd.read_csv(fpath, header=None)[0].astype(str).tolist()\n",
    "    paths = [Path(DATA_DIR) / n for n in names]\n",
    "\n",
    "    missing = [p.name for p in paths if not p.exists()]\n",
    "    if missing:\n",
    "        raise FileNotFoundError(f\"Some split files are missing under DATA_DIR. Examples: {missing[:5]}\")\n",
    "    return paths\n",
    "\n",
    "\n",
    "def load_scaler_from_csv(seed_dir: str) -> StandardScaler:\n",
    "    path = os.path.join(seed_dir, \"scaler_x_mean_std.csv\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Missing scaler file: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.mean_ = df[\"mean\"].values.astype(np.float64)\n",
    "    std = df[\"std\"].values.astype(np.float64)\n",
    "    scaler.var_ = (std ** 2)\n",
    "    scaler.scale_ = std\n",
    "    scaler.n_features_in_ = len(df)\n",
    "    return scaler\n",
    "\n",
    "\n",
    "def read_one_csv(csv_path: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(f\"{csv_path.name}: expected >=2 cols, got {df.shape[1]}\")\n",
    "    vce = df.iloc[:, 0].astype(np.float32).to_numpy()\n",
    "    rul = df.iloc[:, 1].astype(np.float32).to_numpy()\n",
    "    if len(vce) != len(rul):\n",
    "        raise ValueError(f\"{csv_path.name}: length mismatch\")\n",
    "    return vce, rul\n",
    "\n",
    "\n",
    "def compute_dvce(vce: np.ndarray) -> np.ndarray:\n",
    "    dv = np.zeros_like(vce, dtype=np.float32)\n",
    "    dv[1:] = vce[1:] - vce[:-1]\n",
    "    return dv\n",
    "\n",
    "\n",
    "class WindowedRULDatasetNorm2F(Dataset):\n",
    "    def __init__(self, file_list: List[Path], seq_len: int, stride: int,\n",
    "                 pred_horizon: int, scaler_x: StandardScaler):\n",
    "        self.file_list = file_list\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.scaler_x = scaler_x\n",
    "\n",
    "        # series: (name, X2(T,2), RUL(T,), rul0)\n",
    "        self.series: List[Tuple[str, np.ndarray, np.ndarray, float]] = []\n",
    "        for fp in self.file_list:\n",
    "            vce, rul = read_one_csv(fp)\n",
    "            rul0 = float(rul[0])\n",
    "            if rul0 <= 0:\n",
    "                raise ValueError(f\"{fp.name}: RUL0 must be >0, got {rul0}\")\n",
    "\n",
    "            dv = compute_dvce(vce)\n",
    "            x2 = np.stack([vce, dv], axis=1).astype(np.float32)  # (T,2)\n",
    "            self.series.append((fp.name, x2, rul.astype(np.float32), rul0))\n",
    "\n",
    "        # window index\n",
    "        self.index: List[Tuple[int, int]] = []\n",
    "        for fi, (_name, x2, _rul, _rul0) in enumerate(self.series):\n",
    "            T = x2.shape[0]\n",
    "            last_start = T - (seq_len + pred_horizon)\n",
    "            if last_start < 0:\n",
    "                continue\n",
    "            for s in range(0, last_start + 1, stride):\n",
    "                self.index.append((fi, s))\n",
    "\n",
    "        if len(self.index) == 0:\n",
    "            raise ValueError(\"No windows created. Check seq_len/pred_horizon vs file lengths.\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fi, s = self.index[idx]\n",
    "        name, x2, rul, rul0 = self.series[fi]\n",
    "\n",
    "        x = x2[s:s + self.seq_len, :]\n",
    "        y_idx = s + self.seq_len - 1 + self.pred_horizon\n",
    "        y_cycles = float(rul[y_idx])\n",
    "        y_norm = np.array([y_cycles / rul0], dtype=np.float32)\n",
    "\n",
    "        x = self.scaler_x.transform(x).astype(np.float32)\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x),                     # (T,2)\n",
    "            torch.from_numpy(y_norm),                # (1,)\n",
    "            name,\n",
    "            torch.tensor(s, dtype=torch.long),\n",
    "            torch.tensor(y_cycles, dtype=torch.float32),\n",
    "            torch.tensor(rul0, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.head(last)\n",
    "\n",
    "\n",
    "def make_baseline(x: torch.Tensor, mode: str = \"zero\") -> torch.Tensor:\n",
    "    # input은 scaler 적용되어 \"표준화된 값\" → mean baseline = 0 이므로 zero가 가장 흔한 baseline\n",
    "    if mode == \"zero\":\n",
    "        return torch.zeros_like(x)\n",
    "    return torch.zeros_like(x)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Load artifacts\n",
    "# ----------------------------\n",
    "seed_dir = os.path.join(TRIAL_DIR, f\"seed_{SEED}\")\n",
    "ckpt_path = os.path.join(seed_dir, f\"{CKPT_TAG}.pt\")\n",
    "if not os.path.exists(ckpt_path):\n",
    "    raise FileNotFoundError(f\"Missing checkpoint: {ckpt_path}\")\n",
    "\n",
    "scaler_x = load_scaler_from_csv(seed_dir)\n",
    "file_list = load_split_files(seed_dir, SPLIT)\n",
    "\n",
    "ds = WindowedRULDatasetNorm2F(\n",
    "    file_list=file_list,\n",
    "    seq_len=SEQ_LEN,\n",
    "    stride=STRIDE,\n",
    "    pred_horizon=PRED_HORIZON,\n",
    "    scaler_x=scaler_x\n",
    ")\n",
    "loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "model = LSTMRegressor(\n",
    "    input_size=2, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, dropout=DROPOUT\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "# ----------------------------\n",
    "# ✅ FIX for cuDNN RNN backward + stable attribution\n",
    "# ----------------------------\n",
    "orig_dropout = float(model.lstm.dropout)\n",
    "model.train()                 # cuDNN RNN backward 허용\n",
    "model.lstm.dropout = 0.0      # attribution 안정화 (dropout OFF)\n",
    "\n",
    "# ----------------------------\n",
    "# Compute global attributions\n",
    "# ----------------------------\n",
    "feat_abs_sum = np.zeros((2,), dtype=np.float64)\n",
    "time_abs_sum = np.zeros((SEQ_LEN,), dtype=np.float64)\n",
    "tf_abs_sum   = np.zeros((SEQ_LEN, 2), dtype=np.float64)\n",
    "\n",
    "n_total = 0\n",
    "for batch in loader:\n",
    "    x, y_norm, name, s, y_cycles, rul0 = batch\n",
    "    x = x.to(device)\n",
    "\n",
    "    if n_total >= NUM_SAMPLES:\n",
    "        break\n",
    "\n",
    "    baseline = make_baseline(x, BASELINE).to(device)\n",
    "\n",
    "    # IG attribution: (B,T,2)\n",
    "    attr = ig.attribute(x, baselines=baseline)\n",
    "    attr = attr.detach().cpu().numpy()\n",
    "\n",
    "    abs_attr = np.abs(attr)\n",
    "    feat_abs_sum += abs_attr.sum(axis=(0, 1))\n",
    "    time_abs_sum += abs_attr.sum(axis=(0, 2))\n",
    "    tf_abs_sum   += abs_attr.sum(axis=0)\n",
    "\n",
    "    n_total += x.shape[0]\n",
    "\n",
    "print(\"used windows:\", n_total)\n",
    "\n",
    "# ----------------------------\n",
    "# Restore model state\n",
    "# ----------------------------\n",
    "model.lstm.dropout = orig_dropout\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------\n",
    "# Normalize + Save\n",
    "# ----------------------------\n",
    "feat_imp = feat_abs_sum / (feat_abs_sum.sum() + 1e-12)\n",
    "time_imp = time_abs_sum / (time_abs_sum.sum() + 1e-12)\n",
    "tf_imp   = tf_abs_sum   / (tf_abs_sum.sum()   + 1e-12)\n",
    "\n",
    "pd.DataFrame({\"feature\": [\"min_vce\", \"d_min_vce\"], \"importance\": feat_imp}).to_csv(\n",
    "    os.path.join(OUT_DIR, \"global_feature_importance.csv\"), index=False\n",
    ")\n",
    "pd.DataFrame({\"t_in_window\": np.arange(SEQ_LEN), \"importance\": time_imp}).to_csv(\n",
    "    os.path.join(OUT_DIR, \"global_time_importance.csv\"), index=False\n",
    ")\n",
    "\n",
    "rows = []\n",
    "for t in range(SEQ_LEN):\n",
    "    rows.append({\"t_in_window\": t, \"feature\": \"min_vce\",   \"importance\": tf_imp[t, 0]})\n",
    "    rows.append({\"t_in_window\": t, \"feature\": \"d_min_vce\", \"importance\": tf_imp[t, 1]})\n",
    "pd.DataFrame(rows).to_csv(os.path.join(OUT_DIR, \"global_time_feature_importance.csv\"), index=False)\n",
    "\n",
    "# plots\n",
    "plt.figure()\n",
    "plt.bar([\"min_vce\", \"d_min_vce\"], feat_imp)\n",
    "plt.ylabel(\"Normalized |IG attribution|\")\n",
    "plt.title(f\"Global Feature Importance (IG) | seed={SEED} | {CKPT_TAG} | {SPLIT}\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(OUT_DIR, \"global_feature_importance.png\"), dpi=200, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(SEQ_LEN), time_imp)\n",
    "plt.xlabel(\"t in window\")\n",
    "plt.ylabel(\"Normalized |IG attribution|\")\n",
    "plt.title(f\"Global Time Importance (IG) | seed={SEED} | {CKPT_TAG} | {SPLIT}\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(OUT_DIR, \"global_time_importance.png\"), dpi=200, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"[DONE] Saved XAI results to:\", OUT_DIR)\n",
    "\n",
    "# global feature importance.cvs: 모델이 주로 보는 물리량은 무엇인가?\n",
    "# global time imporatnce.csv: window 안에서 어느 시점을 가장 본다?\n",
    "# global time feature imporatnce.csv: 언제, 어떤 feature가 중요한가? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d26410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "OUT_DIR: ./Trial9\\seed_333\\best_by_val_norm\\xai_debug_pack\\test\n",
      "Loaded:\n",
      " - windows: ./Trial9\\seed_333\\best_by_val_norm\\test_predictions_windows.csv rows: 8621\n",
      " - metrics: ./Trial9\\seed_333\\best_by_val_norm\\test_prognostics_metrics_per_file.csv rows: 10\n",
      "Dataset windows: 8621 Lookup size: 8621\n",
      "\n",
      "[1] Error-conditioned: mode=quantile, threshold=3399.648, n=863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_45064\\2139996575.py:266: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2] PH-conditioned: good_files=6 bad_files=3\n",
      "\n",
      "[3] Lambda-conditioned: λ=0.60, band=±50, n=210\n",
      "\n",
      "[DONE] XAI Debug Pack saved to: ./Trial9\\seed_333\\best_by_val_norm\\xai_debug_pack\\test\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# XAI DEBUG PACK (Trial9)\n",
    "# 1) Error-conditioned Local XAI\n",
    "# 2) PH-conditioned Group XAI\n",
    "# 3) λ-conditioned Local XAI (λ=0.6)\n",
    "#\n",
    "# Outputs (under Trial9/seed_<seed>/<ckpt>/xai_debug_pack/<split>/):\n",
    "#  - 01_error_group_feature_time_importance.(csv/png)\n",
    "#  - 02_ph_good_vs_bad_feature_importance.(csv/png)  (+ time plots)\n",
    "#  - 03_lambda_local_feature_time_importance.(csv/png)\n",
    "#\n",
    "# Requirements:\n",
    "#   pip install captum\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# USER CONFIG\n",
    "# ----------------------------\n",
    "DATA_DIR   = r\"C:\\Users\\11HOME_AHCI\\Desktop\\PoF\\Winter 2026\\Ref Data4\\100\"\n",
    "TRIAL_DIR  = r\"./Trial9\"\n",
    "SEED       = 333\n",
    "CKPT_TAG   = \"best_by_val_norm\"   # \"best_by_val_norm\" or \"last_epoch\"\n",
    "SPLIT      = \"test\"               # \"train\"/\"val\"/\"test\"\n",
    "\n",
    "SEQ_LEN      = 100\n",
    "STRIDE       = 5\n",
    "PRED_HORIZON = 0\n",
    "\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS  = 2\n",
    "DROPOUT     = 0.2\n",
    "\n",
    "BATCH_SIZE_IG = 128               # IG는 메모리/시간 고려해서 더 작게 추천\n",
    "BASELINE = \"zero\"\n",
    "\n",
    "# 샘플 수 (각 분석마다 몇 개 window 쓸지)\n",
    "N_ERROR_SAMPLES  = 1000\n",
    "N_GROUP_SAMPLES  = 2000\n",
    "N_LAMBDA_SAMPLES = 1000\n",
    "\n",
    "# Error-conditioned 기준: absolute error threshold (cycles)\n",
    "# - 고정값으로 주면 쉬움. 또는 quantile 방식 사용 가능.\n",
    "ERROR_MODE = \"quantile\"   # \"quantile\" or \"fixed\"\n",
    "ERROR_Q = 0.90            # 상위 10% 오차를 \"error windows\"로\n",
    "ERROR_FIXED = 300.0       # (ERROR_MODE=\"fixed\"일 때)\n",
    "\n",
    "# PH group split 기준\n",
    "PH_GOOD_MAX_CONV_CYCLES = 200     # convergence_cycles <= 이 값이면 \"PH good\"\n",
    "# (없으면 자동으로 median split으로 fallback)\n",
    "\n",
    "# λ-conditioned 설정\n",
    "LAMBDA_TARGET = 0.60\n",
    "LAMBDA_WINDOW_BAND = 50           # t_lambda ± band 안쪽 windows만 사용\n",
    "\n",
    "OUT_DIR = os.path.join(\n",
    "    TRIAL_DIR, f\"seed_{SEED}\", CKPT_TAG, \"xai_debug_pack\", SPLIT\n",
    ")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helpers: load artifacts\n",
    "# ============================================================\n",
    "def load_scaler_from_csv(seed_dir: str) -> StandardScaler:\n",
    "    path = os.path.join(seed_dir, \"scaler_x_mean_std.csv\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Missing scaler file: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.mean_ = df[\"mean\"].values.astype(np.float64)\n",
    "    std = df[\"std\"].values.astype(np.float64)\n",
    "    scaler.var_ = (std ** 2)\n",
    "    scaler.scale_ = std\n",
    "    scaler.n_features_in_ = len(df)\n",
    "    return scaler\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Dataset (same behavior as training) + also return cycle index\n",
    "# ============================================================\n",
    "def read_one_csv(csv_path: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(f\"{csv_path.name}: expected >=2 cols\")\n",
    "    vce = df.iloc[:, 0].astype(np.float32).to_numpy()\n",
    "    rul = df.iloc[:, 1].astype(np.float32).to_numpy()\n",
    "    if len(vce) != len(rul):\n",
    "        raise ValueError(f\"{csv_path.name}: length mismatch\")\n",
    "    return vce, rul\n",
    "\n",
    "\n",
    "def compute_dvce(vce: np.ndarray) -> np.ndarray:\n",
    "    dv = np.zeros_like(vce, dtype=np.float32)\n",
    "    dv[1:] = vce[1:] - vce[:-1]\n",
    "    return dv\n",
    "\n",
    "\n",
    "class WindowedRULDatasetNorm2F_WithCycle(Dataset):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "      x(T,2), y_norm(1), file, start_idx, cycle_target, y_cycles, rul0\n",
    "    \"\"\"\n",
    "    def __init__(self, file_list: List[Path], seq_len: int, stride: int,\n",
    "                 pred_horizon: int, scaler_x: StandardScaler):\n",
    "        self.file_list = file_list\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.scaler_x = scaler_x\n",
    "\n",
    "        self.series: List[Tuple[str, np.ndarray, np.ndarray, float]] = []\n",
    "        for fp in self.file_list:\n",
    "            vce, rul = read_one_csv(fp)\n",
    "            rul0 = float(rul[0])\n",
    "            if rul0 <= 0:\n",
    "                raise ValueError(f\"{fp.name}: RUL0 must be >0\")\n",
    "            dv = compute_dvce(vce)\n",
    "            x2 = np.stack([vce, dv], axis=1).astype(np.float32)\n",
    "            self.series.append((fp.name, x2, rul.astype(np.float32), rul0))\n",
    "\n",
    "        self.index: List[Tuple[int, int]] = []\n",
    "        for fi, (_name, x2, _rul, _rul0) in enumerate(self.series):\n",
    "            T = x2.shape[0]\n",
    "            last_start = T - (seq_len + pred_horizon)\n",
    "            if last_start < 0:\n",
    "                continue\n",
    "            for s in range(0, last_start + 1, stride):\n",
    "                self.index.append((fi, s))\n",
    "\n",
    "        if len(self.index) == 0:\n",
    "            raise ValueError(\"No windows created. Check seq_len/pred_horizon vs file lengths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fi, s = self.index[idx]\n",
    "        name, x2, rul, rul0 = self.series[fi]\n",
    "        x = x2[s:s + self.seq_len, :]\n",
    "        y_idx = s + self.seq_len - 1 + self.pred_horizon\n",
    "        y_cycles = float(rul[y_idx])\n",
    "        y_norm = np.array([y_cycles / rul0], dtype=np.float32)\n",
    "        x = self.scaler_x.transform(x).astype(np.float32)\n",
    "        cycle_target = int(s + (self.seq_len - 1) + self.pred_horizon)\n",
    "        return (\n",
    "            torch.from_numpy(x),\n",
    "            torch.from_numpy(y_norm),\n",
    "            name,\n",
    "            torch.tensor(s, dtype=torch.long),\n",
    "            torch.tensor(cycle_target, dtype=torch.long),\n",
    "            torch.tensor(y_cycles, dtype=torch.float32),\n",
    "            torch.tensor(rul0, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Model\n",
    "# ============================================================\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.head(last)\n",
    "\n",
    "\n",
    "def make_baseline(x: torch.Tensor, mode: str = \"zero\") -> torch.Tensor:\n",
    "    return torch.zeros_like(x)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load saved CSVs produced by your Trial9 export (recommended)\n",
    "# - windows predictions: <seed>/<ckpt>/test_predictions_windows.csv\n",
    "# - metrics per file:    <seed>/<ckpt>/test_prognostics_metrics_per_file.csv\n",
    "# ============================================================\n",
    "seed_dir = os.path.join(TRIAL_DIR, f\"seed_{SEED}\")\n",
    "ckpt_path = os.path.join(seed_dir, f\"{CKPT_TAG}.pt\")\n",
    "if not os.path.exists(ckpt_path):\n",
    "    raise FileNotFoundError(f\"Missing checkpoint: {ckpt_path}\")\n",
    "\n",
    "sub_dir = os.path.join(seed_dir, CKPT_TAG)\n",
    "win_csv = os.path.join(sub_dir, f\"{SPLIT}_predictions_windows.csv\")\n",
    "met_csv = os.path.join(sub_dir, f\"{SPLIT}_prognostics_metrics_per_file.csv\")\n",
    "\n",
    "if not os.path.exists(win_csv):\n",
    "    raise FileNotFoundError(f\"Missing windows prediction CSV: {win_csv}\")\n",
    "if not os.path.exists(met_csv):\n",
    "    raise FileNotFoundError(f\"Missing metrics-per-file CSV: {met_csv}\")\n",
    "\n",
    "dfw = pd.read_csv(win_csv)\n",
    "dfm = pd.read_csv(met_csv)\n",
    "\n",
    "# absolute error per window (cycles)\n",
    "dfw[\"abs_err\"] = np.abs(dfw[\"RUL_pred\"].values - dfw[\"RUL_true\"].values)\n",
    "\n",
    "print(\"Loaded:\")\n",
    "print(\" - windows:\", win_csv, \"rows:\", len(dfw))\n",
    "print(\" - metrics:\", met_csv, \"rows:\", len(dfm))\n",
    "\n",
    "# ============================================================\n",
    "# Build dataset/loader (for sampling windows & getting x)\n",
    "# We must use the EXACT scaler from training.\n",
    "# ============================================================\n",
    "scaler_x = load_scaler_from_csv(seed_dir)\n",
    "\n",
    "# file list for this split = from saved split list\n",
    "split_list_path = os.path.join(seed_dir, f\"{SPLIT}_files.csv\")\n",
    "names = pd.read_csv(split_list_path, header=None)[0].astype(str).tolist()\n",
    "file_list = [Path(DATA_DIR) / n for n in names]\n",
    "\n",
    "ds = WindowedRULDatasetNorm2F_WithCycle(\n",
    "    file_list=file_list,\n",
    "    seq_len=SEQ_LEN,\n",
    "    stride=STRIDE,\n",
    "    pred_horizon=PRED_HORIZON,\n",
    "    scaler_x=scaler_x\n",
    ")\n",
    "\n",
    "# We'll NOT rely on DataLoader shuffle for targeted sampling.\n",
    "# We'll sample indices from ds by matching (file, start_idx).\n",
    "# Build fast lookup: (file, start_idx) -> ds_index\n",
    "lookup: Dict[Tuple[str, int], int] = {}\n",
    "for idx in range(len(ds)):\n",
    "    _x, _y, name, s, cyc, yc, r0 = ds[idx]\n",
    "    lookup[(name, int(s.item()))] = idx\n",
    "\n",
    "print(\"Dataset windows:\", len(ds), \"Lookup size:\", len(lookup))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load model + Captum IG (Version A fix)\n",
    "# ============================================================\n",
    "model = LSTMRegressor(2, HIDDEN_SIZE, NUM_LAYERS, DROPOUT).to(device)\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "# ✅ cuDNN RNN backward fix + stable attribution\n",
    "orig_dropout = float(model.lstm.dropout)\n",
    "model.train()\n",
    "model.lstm.dropout = 0.0\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Core: compute IG over a selected set of windows\n",
    "# windows_df must have columns: file, start_idx\n",
    "# ============================================================\n",
    "def compute_ig_for_windows(windows_df: pd.DataFrame, max_samples: int) -> Dict[str, np.ndarray]:\n",
    "    windows_df = windows_df.copy()\n",
    "\n",
    "    # keep only rows that exist in lookup\n",
    "    ok_mask = []\n",
    "    for _, r in windows_df.iterrows():\n",
    "        ok_mask.append((r[\"file\"], int(r[\"start_idx\"])) in lookup)\n",
    "    windows_df = windows_df[np.array(ok_mask, dtype=bool)]\n",
    "\n",
    "    if windows_df.empty:\n",
    "        raise ValueError(\"No matching windows found in dataset for IG.\")\n",
    "\n",
    "    # sample\n",
    "    if len(windows_df) > max_samples:\n",
    "        windows_df = windows_df.sample(n=max_samples, random_state=0)\n",
    "\n",
    "    # batch through\n",
    "    feat_abs_sum = np.zeros((2,), dtype=np.float64)\n",
    "    time_abs_sum = np.zeros((SEQ_LEN,), dtype=np.float64)\n",
    "    tf_abs_sum   = np.zeros((SEQ_LEN, 2), dtype=np.float64)\n",
    "\n",
    "    n = 0\n",
    "    batch_x = []\n",
    "    for _, r in windows_df.iterrows():\n",
    "        ds_idx = lookup[(r[\"file\"], int(r[\"start_idx\"]))]\n",
    "        x, *_ = ds[ds_idx]\n",
    "        batch_x.append(x.numpy())\n",
    "        n += 1\n",
    "\n",
    "        if len(batch_x) == BATCH_SIZE_IG:\n",
    "            xb = torch.tensor(np.stack(batch_x, axis=0), dtype=torch.float32).to(device)\n",
    "            baseline = make_baseline(xb, BASELINE).to(device)\n",
    "\n",
    "            attr = ig.attribute(xb, baselines=baseline).detach().cpu().numpy()\n",
    "            abs_attr = np.abs(attr)\n",
    "\n",
    "            feat_abs_sum += abs_attr.sum(axis=(0, 1))\n",
    "            time_abs_sum += abs_attr.sum(axis=(0, 2))\n",
    "            tf_abs_sum   += abs_attr.sum(axis=0)\n",
    "\n",
    "            batch_x = []\n",
    "\n",
    "    # remaining\n",
    "    if len(batch_x) > 0:\n",
    "        xb = torch.tensor(np.stack(batch_x, axis=0), dtype=torch.float32).to(device)\n",
    "        baseline = make_baseline(xb, BASELINE).to(device)\n",
    "        attr = ig.attribute(xb, baselines=baseline).detach().cpu().numpy()\n",
    "        abs_attr = np.abs(attr)\n",
    "\n",
    "        feat_abs_sum += abs_attr.sum(axis=(0, 1))\n",
    "        time_abs_sum += abs_attr.sum(axis=(0, 2))\n",
    "        tf_abs_sum   += abs_attr.sum(axis=0)\n",
    "\n",
    "    # normalize\n",
    "    feat_imp = feat_abs_sum / (feat_abs_sum.sum() + 1e-12)\n",
    "    time_imp = time_abs_sum / (time_abs_sum.sum() + 1e-12)\n",
    "    tf_imp   = tf_abs_sum   / (tf_abs_sum.sum()   + 1e-12)\n",
    "\n",
    "    return {\n",
    "        \"feat_imp\": feat_imp,\n",
    "        \"time_imp\": time_imp,\n",
    "        \"tf_imp\": tf_imp,\n",
    "        \"n_used\": np.array([min(len(windows_df), max_samples)], dtype=np.int64),\n",
    "        \"windows_df\": windows_df,\n",
    "    }\n",
    "\n",
    "\n",
    "def save_importance(prefix: str, feat_imp: np.ndarray, time_imp: np.ndarray) -> None:\n",
    "    # CSVs\n",
    "    pd.DataFrame({\"feature\": [\"min_vce\", \"d_min_vce\"], \"importance\": feat_imp}).to_csv(\n",
    "        os.path.join(OUT_DIR, f\"{prefix}_feature_importance.csv\"), index=False\n",
    "    )\n",
    "    pd.DataFrame({\"t_in_window\": np.arange(SEQ_LEN), \"importance\": time_imp}).to_csv(\n",
    "        os.path.join(OUT_DIR, f\"{prefix}_time_importance.csv\"), index=False\n",
    "    )\n",
    "\n",
    "    # PNGs\n",
    "    plt.figure()\n",
    "    plt.bar([\"min_vce\", \"d_min_vce\"], feat_imp)\n",
    "    plt.ylabel(\"Normalized |IG attribution|\")\n",
    "    plt.title(prefix)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(OUT_DIR, f\"{prefix}_feature_importance.png\"), dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(SEQ_LEN), time_imp)\n",
    "    plt.xlabel(\"t in window\")\n",
    "    plt.ylabel(\"Normalized |IG attribution|\")\n",
    "    plt.title(prefix)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(OUT_DIR, f\"{prefix}_time_importance.png\"), dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# (1) Error-conditioned Local XAI\n",
    "# ============================================================\n",
    "if ERROR_MODE == \"quantile\":\n",
    "    thr = float(dfw[\"abs_err\"].quantile(ERROR_Q))\n",
    "else:\n",
    "    thr = float(ERROR_FIXED)\n",
    "\n",
    "df_error = dfw[dfw[\"abs_err\"] >= thr].copy()\n",
    "print(f\"\\n[1] Error-conditioned: mode={ERROR_MODE}, threshold={thr:.3f}, n={len(df_error)}\")\n",
    "\n",
    "res_err = compute_ig_for_windows(df_error[[\"file\", \"start_idx\"]], max_samples=N_ERROR_SAMPLES)\n",
    "save_importance(\n",
    "    prefix=\"01_error_group_feature_time_importance\",\n",
    "    feat_imp=res_err[\"feat_imp\"],\n",
    "    time_imp=res_err[\"time_imp\"],\n",
    ")\n",
    "res_err[\"windows_df\"].to_csv(os.path.join(OUT_DIR, \"01_error_windows_used.csv\"), index=False)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# (2) PH-conditioned Group XAI (PH good vs bad)\n",
    "#   - uses Convergence_cycles or t_PH_start availability\n",
    "# ============================================================\n",
    "# clean numeric\n",
    "dfm2 = dfm.copy()\n",
    "for col in [\"Convergence_cycles\", \"t_PH_start\"]:\n",
    "    if col in dfm2.columns:\n",
    "        dfm2[col] = pd.to_numeric(dfm2[col], errors=\"coerce\")\n",
    "\n",
    "# Define groups\n",
    "if \"Convergence_cycles\" in dfm2.columns and dfm2[\"Convergence_cycles\"].notna().any():\n",
    "    conv = dfm2[\"Convergence_cycles\"].dropna()\n",
    "    if np.isfinite(PH_GOOD_MAX_CONV_CYCLES):\n",
    "        good_files = dfm2.loc[dfm2[\"Convergence_cycles\"] <= PH_GOOD_MAX_CONV_CYCLES, \"file\"].astype(str).tolist()\n",
    "        bad_files  = dfm2.loc[dfm2[\"Convergence_cycles\"] >  PH_GOOD_MAX_CONV_CYCLES, \"file\"].astype(str).tolist()\n",
    "\n",
    "        # fallback if too unbalanced\n",
    "        if len(good_files) < 3 or len(bad_files) < 3:\n",
    "            med = float(np.median(conv.values))\n",
    "            good_files = dfm2.loc[dfm2[\"Convergence_cycles\"] <= med, \"file\"].astype(str).tolist()\n",
    "            bad_files  = dfm2.loc[dfm2[\"Convergence_cycles\"] >  med, \"file\"].astype(str).tolist()\n",
    "    else:\n",
    "        med = float(np.median(conv.values))\n",
    "        good_files = dfm2.loc[dfm2[\"Convergence_cycles\"] <= med, \"file\"].astype(str).tolist()\n",
    "        bad_files  = dfm2.loc[dfm2[\"Convergence_cycles\"] >  med, \"file\"].astype(str).tolist()\n",
    "elif \"t_PH_start\" in dfm2.columns:\n",
    "    good_files = dfm2.loc[dfm2[\"t_PH_start\"].notna(), \"file\"].astype(str).tolist()\n",
    "    bad_files  = dfm2.loc[dfm2[\"t_PH_start\"].isna(), \"file\"].astype(str).tolist()\n",
    "else:\n",
    "    raise ValueError(\"Metrics CSV does not include Convergence_cycles or t_PH_start. Cannot form PH groups.\")\n",
    "\n",
    "print(f\"\\n[2] PH-conditioned: good_files={len(good_files)} bad_files={len(bad_files)}\")\n",
    "\n",
    "df_good = dfw[dfw[\"file\"].isin(good_files)].copy()\n",
    "df_bad  = dfw[dfw[\"file\"].isin(bad_files)].copy()\n",
    "\n",
    "# if empty fallback\n",
    "if df_good.empty or df_bad.empty:\n",
    "    raise ValueError(\"PH grouping produced empty windows. Check metrics CSV and file names.\")\n",
    "\n",
    "res_good = compute_ig_for_windows(df_good[[\"file\", \"start_idx\"]], max_samples=N_GROUP_SAMPLES)\n",
    "res_bad  = compute_ig_for_windows(df_bad[[\"file\", \"start_idx\"]],  max_samples=N_GROUP_SAMPLES)\n",
    "\n",
    "save_importance(\"02_ph_good_feature_time_importance\", res_good[\"feat_imp\"], res_good[\"time_imp\"])\n",
    "save_importance(\"02_ph_bad_feature_time_importance\",  res_bad[\"feat_imp\"],  res_bad[\"time_imp\"])\n",
    "\n",
    "pd.DataFrame({\"good_files\": good_files}).to_csv(os.path.join(OUT_DIR, \"02_ph_good_files.csv\"), index=False)\n",
    "pd.DataFrame({\"bad_files\": bad_files}).to_csv(os.path.join(OUT_DIR, \"02_ph_bad_files.csv\"), index=False)\n",
    "res_good[\"windows_df\"].to_csv(os.path.join(OUT_DIR, \"02_ph_good_windows_used.csv\"), index=False)\n",
    "res_bad[\"windows_df\"].to_csv(os.path.join(OUT_DIR, \"02_ph_bad_windows_used.csv\"), index=False)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# (3) λ-conditioned Local XAI (λ=0.60)\n",
    "#   - use cycle sequence mean CSV to get t_lambda for each file\n",
    "# ============================================================\n",
    "seq_csv = os.path.join(sub_dir, f\"{SPLIT}_cycle_sequence_mean.csv\")\n",
    "if not os.path.exists(seq_csv):\n",
    "    raise FileNotFoundError(f\"Missing cycle sequence mean CSV: {seq_csv}\")\n",
    "\n",
    "dfseq = pd.read_csv(seq_csv)\n",
    "dfseq = dfseq.sort_values([\"file\", \"cycle\"]).reset_index(drop=True)\n",
    "\n",
    "# compute t_lambda per file using true RUL and rul0\n",
    "tlam_map: Dict[str, int] = {}\n",
    "for f in dfseq[\"file\"].unique():\n",
    "    sub = dfseq[dfseq[\"file\"] == f]\n",
    "    rul0 = float(sub[\"rul0\"].iloc[0])\n",
    "    target = (1.0 - float(LAMBDA_TARGET)) * rul0\n",
    "    idx = int(np.argmin(np.abs(sub[\"RUL_true\"].values - target)))\n",
    "    t_lam = int(sub.iloc[idx][\"cycle\"])\n",
    "    tlam_map[str(f)] = t_lam\n",
    "\n",
    "dfw[\"t_lambda\"] = dfw[\"file\"].map(tlam_map).astype(\"float64\")\n",
    "dfw[\"dist_to_tlam\"] = np.abs(dfw[\"cycle\"].values - dfw[\"t_lambda\"].values)\n",
    "\n",
    "df_lam = dfw[dfw[\"dist_to_tlam\"] <= float(LAMBDA_WINDOW_BAND)].copy()\n",
    "print(f\"\\n[3] Lambda-conditioned: λ={LAMBDA_TARGET:.2f}, band=±{LAMBDA_WINDOW_BAND}, n={len(df_lam)}\")\n",
    "\n",
    "res_lam = compute_ig_for_windows(df_lam[[\"file\", \"start_idx\"]], max_samples=N_LAMBDA_SAMPLES)\n",
    "save_importance(\n",
    "    prefix=f\"03_lambda_{LAMBDA_TARGET:.2f}_feature_time_importance\",\n",
    "    feat_imp=res_lam[\"feat_imp\"],\n",
    "    time_imp=res_lam[\"time_imp\"],\n",
    ")\n",
    "res_lam[\"windows_df\"].to_csv(os.path.join(OUT_DIR, \"03_lambda_windows_used.csv\"), index=False)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Restore model state\n",
    "# ============================================================\n",
    "model.lstm.dropout = orig_dropout\n",
    "model.eval()\n",
    "\n",
    "print(\"\\n[DONE] XAI Debug Pack saved to:\", OUT_DIR)\n",
    "\n",
    "# 01 errir group *: feature importance (오차가 큰 window들에서 모델이 무엇에 의존하는지), time importance: 오차가 큰 window들에서 window 내 어떤 시점이 중요한지\n",
    "# 02 ph good vs bad *: feature importance + time importance (PH good/bad 그룹별 모델이 무엇에 의존하는지, window 내 어떤 시점이 중요한지)\n",
    "# 03 lambda 0.6 *: 0.6 구간에서 feature imporatnce, time imporatance, lambda_windows_used"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igbt_rnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
