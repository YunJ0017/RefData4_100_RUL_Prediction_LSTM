{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8abbb784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "[SEED 9819123] device=cuda\n",
      "[SEED 9819123] out=./Trial13\\seed_9819123\n",
      "==============================\n",
      "[SEED 9819123] [001/300] train_mse_norm=0.022341 | val_rmse_norm=0.164516 | val_mae_cycles=2300.540 | best_val_rmse_norm=0.164516\n",
      "[SEED 9819123] [010/300] train_mse_norm=0.016847 | val_rmse_norm=0.159369 | val_mae_cycles=2211.430 | best_val_rmse_norm=0.157482\n",
      "[SEED 9819123] [020/300] train_mse_norm=0.017272 | val_rmse_norm=0.161026 | val_mae_cycles=2248.656 | best_val_rmse_norm=0.157482\n",
      "[SEED 9819123] [030/300] train_mse_norm=0.016444 | val_rmse_norm=0.161414 | val_mae_cycles=2220.984 | best_val_rmse_norm=0.157482\n",
      "[SEED 9819123] [040/300] train_mse_norm=0.016667 | val_rmse_norm=0.159525 | val_mae_cycles=2201.029 | best_val_rmse_norm=0.157212\n",
      "[SEED 9819123] [050/300] train_mse_norm=0.015359 | val_rmse_norm=0.153406 | val_mae_cycles=2136.827 | best_val_rmse_norm=0.153406\n",
      "[SEED 9819123] [060/300] train_mse_norm=0.009020 | val_rmse_norm=0.166257 | val_mae_cycles=2148.549 | best_val_rmse_norm=0.150209\n",
      "[SEED 9819123] [070/300] train_mse_norm=0.000348 | val_rmse_norm=0.176025 | val_mae_cycles=2292.517 | best_val_rmse_norm=0.150209\n",
      "[SEED 9819123] [080/300] train_mse_norm=0.000128 | val_rmse_norm=0.177869 | val_mae_cycles=2335.959 | best_val_rmse_norm=0.150209\n",
      "[SEED 9819123] Early stopping at epoch 81.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_56060\\1836231154.py:895: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 9819123] best_by_val_norm: TEST mae_cycles=1019.842 | rmse_cycles=1484.817 | rmse_norm=0.106261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_56060\\1836231154.py:895: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 9819123] last_epoch: TEST mae_cycles=1278.791 | rmse_cycles=1920.577 | rmse_norm=0.130899\n",
      "\n",
      "==============================\n",
      "[SEED 111] device=cuda\n",
      "[SEED 111] out=./Trial13\\seed_111\n",
      "==============================\n",
      "[SEED 111] [001/300] train_mse_norm=0.034546 | val_rmse_norm=0.137336 | val_mae_cycles=2624.493 | best_val_rmse_norm=0.137336\n",
      "[SEED 111] [010/300] train_mse_norm=0.017678 | val_rmse_norm=0.143285 | val_mae_cycles=2807.525 | best_val_rmse_norm=0.137336\n",
      "[SEED 111] [020/300] train_mse_norm=0.017662 | val_rmse_norm=0.142925 | val_mae_cycles=2744.847 | best_val_rmse_norm=0.137336\n",
      "[SEED 111] [030/300] train_mse_norm=0.017908 | val_rmse_norm=0.142177 | val_mae_cycles=2721.150 | best_val_rmse_norm=0.137336\n",
      "[SEED 111] Early stopping at epoch 31.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_56060\\1836231154.py:895: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 111] best_by_val_norm: TEST mae_cycles=1902.274 | rmse_cycles=2939.737 | rmse_norm=0.152849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_56060\\1836231154.py:895: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 111] last_epoch: TEST mae_cycles=1677.506 | rmse_cycles=2609.431 | rmse_norm=0.138757\n",
      "\n",
      "==============================\n",
      "[SEED 222] device=cuda\n",
      "[SEED 222] out=./Trial13\\seed_222\n",
      "==============================\n",
      "[SEED 222] [001/300] train_mse_norm=0.022965 | val_rmse_norm=0.140745 | val_mae_cycles=1571.250 | best_val_rmse_norm=0.140745\n",
      "[SEED 222] [010/300] train_mse_norm=0.018385 | val_rmse_norm=0.139080 | val_mae_cycles=1574.665 | best_val_rmse_norm=0.136722\n",
      "[SEED 222] [020/300] train_mse_norm=0.017942 | val_rmse_norm=0.140924 | val_mae_cycles=1568.129 | best_val_rmse_norm=0.136722\n",
      "[SEED 222] [030/300] train_mse_norm=0.017880 | val_rmse_norm=0.144510 | val_mae_cycles=1576.052 | best_val_rmse_norm=0.136722\n",
      "[SEED 222] Early stopping at epoch 35.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_56060\\1836231154.py:895: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 222] best_by_val_norm: TEST mae_cycles=2107.318 | rmse_cycles=3144.453 | rmse_norm=0.129314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_56060\\1836231154.py:895: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 222] last_epoch: TEST mae_cycles=2070.613 | rmse_cycles=3097.660 | rmse_norm=0.123636\n",
      "\n",
      "==============================\n",
      "[SEED 333] device=cuda\n",
      "[SEED 333] out=./Trial13\\seed_333\n",
      "==============================\n",
      "[SEED 333] [001/300] train_mse_norm=0.023215 | val_rmse_norm=0.122945 | val_mae_cycles=1501.943 | best_val_rmse_norm=0.122945\n",
      "[SEED 333] [010/300] train_mse_norm=0.018842 | val_rmse_norm=0.126712 | val_mae_cycles=1514.045 | best_val_rmse_norm=0.122945\n",
      "[SEED 333] [020/300] train_mse_norm=0.018741 | val_rmse_norm=0.123560 | val_mae_cycles=1455.800 | best_val_rmse_norm=0.122585\n",
      "[SEED 333] [030/300] train_mse_norm=0.018771 | val_rmse_norm=0.125060 | val_mae_cycles=1475.115 | best_val_rmse_norm=0.122585\n",
      "[SEED 333] [040/300] train_mse_norm=0.018436 | val_rmse_norm=0.130513 | val_mae_cycles=1523.710 | best_val_rmse_norm=0.122585\n",
      "[SEED 333] Early stopping at epoch 41.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_56060\\1836231154.py:895: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 333] best_by_val_norm: TEST mae_cycles=971.670 | rmse_cycles=1556.094 | rmse_norm=0.148056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_56060\\1836231154.py:895: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 333] last_epoch: TEST mae_cycles=879.313 | rmse_cycles=1423.464 | rmse_norm=0.140472\n",
      "\n",
      "==============================\n",
      "[SEED 444] device=cuda\n",
      "[SEED 444] out=./Trial13\\seed_444\n",
      "==============================\n",
      "[SEED 444] [001/300] train_mse_norm=0.037533 | val_rmse_norm=0.143232 | val_mae_cycles=2083.208 | best_val_rmse_norm=0.143232\n",
      "[SEED 444] [010/300] train_mse_norm=0.017628 | val_rmse_norm=0.146131 | val_mae_cycles=2179.406 | best_val_rmse_norm=0.139889\n",
      "[SEED 444] [020/300] train_mse_norm=0.017418 | val_rmse_norm=0.141325 | val_mae_cycles=2107.547 | best_val_rmse_norm=0.139408\n",
      "[SEED 444] [030/300] train_mse_norm=0.017523 | val_rmse_norm=0.139733 | val_mae_cycles=2037.394 | best_val_rmse_norm=0.138958\n",
      "[SEED 444] [040/300] train_mse_norm=0.016365 | val_rmse_norm=0.133728 | val_mae_cycles=1934.304 | best_val_rmse_norm=0.133728\n",
      "[SEED 444] [050/300] train_mse_norm=0.010315 | val_rmse_norm=0.149364 | val_mae_cycles=2222.814 | best_val_rmse_norm=0.133281\n",
      "[SEED 444] [060/300] train_mse_norm=0.000404 | val_rmse_norm=0.158725 | val_mae_cycles=2265.072 | best_val_rmse_norm=0.133281\n",
      "[SEED 444] [070/300] train_mse_norm=0.000124 | val_rmse_norm=0.157626 | val_mae_cycles=2245.117 | best_val_rmse_norm=0.133281\n",
      "[SEED 444] Early stopping at epoch 72.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_56060\\1836231154.py:895: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 444] best_by_val_norm: TEST mae_cycles=1375.598 | rmse_cycles=1990.461 | rmse_norm=0.132163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_56060\\1836231154.py:895: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 444] last_epoch: TEST mae_cycles=1510.491 | rmse_cycles=2273.473 | rmse_norm=0.152725\n",
      "=== WIN-RATE SUMMARY (TEST; lower is better) ===\n",
      "- test_mae_cycles: last wins=3, best wins=2, ties=0 | mean(last-best)=8.002528, std(last-best)=170.486236\n",
      "- test_rmse_cycles: last wins=3, best wins=2, ties=0 | mean(last-best)=41.808385, std(last-best)=279.329603\n",
      "- test_mae_norm: last wins=3, best wins=2, ties=0 | mean(last-best)=0.000964, std(last-best)=0.011430\n",
      "- test_rmse_norm: last wins=3, best wins=2, ties=0 | mean(last-best)=0.003569, std(last-best)=0.015839\n",
      "\n",
      "=== MEAN ± STD across seeds (TEST) ===\n",
      "                 test_mae_cycles             test_rmse_cycles              \\\n",
      "                            mean         std             mean         std   \n",
      "checkpoint                                                                  \n",
      "best_by_val_norm     1475.340351  513.025292      2223.112415  775.651427   \n",
      "last_epoch           1483.342879  444.414190      2264.920801  640.085298   \n",
      "\n",
      "                 test_mae_norm           test_rmse_norm            \n",
      "                          mean       std           mean       std  \n",
      "checkpoint                                                         \n",
      "best_by_val_norm      0.103431  0.015628       0.133729  0.018352  \n",
      "last_epoch            0.104395  0.008534       0.137298  0.010929  \n",
      "\n",
      "Saved:\n",
      " - ./Trial13\\summary_across_seeds.csv\n",
      " - ./Trial13\\win_rate_summary.csv\n",
      " - ./Trial13\\win_rate_summary.txt\n",
      "\n",
      "DONE. Check Trial13 folder:\n",
      " - per seed results: Trial13/seed_<seed>/...\n",
      " - figures (paper-style): seed_<seed>/<ckpt>/paper_figures/<split>/\n",
      " - cycle sequence mean CSV: <ckpt>/<split>_cycle_sequence_mean.csv\n",
      " - PH/α–λ metrics CSV: <ckpt>/<split>_prognostics_metrics_per_file.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Trial13: min_vce-only + Multi-scale ΔVCE + (light) trend/stats features\n",
    "# - Input is STILL only derived from min_vce (no new sensors)\n",
    "# - Adds multi-scale deltas: v(t)-v(t-k), k in {1,5,20,50} (configurable)\n",
    "# - Adds EMA trend features + rolling std feature (configurable)\n",
    "# - Keeps Trial9 evaluation pack: PH / α–λ / CRA / convergence + paper figures\n",
    "#\n",
    "# Folder:\n",
    "#   ./Trial13/seed_<seed>/best_by_val_norm/...\n",
    "#   ./Trial13/seed_<seed>/last_epoch/...\n",
    "#\n",
    "# Notes:\n",
    "# - This is a \"drop-in\" replacement for Trial9 training/eval script.\n",
    "# - You can later add XAI pack on top of this exactly like Trial12 did.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0) Reproducibility\n",
    "# ============================================================\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Config\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    data_dir: str = r\"C:\\Users\\11HOME_AHCI\\Desktop\\PoF\\Winter 2026\\Ref Data4\\100\"\n",
    "    out_dir: str = r\"./Trial13\"\n",
    "\n",
    "    # seeds to sweep\n",
    "    seeds: Tuple[int, ...] = (9819123, 111, 222, 333, 444)\n",
    "\n",
    "    # sliding window\n",
    "    seq_len: int = 100\n",
    "    stride: int = 5\n",
    "    pred_horizon: int = 0\n",
    "\n",
    "    # split by FILE\n",
    "    train_ratio: float = 0.7\n",
    "    val_ratio: float = 0.2\n",
    "    test_ratio: float = 0.1\n",
    "\n",
    "    # training\n",
    "    batch_size: int = 512\n",
    "    epochs: int = 300\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 0.0\n",
    "    patience: int = 30\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # model\n",
    "    hidden_size: int = 512\n",
    "    num_layers: int = 2\n",
    "    dropout: float = 0.2\n",
    "\n",
    "    # data loading\n",
    "    num_workers: int = 0\n",
    "\n",
    "    # output controls\n",
    "    save_figures: bool = True\n",
    "    max_files_to_plot: Optional[int] = None  # None=all\n",
    "\n",
    "    # ===========================\n",
    "    # Trial9-style Evaluation settings\n",
    "    # ===========================\n",
    "    alpha: float = 0.20\n",
    "    ph_consecutive_m: int = 5\n",
    "    rep_method: str = \"mean\"\n",
    "    lambdas: Tuple[float, ...] = (0.2, 0.4, 0.6, 0.8)\n",
    "    lambda_to_plot: float = 0.6\n",
    "    eps_rul: float = 1e-8\n",
    "\n",
    "    # ===========================\n",
    "    # Trial13: Feature engineering from min_vce ONLY\n",
    "    # ===========================\n",
    "    # multi-scale deltas: v(t)-v(t-k)\n",
    "    delta_steps: Tuple[int, ...] = (1, 5, 20, 50)\n",
    "\n",
    "    # EMA trend features (recursive): span ~ smoothing strength\n",
    "    ema_spans: Tuple[int, ...] = (10, 50)\n",
    "\n",
    "    # rolling std over window (simple, O(T*W) but small W)\n",
    "    roll_std_window: int = 10\n",
    "\n",
    "    # add constant-per-window stats replicated across time\n",
    "    add_window_stats: bool = True\n",
    "    # window stats: mean, std, slope (linear regression) computed on raw vce in the window\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Data utils\n",
    "# ============================================================\n",
    "def list_csv_files(data_dir: str) -> List[Path]:\n",
    "    p = Path(data_dir)\n",
    "    files = sorted([f for f in p.glob(\"*.csv\") if f.is_file()])\n",
    "    if len(files) == 0:\n",
    "        raise FileNotFoundError(f\"No CSV files found in: {data_dir}\")\n",
    "    return files\n",
    "\n",
    "\n",
    "def read_one_csv(csv_path: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(f\"{csv_path.name}: expected at least 2 columns, got {df.shape[1]}\")\n",
    "    vce = df.iloc[:, 0].astype(np.float32).to_numpy()\n",
    "    rul = df.iloc[:, 1].astype(np.float32).to_numpy()\n",
    "\n",
    "    if len(vce) != len(rul):\n",
    "        raise ValueError(f\"{csv_path.name}: length mismatch vce={len(vce)}, rul={len(rul)}\")\n",
    "    if len(vce) < 5:\n",
    "        raise ValueError(f\"{csv_path.name}: too short sequence length={len(vce)}\")\n",
    "    return vce, rul\n",
    "\n",
    "\n",
    "def split_files(\n",
    "    files: List[Path],\n",
    "    train_ratio: float,\n",
    "    val_ratio: float,\n",
    "    test_ratio: float,\n",
    "    seed: int\n",
    ") -> Dict[str, List[Path]]:\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-9\n",
    "\n",
    "    rng = random.Random(seed)\n",
    "    files_shuffled = files[:]\n",
    "    rng.shuffle(files_shuffled)\n",
    "\n",
    "    n = len(files_shuffled)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_val = int(n * val_ratio)\n",
    "\n",
    "    train_files = files_shuffled[:n_train]\n",
    "    val_files = files_shuffled[n_train:n_train + n_val]\n",
    "    test_files = files_shuffled[n_train + n_val:]\n",
    "\n",
    "    return {\"train\": train_files, \"val\": val_files, \"test\": test_files}\n",
    "\n",
    "\n",
    "def delta_k(v: np.ndarray, k: int) -> np.ndarray:\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    if k <= 0:\n",
    "        return out\n",
    "    out[k:] = v[k:] - v[:-k]\n",
    "    return out\n",
    "\n",
    "\n",
    "def ema(v: np.ndarray, span: int) -> np.ndarray:\n",
    "    # simple EMA: alpha = 2/(span+1)\n",
    "    if span <= 1:\n",
    "        return v.astype(np.float32).copy()\n",
    "    a = 2.0 / (float(span) + 1.0)\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    out[0] = v[0]\n",
    "    for i in range(1, len(v)):\n",
    "        out[i] = a * v[i] + (1.0 - a) * out[i - 1]\n",
    "    return out\n",
    "\n",
    "\n",
    "def rolling_std(v: np.ndarray, w: int) -> np.ndarray:\n",
    "    # causal-ish rolling std using last w points; for first points, use available prefix\n",
    "    w = int(w)\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    for i in range(len(v)):\n",
    "        j0 = max(0, i - w + 1)\n",
    "        out[i] = float(np.std(v[j0:i + 1], ddof=0))\n",
    "    return out\n",
    "\n",
    "\n",
    "def feature_names(cfg: Config) -> List[str]:\n",
    "    names = [\"min_vce\"]\n",
    "    for k in cfg.delta_steps:\n",
    "        names.append(f\"delta_{k}\")\n",
    "    for s in cfg.ema_spans:\n",
    "        names.append(f\"ema_{s}\")\n",
    "    if cfg.roll_std_window and cfg.roll_std_window > 1:\n",
    "        names.append(f\"rollstd_{cfg.roll_std_window}\")\n",
    "    if cfg.add_window_stats:\n",
    "        names += [\"win_mean\", \"win_std\", \"win_slope\"]\n",
    "    return names\n",
    "\n",
    "\n",
    "def build_features_from_min_vce(vce: np.ndarray, cfg: Config) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return X(T,F) features derived ONLY from min_vce.\n",
    "    \"\"\"\n",
    "    feats = [vce.astype(np.float32)]\n",
    "\n",
    "    # multi-scale deltas\n",
    "    for k in cfg.delta_steps:\n",
    "        feats.append(delta_k(vce, int(k)))\n",
    "\n",
    "    # EMA trends\n",
    "    for s in cfg.ema_spans:\n",
    "        feats.append(ema(vce, int(s)))\n",
    "\n",
    "    # rolling std\n",
    "    if cfg.roll_std_window and cfg.roll_std_window > 1:\n",
    "        feats.append(rolling_std(vce, int(cfg.roll_std_window)))\n",
    "\n",
    "    X = np.stack(feats, axis=1).astype(np.float32)  # (T, F_base)\n",
    "\n",
    "    # window stats (placeholder here; computed per-window in dataset)\n",
    "    # We'll append later inside __getitem__ (because stats depend on window segment).\n",
    "    return X\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Dataset\n",
    "# ============================================================\n",
    "class WindowedRULDatasetNormMinVCE_Trial13(Dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x: (seq_len, F)\n",
    "      y_norm: (1,)\n",
    "      name, start_idx, y_cycles, rul0\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_list: List[Path],\n",
    "        cfg: Config,\n",
    "        scaler_x: StandardScaler = None,\n",
    "        fit_scaler: bool = False,\n",
    "    ):\n",
    "        self.file_list = file_list\n",
    "        self.cfg = cfg\n",
    "        self.seq_len = cfg.seq_len\n",
    "        self.stride = cfg.stride\n",
    "        self.pred_horizon = cfg.pred_horizon\n",
    "        self.scaler_x = scaler_x if scaler_x is not None else StandardScaler()\n",
    "\n",
    "        # store: (name, Xbase(T,Fbase), vce(T,), rul(T,), rul0)\n",
    "        self.series: List[Tuple[str, np.ndarray, np.ndarray, np.ndarray, float]] = []\n",
    "        for fp in self.file_list:\n",
    "            vce, rul = read_one_csv(fp)\n",
    "            rul0 = float(rul[0])\n",
    "            if rul0 <= 0:\n",
    "                raise ValueError(f\"{fp.name}: RUL0 must be > 0, got {rul0}\")\n",
    "\n",
    "            Xbase = build_features_from_min_vce(vce, cfg).astype(np.float32)\n",
    "            self.series.append((fp.name, Xbase, vce.astype(np.float32), rul.astype(np.float32), rul0))\n",
    "\n",
    "        # Fit scaler on TRAIN windows (recommended):\n",
    "        # We fit using ALL timesteps of ALL files here (same as your previous approach),\n",
    "        # BUT with window-stats appended we need a strategy.\n",
    "        #\n",
    "        # Strategy:\n",
    "        # - Fit scaler on base features only + append \"typical\" window stats computed from sliding windows\n",
    "        #   => easiest: build a sampled set of windows and gather per-timestep feature rows.\n",
    "        #\n",
    "        # To keep it simple & stable:\n",
    "        # - If add_window_stats=False: fit on all timesteps base features (fast).\n",
    "        # - If add_window_stats=True: sample up to N windows across files and fit on resulting full features.\n",
    "        if fit_scaler:\n",
    "            if not cfg.add_window_stats:\n",
    "                all_x = np.concatenate([Xbase for _, Xbase, _, _, _ in self.series], axis=0)\n",
    "                self.scaler_x.fit(all_x)\n",
    "            else:\n",
    "                # sample windows to fit scaler with full feature dimension\n",
    "                rng = np.random.RandomState(0)\n",
    "                rows = []\n",
    "                max_windows_for_scaler = 5000  # reasonable cap\n",
    "                for (_name, Xbase, vce_raw, _rul, _rul0) in self.series:\n",
    "                    T = Xbase.shape[0]\n",
    "                    last_start = T - (self.seq_len + self.pred_horizon)\n",
    "                    if last_start < 0:\n",
    "                        continue\n",
    "                    starts = list(range(0, last_start + 1, self.stride))\n",
    "                    if len(starts) == 0:\n",
    "                        continue\n",
    "                    # subsample per file\n",
    "                    if len(starts) > 200:\n",
    "                        starts = rng.choice(starts, size=200, replace=False).tolist()\n",
    "                    for s in starts:\n",
    "                        xw = Xbase[s:s + self.seq_len, :]  # (L,Fbase)\n",
    "                        # window stats from raw vce segment\n",
    "                        seg = vce_raw[s:s + self.seq_len]\n",
    "                        wmean = float(np.mean(seg))\n",
    "                        wstd = float(np.std(seg, ddof=0))\n",
    "                        # slope via least squares on t\n",
    "                        t = np.arange(self.seq_len, dtype=np.float32)\n",
    "                        # slope = cov(t,seg)/var(t)\n",
    "                        denom = float(np.var(t) + 1e-12)\n",
    "                        slope = float(np.cov(t, seg, ddof=0)[0, 1] / denom) if denom > 0 else 0.0\n",
    "                        stats = np.array([wmean, wstd, slope], dtype=np.float32).reshape(1, 3)\n",
    "                        stats_rep = np.repeat(stats, repeats=self.seq_len, axis=0)\n",
    "                        xfull = np.concatenate([xw, stats_rep], axis=1)  # (L,F)\n",
    "                        rows.append(xfull)\n",
    "                        if len(rows) >= max_windows_for_scaler:\n",
    "                            break\n",
    "                    if len(rows) >= max_windows_for_scaler:\n",
    "                        break\n",
    "                if len(rows) == 0:\n",
    "                    raise ValueError(\"Scaler fitting failed: no windows sampled. Check seq_len/stride.\")\n",
    "                fit_mat = np.concatenate(rows, axis=0)\n",
    "                self.scaler_x.fit(fit_mat)\n",
    "\n",
    "        # window index\n",
    "        self.index: List[Tuple[int, int]] = []\n",
    "        for fi, (_name, Xbase, _vce, _rul, _rul0) in enumerate(self.series):\n",
    "            T = Xbase.shape[0]\n",
    "            last_start = T - (self.seq_len + self.pred_horizon)\n",
    "            if last_start < 0:\n",
    "                continue\n",
    "            for s in range(0, last_start + 1, self.stride):\n",
    "                self.index.append((fi, s))\n",
    "\n",
    "        if len(self.index) == 0:\n",
    "            raise ValueError(\"No windows were created. Check seq_len/pred_horizon vs file lengths.\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fi, s = self.index[idx]\n",
    "        name, Xbase, vce_raw, rul, rul0 = self.series[fi]\n",
    "\n",
    "        x = Xbase[s:s + self.seq_len, :]  # (L,Fbase)\n",
    "\n",
    "        if self.cfg.add_window_stats:\n",
    "            seg = vce_raw[s:s + self.seq_len]\n",
    "            wmean = float(np.mean(seg))\n",
    "            wstd = float(np.std(seg, ddof=0))\n",
    "            t = np.arange(self.seq_len, dtype=np.float32)\n",
    "            denom = float(np.var(t) + 1e-12)\n",
    "            slope = float(np.cov(t, seg, ddof=0)[0, 1] / denom) if denom > 0 else 0.0\n",
    "            stats = np.array([wmean, wstd, slope], dtype=np.float32).reshape(1, 3)\n",
    "            stats_rep = np.repeat(stats, repeats=self.seq_len, axis=0)\n",
    "            x = np.concatenate([x, stats_rep], axis=1).astype(np.float32)  # (L,F)\n",
    "\n",
    "        y_idx = s + self.seq_len - 1 + self.pred_horizon\n",
    "        y_cycles = float(rul[y_idx])\n",
    "        y_norm = np.array([y_cycles / rul0], dtype=np.float32)\n",
    "\n",
    "        x = self.scaler_x.transform(x).astype(np.float32)\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x),\n",
    "            torch.from_numpy(y_norm),\n",
    "            name,\n",
    "            torch.tensor(s, dtype=torch.long),\n",
    "            torch.tensor(y_cycles, dtype=torch.float32),\n",
    "            torch.tensor(rul0, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Model\n",
    "# ============================================================\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.head(last)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Basic Eval + Save window-level predictions\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def evaluate_basic(model, loader, device) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "\n",
    "    mae_norm_list, mse_norm_list = [], []\n",
    "    mae_cyc_list, mse_cyc_list = [], []\n",
    "\n",
    "    for x, y_norm, _name, _s, y_cycles, rul0 in loader:\n",
    "        x = x.to(device)\n",
    "        y_norm = y_norm.to(device)\n",
    "        y_cycles = y_cycles.to(device).view(-1, 1)\n",
    "        rul0 = rul0.to(device).view(-1, 1)\n",
    "\n",
    "        pred_norm = model(x)\n",
    "\n",
    "        err_norm = pred_norm - y_norm\n",
    "        mae_norm_list.append(torch.mean(torch.abs(err_norm)).item())\n",
    "        mse_norm_list.append(torch.mean(err_norm ** 2).item())\n",
    "\n",
    "        pred_cycles = pred_norm * rul0\n",
    "        err_cyc = pred_cycles - y_cycles\n",
    "        mae_cyc_list.append(torch.mean(torch.abs(err_cyc)).item())\n",
    "        mse_cyc_list.append(torch.mean(err_cyc ** 2).item())\n",
    "\n",
    "    return {\n",
    "        \"mae_norm\": float(np.mean(mae_norm_list)) if mae_norm_list else float(\"nan\"),\n",
    "        \"rmse_norm\": float(np.sqrt(np.mean(mse_norm_list))) if mse_norm_list else float(\"nan\"),\n",
    "        \"mae_cycles\": float(np.mean(mae_cyc_list)) if mae_cyc_list else float(\"nan\"),\n",
    "        \"rmse_cycles\": float(np.sqrt(np.mean(mse_cyc_list))) if mse_cyc_list else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_predictions_windows_csv(model, loader, device, out_csv: str, seq_len: int) -> None:\n",
    "    model.eval()\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for x, y_norm, name, s, y_cycles, rul0 in loader:\n",
    "        x = x.to(device)\n",
    "        y_norm = y_norm.to(device)\n",
    "        y_cycles = y_cycles.to(device).view(-1, 1)\n",
    "        rul0 = rul0.to(device).view(-1, 1)\n",
    "\n",
    "        pred_norm = model(x)\n",
    "        pred_cycles = pred_norm * rul0\n",
    "\n",
    "        pred_norm_np = pred_norm.cpu().numpy().reshape(-1)\n",
    "        y_norm_np = y_norm.cpu().numpy().reshape(-1)\n",
    "        pred_cyc_np = pred_cycles.cpu().numpy().reshape(-1)\n",
    "        y_cyc_np = y_cycles.cpu().numpy().reshape(-1)\n",
    "\n",
    "        rul0_np = rul0.cpu().numpy().reshape(-1)\n",
    "        s_np = s.cpu().numpy().reshape(-1)\n",
    "        name_list = list(name)\n",
    "\n",
    "        for i in range(len(pred_norm_np)):\n",
    "            rows.append({\n",
    "                \"file\": name_list[i],\n",
    "                \"start_idx\": int(s_np[i]),\n",
    "                \"cycle\": int(s_np[i] + (seq_len - 1)),\n",
    "                \"rul0\": float(rul0_np[i]),\n",
    "                \"RUL_true\": float(y_cyc_np[i]),\n",
    "                \"RUL_pred\": float(pred_cyc_np[i]),\n",
    "                \"RUL_true_norm\": float(y_norm_np[i]),\n",
    "                \"RUL_pred_norm\": float(pred_norm_np[i]),\n",
    "            })\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) Window -> Cycle sequence (mean representative)\n",
    "# ============================================================\n",
    "def windows_to_cycle_sequence_mean(windows_csv: str) -> pd.DataFrame:\n",
    "    dfw = pd.read_csv(windows_csv)\n",
    "    if dfw.empty:\n",
    "        raise ValueError(f\"Empty windows csv: {windows_csv}\")\n",
    "\n",
    "    g = dfw.groupby([\"file\", \"cycle\"], as_index=False).agg(\n",
    "        rul0=(\"rul0\", \"first\"),\n",
    "        RUL_true=(\"RUL_true\", \"mean\"),\n",
    "        RUL_pred=(\"RUL_pred\", \"mean\"),\n",
    "        n_windows=(\"RUL_pred\", \"count\"),\n",
    "    )\n",
    "    return g\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) Prognostics metrics (same as Trial9)\n",
    "# ============================================================\n",
    "def compute_metrics_for_one_file(\n",
    "    df_seq_one_file: pd.DataFrame,\n",
    "    seq_len: int,\n",
    "    alpha: float,\n",
    "    ph_consecutive_m: int,\n",
    "    lambdas: Tuple[float, ...],\n",
    "    eps_rul: float,\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "\n",
    "    df = df_seq_one_file.sort_values(\"cycle\").reset_index(drop=True).copy()\n",
    "\n",
    "    t_s = seq_len - 1\n",
    "    last_cycle = int(df[\"cycle\"].max())\n",
    "    EOL_true = last_cycle + 1\n",
    "    t_e = EOL_true - 1\n",
    "\n",
    "    df_eval = df[(df[\"cycle\"] >= t_s) & (df[\"cycle\"] <= t_e)].copy()\n",
    "    df_eval.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if df_eval.empty:\n",
    "        summary = {\n",
    "            \"t_s\": t_s, \"t_e\": t_e, \"EOL_true\": EOL_true,\n",
    "            \"PH\": np.nan, \"t_PH_start\": np.nan,\n",
    "            \"CRA\": np.nan, \"Convergence_cycles\": np.nan,\n",
    "        }\n",
    "        for lam in lambdas:\n",
    "            summary[f\"t_lambda_{lam:.2f}\"] = np.nan\n",
    "            summary[f\"alpha_lambda_ok_{lam:.2f}\"] = np.nan\n",
    "        return df_eval, summary\n",
    "\n",
    "    denom = np.maximum(np.abs(df_eval[\"RUL_true\"].values), eps_rul)\n",
    "    rel_err = np.abs(df_eval[\"RUL_true\"].values - df_eval[\"RUL_pred\"].values) / denom\n",
    "    RA = 1.0 - rel_err\n",
    "\n",
    "    df_eval[\"rel_err\"] = rel_err\n",
    "    df_eval[\"RA\"] = RA\n",
    "    df_eval[\"in_alpha\"] = df_eval[\"rel_err\"] <= alpha\n",
    "\n",
    "    CRA = float(np.mean(df_eval[\"RA\"].values))\n",
    "\n",
    "    flags = df_eval[\"in_alpha\"].values.astype(np.int32)\n",
    "    t_PH_start = np.nan\n",
    "    if len(flags) >= ph_consecutive_m:\n",
    "        run = 0\n",
    "        for i, ok in enumerate(flags):\n",
    "            if ok:\n",
    "                run += 1\n",
    "                if run >= ph_consecutive_m:\n",
    "                    start_i = i - ph_consecutive_m + 1\n",
    "                    t_PH_start = int(df_eval.loc[start_i, \"cycle\"])\n",
    "                    break\n",
    "            else:\n",
    "                run = 0\n",
    "\n",
    "    if np.isfinite(t_PH_start):\n",
    "        PH = float(EOL_true - t_PH_start)\n",
    "        Convergence_cycles = float(t_PH_start - t_s)\n",
    "    else:\n",
    "        PH = np.nan\n",
    "        Convergence_cycles = np.nan\n",
    "\n",
    "    rul0 = float(df_eval[\"rul0\"].iloc[0])\n",
    "    lam_results = {}\n",
    "    for lam in lambdas:\n",
    "        target_rul = (1.0 - float(lam)) * rul0\n",
    "        idx = int(np.argmin(np.abs(df_eval[\"RUL_true\"].values - target_rul)))\n",
    "        t_lam = int(df_eval.loc[idx, \"cycle\"])\n",
    "        ok = bool(df_eval.loc[idx, \"rel_err\"] <= alpha)\n",
    "\n",
    "        lam_results[f\"t_lambda_{lam:.2f}\"] = t_lam\n",
    "        lam_results[f\"alpha_lambda_ok_{lam:.2f}\"] = int(ok)\n",
    "\n",
    "    summary = {\n",
    "        \"t_s\": int(t_s),\n",
    "        \"t_e\": int(t_e),\n",
    "        \"EOL_true\": int(EOL_true),\n",
    "        \"alpha\": float(alpha),\n",
    "        \"ph_consecutive_m\": int(ph_consecutive_m),\n",
    "        \"CRA\": CRA,\n",
    "        \"t_PH_start\": t_PH_start if np.isfinite(t_PH_start) else np.nan,\n",
    "        \"PH\": PH,\n",
    "        \"Convergence_cycles\": Convergence_cycles,\n",
    "        **lam_results\n",
    "    }\n",
    "    return df_eval, summary\n",
    "\n",
    "\n",
    "def compute_metrics_from_windows_csv(\n",
    "    windows_csv: str,\n",
    "    seq_len: int,\n",
    "    alpha: float,\n",
    "    ph_consecutive_m: int,\n",
    "    lambdas: Tuple[float, ...],\n",
    "    eps_rul: float,\n",
    "    out_dir: str,\n",
    "    split_name: str,\n",
    ") -> Tuple[str, str]:\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    df_seq = windows_to_cycle_sequence_mean(windows_csv)\n",
    "    seq_path = os.path.join(out_dir, f\"{split_name}_cycle_sequence_mean.csv\")\n",
    "    df_seq.to_csv(seq_path, index=False)\n",
    "\n",
    "    rows = []\n",
    "    for f in df_seq[\"file\"].unique():\n",
    "        sub = df_seq[df_seq[\"file\"] == f].copy()\n",
    "        _df_eval, summary = compute_metrics_for_one_file(\n",
    "            df_seq_one_file=sub,\n",
    "            seq_len=seq_len,\n",
    "            alpha=alpha,\n",
    "            ph_consecutive_m=ph_consecutive_m,\n",
    "            lambdas=lambdas,\n",
    "            eps_rul=eps_rul,\n",
    "        )\n",
    "        summary[\"file\"] = f\n",
    "        rows.append(summary)\n",
    "\n",
    "    dfm = pd.DataFrame(rows)\n",
    "    metrics_path = os.path.join(out_dir, f\"{split_name}_prognostics_metrics_per_file.csv\")\n",
    "    dfm.to_csv(metrics_path, index=False)\n",
    "\n",
    "    return seq_path, metrics_path\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) Plotters (same as Trial9 + your fixes)\n",
    "# ============================================================\n",
    "def _safe_name(s: str) -> str:\n",
    "    return s.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\")\n",
    "\n",
    "\n",
    "def plot_alpha_ph(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    title: str,\n",
    "    alpha: float,\n",
    "    PH_start: Optional[float],\n",
    "    out_path: str,\n",
    "    dpi: int = 200,\n",
    ") -> None:\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    plt.figure()\n",
    "\n",
    "    x = df_eval[\"cycle\"].values\n",
    "    y_true = df_eval[\"RUL_true\"].values\n",
    "    y_pred = df_eval[\"RUL_pred\"].values\n",
    "\n",
    "    upper = y_true * (1.0 + alpha)\n",
    "    lower = y_true * (1.0 - alpha)\n",
    "\n",
    "    plt.plot(x, y_true, color=\"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, color=\"r\", label=\"Prediction (cycles)\")\n",
    "    plt.plot(x, upper, color=\"b\", linestyle=\"--\", label=f\"+{alpha:.2f} alpha accuracy zone\")\n",
    "    plt.plot(x, lower, color=\"b\", linestyle=\"--\", label=f\"-{alpha:.2f} alpha accuracy zone\")\n",
    "\n",
    "    if PH_start is not None and np.isfinite(PH_start):\n",
    "        plt.axvline(int(PH_start), color=\"g\", linestyle=\"-.\", label=\"PH start\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title}\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_alpha_lambda(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    title: str,\n",
    "    alpha: float,\n",
    "    lambda_to_plot: float,\n",
    "    t_lambda: Optional[int],\n",
    "    out_path: str,\n",
    "    dpi: int = 200,\n",
    ") -> None:\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    x = df_eval[\"cycle\"].values\n",
    "    y_true = df_eval[\"RUL_true\"].values\n",
    "    y_pred = df_eval[\"RUL_pred\"].values\n",
    "\n",
    "    upper = y_true * (1.0 + alpha)\n",
    "    lower = y_true * (1.0 - alpha)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y_true, color=\"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, color=\"r\", label=\"Prediction (cycles)\")\n",
    "\n",
    "    if t_lambda is not None and np.isfinite(t_lambda):\n",
    "        t_lambda = int(t_lambda)\n",
    "        plt.axvline(t_lambda, linestyle=\":\", color=\"g\", label=f\"t_λ (λ={lambda_to_plot:.2f})\")\n",
    "        mask = x >= t_lambda\n",
    "        if np.any(mask):\n",
    "            plt.plot(x[mask], upper[mask], color=\"b\", linestyle=\"--\", label=f\"+{alpha:.2f} alpha–lambda zone\")\n",
    "            plt.plot(x[mask], lower[mask], color=\"b\", linestyle=\"--\", label=f\"-{alpha:.2f} alpha–lambda zone\")\n",
    "        else:\n",
    "            plt.plot(x, upper, color=\"b\", linestyle=\"--\", label=f\"+{alpha:.2f} α zone\")\n",
    "            plt.plot(x, lower, color=\"b\", linestyle=\"--\", label=f\"-{alpha:.2f} α zone\")\n",
    "    else:\n",
    "        plt.plot(x, upper, color=\"b\", linestyle=\"--\", label=f\"+{alpha:.2f} α zone\")\n",
    "        plt.plot(x, lower, color=\"b\", linestyle=\"--\", label=f\"-{alpha:.2f} α zone\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title}\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def make_paper_figures_for_split(\n",
    "    cycle_seq_csv: str,\n",
    "    metrics_per_file_csv: str,\n",
    "    out_fig_dir: str,\n",
    "    title_prefix: str,\n",
    "    alpha: float,\n",
    "    lambda_to_plot: float,\n",
    "    max_files: Optional[int] = None,\n",
    "    dpi: int = 200,\n",
    ") -> None:\n",
    "    df_seq = pd.read_csv(cycle_seq_csv)\n",
    "    dfm = pd.read_csv(metrics_per_file_csv)\n",
    "\n",
    "    files = df_seq[\"file\"].unique().tolist()\n",
    "    if max_files is not None:\n",
    "        files = files[:max_files]\n",
    "\n",
    "    os.makedirs(out_fig_dir, exist_ok=True)\n",
    "\n",
    "    lam_key = f\"t_lambda_{lambda_to_plot:.2f}\"\n",
    "\n",
    "    for f in files:\n",
    "        sub = df_seq[df_seq[\"file\"] == f].sort_values(\"cycle\").copy()\n",
    "        mrow = dfm[dfm[\"file\"] == f]\n",
    "        if mrow.empty:\n",
    "            continue\n",
    "        mrow = mrow.iloc[0].to_dict()\n",
    "\n",
    "        t_s = int(mrow[\"t_s\"])\n",
    "        t_e = int(mrow[\"t_e\"])\n",
    "        PH_start = mrow.get(\"t_PH_start\", np.nan)\n",
    "\n",
    "        df_eval = sub[(sub[\"cycle\"] >= t_s) & (sub[\"cycle\"] <= t_e)].copy()\n",
    "        if df_eval.empty:\n",
    "            continue\n",
    "\n",
    "        t_lambda = None\n",
    "        if lam_key in mrow and np.isfinite(mrow[lam_key]):\n",
    "            t_lambda = int(mrow[lam_key])\n",
    "\n",
    "        safe = _safe_name(f)\n",
    "\n",
    "        out1 = os.path.join(out_fig_dir, f\"FIG1_alpha_PH__{safe}.png\")\n",
    "        plot_alpha_ph(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            title=f\"{title_prefix} | α+PH\",\n",
    "            alpha=alpha,\n",
    "            PH_start=PH_start if np.isfinite(PH_start) else None,\n",
    "            out_path=out1,\n",
    "            dpi=dpi,\n",
    "        )\n",
    "\n",
    "        out2 = os.path.join(out_fig_dir, f\"FIG2_alpha_lambda__lam{lambda_to_plot:.2f}__{safe}.png\")\n",
    "        plot_alpha_lambda(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            title=f\"{title_prefix} | α–λ (λ={lambda_to_plot:.2f})\",\n",
    "            alpha=alpha,\n",
    "            lambda_to_plot=lambda_to_plot,\n",
    "            t_lambda=t_lambda,\n",
    "            out_path=out2,\n",
    "            dpi=dpi,\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) One seed run (train + export best/last + Trial9 metrics)\n",
    "# ============================================================\n",
    "def run_one_seed(cfg: Config, seed: int) -> Dict[str, Any]:\n",
    "    set_seed(seed)\n",
    "\n",
    "    seed_dir = os.path.join(cfg.out_dir, f\"seed_{seed}\")\n",
    "    os.makedirs(seed_dir, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"[SEED {seed}] device={device}\")\n",
    "    print(f\"[SEED {seed}] out={seed_dir}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    # split\n",
    "    files = list_csv_files(cfg.data_dir)\n",
    "    splits = split_files(files, cfg.train_ratio, cfg.val_ratio, cfg.test_ratio, seed)\n",
    "\n",
    "    # save split lists\n",
    "    for k in [\"train\", \"val\", \"test\"]:\n",
    "        pd.Series([p.name for p in splits[k]]).to_csv(\n",
    "            os.path.join(seed_dir, f\"{k}_files.csv\"), index=False, header=False\n",
    "        )\n",
    "\n",
    "    # datasets (fit scaler on train only)\n",
    "    scaler_x = StandardScaler()\n",
    "    train_ds = WindowedRULDatasetNormMinVCE_Trial13(\n",
    "        splits[\"train\"], cfg, scaler_x=scaler_x, fit_scaler=True\n",
    "    )\n",
    "    val_ds = WindowedRULDatasetNormMinVCE_Trial13(\n",
    "        splits[\"val\"], cfg, scaler_x=train_ds.scaler_x, fit_scaler=False\n",
    "    )\n",
    "    test_ds = WindowedRULDatasetNormMinVCE_Trial13(\n",
    "        splits[\"test\"], cfg, scaler_x=train_ds.scaler_x, fit_scaler=False\n",
    "    )\n",
    "\n",
    "    feat_list = feature_names(cfg)\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feat_list,\n",
    "        \"mean\": train_ds.scaler_x.mean_.ravel(),\n",
    "        \"std\": np.sqrt(train_ds.scaler_x.var_).ravel(),\n",
    "    }).to_csv(os.path.join(seed_dir, \"scaler_x_mean_std.csv\"), index=False)\n",
    "\n",
    "    # loaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "\n",
    "    train_eval = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "    val_eval = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "    test_eval = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "\n",
    "    # model\n",
    "    input_size = len(feat_list)\n",
    "    model = LSTMRegressor(\n",
    "        input_size=input_size,\n",
    "        hidden_size=cfg.hidden_size,\n",
    "        num_layers=cfg.num_layers,\n",
    "        dropout=cfg.dropout,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    best_by_val_norm = float(\"inf\")\n",
    "    best_path = os.path.join(seed_dir, \"best_by_val_norm.pt\")\n",
    "    last_path = os.path.join(seed_dir, \"last_epoch.pt\")\n",
    "\n",
    "    history: List[Dict[str, Any]] = []\n",
    "    bad_epochs = 0\n",
    "\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        model.train()\n",
    "        losses = []\n",
    "\n",
    "        for x, y_norm, *_ in train_loader:\n",
    "            x = x.to(device)\n",
    "            y_norm = y_norm.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred_norm = model(x)\n",
    "            loss = criterion(pred_norm, y_norm)\n",
    "            loss.backward()\n",
    "\n",
    "            if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        train_mse_norm = float(np.mean(losses)) if losses else float(\"nan\")\n",
    "        val_metrics = evaluate_basic(model, val_loader, device)\n",
    "\n",
    "        history.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_mse_norm\": train_mse_norm,\n",
    "            \"val_rmse_norm\": val_metrics[\"rmse_norm\"],\n",
    "            \"val_mae_norm\": val_metrics[\"mae_norm\"],\n",
    "            \"val_rmse_cycles\": val_metrics[\"rmse_cycles\"],\n",
    "            \"val_mae_cycles\": val_metrics[\"mae_cycles\"],\n",
    "        })\n",
    "\n",
    "        if val_metrics[\"rmse_norm\"] < best_by_val_norm:\n",
    "            best_by_val_norm = val_metrics[\"rmse_norm\"]\n",
    "            bad_epochs = 0\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(\n",
    "                f\"[SEED {seed}] [{epoch:03d}/{cfg.epochs}] \"\n",
    "                f\"train_mse_norm={train_mse_norm:.6f} | \"\n",
    "                f\"val_rmse_norm={val_metrics['rmse_norm']:.6f} | \"\n",
    "                f\"val_mae_cycles={val_metrics['mae_cycles']:.3f} | \"\n",
    "                f\"best_val_rmse_norm={best_by_val_norm:.6f}\"\n",
    "            )\n",
    "\n",
    "        if bad_epochs >= cfg.patience:\n",
    "            print(f\"[SEED {seed}] Early stopping at epoch {epoch}.\")\n",
    "            break\n",
    "\n",
    "    pd.DataFrame(history).to_csv(os.path.join(seed_dir, \"history.csv\"), index=False)\n",
    "    torch.save(model.state_dict(), last_path)\n",
    "\n",
    "    def export_ckpt(tag: str, ckpt_path: str) -> Dict[str, Any]:\n",
    "        sub_dir = os.path.join(seed_dir, tag)\n",
    "        os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "        model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "        model.eval()\n",
    "\n",
    "        tr = evaluate_basic(model, train_eval, device)\n",
    "        va = evaluate_basic(model, val_eval, device)\n",
    "        te = evaluate_basic(model, test_eval, device)\n",
    "\n",
    "        for split_name, loader in [(\"train\", train_eval), (\"val\", val_eval), (\"test\", test_eval)]:\n",
    "            win_csv = os.path.join(sub_dir, f\"{split_name}_predictions_windows.csv\")\n",
    "            save_predictions_windows_csv(model, loader, device, win_csv, seq_len=cfg.seq_len)\n",
    "\n",
    "            seq_csv, metrics_csv = compute_metrics_from_windows_csv(\n",
    "                windows_csv=win_csv,\n",
    "                seq_len=cfg.seq_len,\n",
    "                alpha=cfg.alpha,\n",
    "                ph_consecutive_m=cfg.ph_consecutive_m,\n",
    "                lambdas=cfg.lambdas,\n",
    "                eps_rul=cfg.eps_rul,\n",
    "                out_dir=sub_dir,\n",
    "                split_name=split_name,\n",
    "            )\n",
    "\n",
    "            if cfg.save_figures:\n",
    "                fig_dir = os.path.join(sub_dir, \"paper_figures\", split_name)\n",
    "                make_paper_figures_for_split(\n",
    "                    cycle_seq_csv=seq_csv,\n",
    "                    metrics_per_file_csv=metrics_csv,\n",
    "                    out_fig_dir=fig_dir,\n",
    "                    title_prefix=f\"SEED {seed} | {tag.upper()} | {split_name}\",\n",
    "                    alpha=cfg.alpha,\n",
    "                    lambda_to_plot=cfg.lambda_to_plot,\n",
    "                    max_files=cfg.max_files_to_plot,\n",
    "                )\n",
    "\n",
    "        ms = {\n",
    "            \"seed\": seed,\n",
    "            \"checkpoint\": tag,\n",
    "            \"train_rmse_cycles\": tr[\"rmse_cycles\"],\n",
    "            \"train_mae_cycles\": tr[\"mae_cycles\"],\n",
    "            \"train_rmse_norm\": tr[\"rmse_norm\"],\n",
    "            \"train_mae_norm\": tr[\"mae_norm\"],\n",
    "            \"val_rmse_cycles\": va[\"rmse_cycles\"],\n",
    "            \"val_mae_cycles\": va[\"mae_cycles\"],\n",
    "            \"val_rmse_norm\": va[\"rmse_norm\"],\n",
    "            \"val_mae_norm\": va[\"mae_norm\"],\n",
    "            \"test_rmse_cycles\": te[\"rmse_cycles\"],\n",
    "            \"test_mae_cycles\": te[\"mae_cycles\"],\n",
    "            \"test_rmse_norm\": te[\"rmse_norm\"],\n",
    "            \"test_mae_norm\": te[\"mae_norm\"],\n",
    "            \"stopped_epoch\": history[-1][\"epoch\"] if len(history) else None,\n",
    "            \"best_val_rmse_norm\": best_by_val_norm,\n",
    "            \"alpha\": cfg.alpha,\n",
    "            \"ph_consecutive_m\": cfg.ph_consecutive_m,\n",
    "            \"rep_method\": cfg.rep_method,\n",
    "            \"lambdas\": str(cfg.lambdas),\n",
    "            \"lambda_to_plot\": cfg.lambda_to_plot,\n",
    "            \"feature_dim\": input_size,\n",
    "            \"features\": \",\".join(feature_names(cfg)),\n",
    "        }\n",
    "        pd.DataFrame([ms]).to_csv(os.path.join(sub_dir, \"metrics_summary.csv\"), index=False)\n",
    "\n",
    "        print(f\"[SEED {seed}] {tag}: TEST mae_cycles={te['mae_cycles']:.3f} | rmse_cycles={te['rmse_cycles']:.3f} | rmse_norm={te['rmse_norm']:.6f}\")\n",
    "        return ms\n",
    "\n",
    "    ms_best = export_ckpt(\"best_by_val_norm\", best_path)\n",
    "    ms_last = export_ckpt(\"last_epoch\", last_path)\n",
    "\n",
    "    return {\"seed\": seed, \"seed_dir\": seed_dir, \"best\": ms_best, \"last\": ms_last}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10) Seed sweep + global comparison\n",
    "# ============================================================\n",
    "def summarize_across_seeds(cfg: Config, results: List[Dict[str, Any]]) -> None:\n",
    "    rows = []\n",
    "    for r in results:\n",
    "        rows.append(r[\"best\"])\n",
    "        rows.append(r[\"last\"])\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(os.path.join(cfg.out_dir, \"summary_across_seeds.csv\"), index=False)\n",
    "\n",
    "    def _isfinite(x: Any) -> bool:\n",
    "        try:\n",
    "            return bool(np.isfinite(float(x)))\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def win_rate(metric: str) -> Dict[str, Any]:\n",
    "        wins_last = 0\n",
    "        wins_best = 0\n",
    "        ties = 0\n",
    "        diffs = []\n",
    "\n",
    "        for r in results:\n",
    "            b = r[\"best\"][metric]\n",
    "            l = r[\"last\"][metric]\n",
    "            if _isfinite(b) and _isfinite(l):\n",
    "                diffs.append(float(l) - float(b))\n",
    "                if float(l) < float(b):\n",
    "                    wins_last += 1\n",
    "                elif float(b) < float(l):\n",
    "                    wins_best += 1\n",
    "                else:\n",
    "                    ties += 1\n",
    "\n",
    "        return {\n",
    "            \"metric\": metric,\n",
    "            \"wins_last\": wins_last,\n",
    "            \"wins_best\": wins_best,\n",
    "            \"ties\": ties,\n",
    "            \"mean(last-best)\": float(np.mean(diffs)) if diffs else float(\"nan\"),\n",
    "            \"std(last-best)\": float(np.std(diffs)) if diffs else float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    metrics = [\"test_mae_cycles\", \"test_rmse_cycles\", \"test_mae_norm\", \"test_rmse_norm\"]\n",
    "    wr = [win_rate(m) for m in metrics]\n",
    "    pd.DataFrame(wr).to_csv(os.path.join(cfg.out_dir, \"win_rate_summary.csv\"), index=False)\n",
    "\n",
    "    lines = []\n",
    "    lines.append(\"=== WIN-RATE SUMMARY (TEST; lower is better) ===\")\n",
    "    for row in wr:\n",
    "        lines.append(\n",
    "            f\"- {row['metric']}: last wins={row['wins_last']}, best wins={row['wins_best']}, ties={row['ties']} | \"\n",
    "            f\"mean(last-best)={row['mean(last-best)']:.6f}, std(last-best)={row['std(last-best)']:.6f}\"\n",
    "        )\n",
    "\n",
    "    agg = df.groupby(\"checkpoint\")[metrics].agg([\"mean\", \"std\"])\n",
    "    lines.append(\"\\n=== MEAN ± STD across seeds (TEST) ===\")\n",
    "    lines.append(str(agg))\n",
    "\n",
    "    with open(os.path.join(cfg.out_dir, \"win_rate_summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nSaved:\")\n",
    "    print(\" -\", os.path.join(cfg.out_dir, \"summary_across_seeds.csv\"))\n",
    "    print(\" -\", os.path.join(cfg.out_dir, \"win_rate_summary.csv\"))\n",
    "    print(\" -\", os.path.join(cfg.out_dir, \"win_rate_summary.txt\"))\n",
    "\n",
    "\n",
    "def run_trial13_seed_sweep(cfg: Config) -> None:\n",
    "    os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "\n",
    "    results = []\n",
    "    for seed in cfg.seeds:\n",
    "        res = run_one_seed(cfg, seed)\n",
    "        results.append(res)\n",
    "\n",
    "    summarize_across_seeds(cfg, results)\n",
    "\n",
    "    print(\"\\nDONE. Check Trial13 folder:\")\n",
    "    print(\" - per seed results: Trial13/seed_<seed>/...\")\n",
    "    print(\" - figures (paper-style): seed_<seed>/<ckpt>/paper_figures/<split>/\")\n",
    "    print(\" - cycle sequence mean CSV: <ckpt>/<split>_cycle_sequence_mean.csv\")\n",
    "    print(\" - PH/α–λ metrics CSV: <ckpt>/<split>_prognostics_metrics_per_file.csv\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 11) Run\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = Config(\n",
    "        data_dir=r\"C:\\Users\\11HOME_AHCI\\Desktop\\PoF\\Winter 2026\\Ref Data4\\100\",\n",
    "        out_dir=r\"./Trial13\",\n",
    "\n",
    "        seeds=(9819123, 111, 222, 333, 444),\n",
    "\n",
    "        seq_len=100,\n",
    "        stride=5,\n",
    "        pred_horizon=0,\n",
    "\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.2,\n",
    "        test_ratio=0.1,\n",
    "\n",
    "        batch_size=512,\n",
    "        epochs=300,\n",
    "        lr=1e-3,\n",
    "        weight_decay=0.0,\n",
    "        patience=30,\n",
    "        grad_clip=1.0,\n",
    "\n",
    "        hidden_size=512,\n",
    "        num_layers=2,\n",
    "        dropout=0.2,\n",
    "\n",
    "        save_figures=True,\n",
    "        max_files_to_plot=None,\n",
    "        num_workers=0,\n",
    "\n",
    "        alpha=0.20,\n",
    "        ph_consecutive_m=5,\n",
    "        rep_method=\"mean\",\n",
    "        lambdas=(0.2, 0.4, 0.6, 0.8),\n",
    "        lambda_to_plot=0.6,\n",
    "\n",
    "        # === Trial13 features (min_vce only) ===\n",
    "        delta_steps=(1, 5, 20, 50),\n",
    "        ema_spans=(10, 50),\n",
    "        roll_std_window=10,\n",
    "        add_window_stats=True,\n",
    "    )\n",
    "\n",
    "    run_trial13_seed_sweep(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8ffe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ BEST MODEL (Trial13) ================\n",
      "[SELECTED BY VAL]  (recommended for model selection)\n",
      "  Seed             : 333\n",
      "  Checkpoint       : best_by_val_norm\n",
      "  VAL  RMSE (cyc)   : 2118.589\n",
      "  VAL  MAE  (cyc)   : 1439.521\n",
      "  VAL  RMSE (norm)  : 0.122585\n",
      "  VAL  MAE  (norm)  : 0.094985\n",
      "  TEST RMSE (cyc)   : 1556.094\n",
      "  TEST MAE  (cyc)   : 971.670\n",
      "  TEST RMSE (norm)  : 0.148056\n",
      "  TEST MAE  (norm)  : 0.116003\n",
      "\n",
      "[SELECTED BY TEST] (for reporting only; not for tuning)\n",
      "  Seed             : 333\n",
      "  Checkpoint       : last_epoch\n",
      "  TEST RMSE (cyc)   : 1423.464\n",
      "  TEST MAE  (cyc)   : 879.313\n",
      "  TEST RMSE (norm)  : 0.140472\n",
      "  TEST MAE  (norm)  : 0.109212\n",
      "  VAL  RMSE (cyc)   : 2176.684\n",
      "  VAL  MAE  (cyc)   : 1462.151\n",
      "  VAL  RMSE (norm)  : 0.124601\n",
      "  VAL  MAE  (norm)  : 0.095972\n",
      "\n",
      "---------------- WIN-RATE (last_epoch vs best_by_val_norm) ----------------\n",
      "- val_rmse_cycles: last wins=0, best wins=5, ties=0 | mean(last-best)=235.338697\n",
      "- test_rmse_cycles: last wins=3, best wins=2, ties=0 | mean(last-best)=41.808385\n",
      "- val_rmse_norm: last wins=0, best wins=5, ties=0 | mean(last-best)=0.011238\n",
      "- test_rmse_norm: last wins=3, best wins=2, ties=0 | mean(last-best)=0.003569\n",
      "=====================================================\n",
      "\n",
      "Saved -> ./Trial13\\BEST_MODEL_BY_VAL.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================\n",
    "# Trial13 paths\n",
    "# ============================\n",
    "TRIAL13_DIR = \"./Trial13\"\n",
    "SUMMARY_CSV = os.path.join(TRIAL13_DIR, \"summary_across_seeds.csv\")\n",
    "\n",
    "BEST_TAG = \"best_by_val_norm\"\n",
    "LAST_TAG = \"last_epoch\"\n",
    "\n",
    "\n",
    "def _require_cols(df: pd.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in summary CSV: {missing}\")\n",
    "\n",
    "\n",
    "def pick_best_row(df: pd.DataFrame, metric_prefix: str = \"val\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    metric_prefix: \"val\" or \"test\"\n",
    "    Sort rule (lower is better):\n",
    "      1) <prefix>_rmse_cycles ascending\n",
    "      2) <prefix>_mae_cycles  ascending\n",
    "    \"\"\"\n",
    "    rmse_col = f\"{metric_prefix}_rmse_cycles\"\n",
    "    mae_col = f\"{metric_prefix}_mae_cycles\"\n",
    "    _require_cols(df, [\"seed\", \"checkpoint\", rmse_col, mae_col])\n",
    "\n",
    "    df_sorted = df.sort_values(\n",
    "        by=[rmse_col, mae_col],\n",
    "        ascending=[True, True]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return df_sorted.iloc[0]\n",
    "\n",
    "\n",
    "def win_rate(df: pd.DataFrame, metric: str) -> dict:\n",
    "    \"\"\"\n",
    "    Compare BEST_TAG vs LAST_TAG within each seed on the given metric (lower is better).\n",
    "    Returns wins for last, wins for best, ties, and mean(last-best).\n",
    "    \"\"\"\n",
    "    _require_cols(df, [\"seed\", \"checkpoint\", metric])\n",
    "\n",
    "    wins_last = 0\n",
    "    wins_best = 0\n",
    "    ties = 0\n",
    "    diffs = []\n",
    "\n",
    "    for seed, g in df.groupby(\"seed\"):\n",
    "        ckpts = set(g[\"checkpoint\"].astype(str).values)\n",
    "        if not ({BEST_TAG, LAST_TAG} <= ckpts):\n",
    "            continue\n",
    "\n",
    "        b = float(g.loc[g[\"checkpoint\"] == BEST_TAG, metric].iloc[0])\n",
    "        l = float(g.loc[g[\"checkpoint\"] == LAST_TAG, metric].iloc[0])\n",
    "\n",
    "        if np.isfinite(b) and np.isfinite(l):\n",
    "            diffs.append(l - b)  # negative => last better\n",
    "            if l < b:\n",
    "                wins_last += 1\n",
    "            elif b < l:\n",
    "                wins_best += 1\n",
    "            else:\n",
    "                ties += 1\n",
    "\n",
    "    return {\n",
    "        \"metric\": metric,\n",
    "        \"wins_last\": wins_last,\n",
    "        \"wins_best\": wins_best,\n",
    "        \"ties\": ties,\n",
    "        \"mean(last-best)\": float(np.mean(diffs)) if diffs else float(\"nan\"),\n",
    "        \"std(last-best)\": float(np.std(diffs)) if diffs else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(SUMMARY_CSV):\n",
    "        raise FileNotFoundError(f\"Not found: {SUMMARY_CSV}\")\n",
    "\n",
    "    df = pd.read_csv(SUMMARY_CSV)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Trial13 summary columns sanity check\n",
    "    # (matches ms dict in your Trial13 run_one_seed.export_ckpt)\n",
    "    # -----------------------------\n",
    "    needed = [\n",
    "        \"seed\", \"checkpoint\",\n",
    "        \"train_rmse_cycles\", \"train_mae_cycles\", \"train_rmse_norm\", \"train_mae_norm\",\n",
    "        \"val_rmse_cycles\", \"val_mae_cycles\", \"val_rmse_norm\", \"val_mae_norm\",\n",
    "        \"test_rmse_cycles\", \"test_mae_cycles\", \"test_rmse_norm\", \"test_mae_norm\",\n",
    "        \"stopped_epoch\", \"best_val_rmse_norm\",\n",
    "        \"alpha\", \"ph_consecutive_m\", \"rep_method\", \"lambdas\", \"lambda_to_plot\",\n",
    "        \"feature_dim\", \"features\",\n",
    "    ]\n",
    "    _require_cols(df, needed)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) VAL 기준 best (권장)\n",
    "    # -----------------------------\n",
    "    best_val = pick_best_row(df, metric_prefix=\"val\")\n",
    "    best_val_seed = int(best_val[\"seed\"])\n",
    "    best_val_ckpt = str(best_val[\"checkpoint\"])\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) TEST 기준 best (보고용)\n",
    "    # -----------------------------\n",
    "    best_test = pick_best_row(df, metric_prefix=\"test\")\n",
    "    best_test_seed = int(best_test[\"seed\"])\n",
    "    best_test_ckpt = str(best_test[\"checkpoint\"])\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) win-rate (seed별 last vs best 비교)\n",
    "    # -----------------------------\n",
    "    wr_val_rmse = win_rate(df, \"val_rmse_cycles\")\n",
    "    wr_test_rmse = win_rate(df, \"test_rmse_cycles\")\n",
    "    wr_val_rmse_norm = win_rate(df, \"val_rmse_norm\")\n",
    "    wr_test_rmse_norm = win_rate(df, \"test_rmse_norm\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4) 출력\n",
    "    # -----------------------------\n",
    "    print(\"\\n================ BEST MODEL (Trial13) ================\")\n",
    "    print(\"[SELECTED BY VAL]  (recommended for model selection)\")\n",
    "    print(f\"  Seed             : {best_val_seed}\")\n",
    "    print(f\"  Checkpoint       : {best_val_ckpt}\")\n",
    "    print(f\"  VAL  RMSE (cyc)   : {best_val['val_rmse_cycles']:.3f}\")\n",
    "    print(f\"  VAL  MAE  (cyc)   : {best_val['val_mae_cycles']:.3f}\")\n",
    "    print(f\"  VAL  RMSE (norm)  : {best_val['val_rmse_norm']:.6f}\")\n",
    "    print(f\"  VAL  MAE  (norm)  : {best_val['val_mae_norm']:.6f}\")\n",
    "    print(f\"  TEST RMSE (cyc)   : {best_val['test_rmse_cycles']:.3f}\")\n",
    "    print(f\"  TEST MAE  (cyc)   : {best_val['test_mae_cycles']:.3f}\")\n",
    "    print(f\"  TEST RMSE (norm)  : {best_val['test_rmse_norm']:.6f}\")\n",
    "    print(f\"  TEST MAE  (norm)  : {best_val['test_mae_norm']:.6f}\")\n",
    "\n",
    "    print(\"\\n[SELECTED BY TEST] (for reporting only; not for tuning)\")\n",
    "    print(f\"  Seed             : {best_test_seed}\")\n",
    "    print(f\"  Checkpoint       : {best_test_ckpt}\")\n",
    "    print(f\"  TEST RMSE (cyc)   : {best_test['test_rmse_cycles']:.3f}\")\n",
    "    print(f\"  TEST MAE  (cyc)   : {best_test['test_mae_cycles']:.3f}\")\n",
    "    print(f\"  TEST RMSE (norm)  : {best_test['test_rmse_norm']:.6f}\")\n",
    "    print(f\"  TEST MAE  (norm)  : {best_test['test_mae_norm']:.6f}\")\n",
    "    print(f\"  VAL  RMSE (cyc)   : {best_test['val_rmse_cycles']:.3f}\")\n",
    "    print(f\"  VAL  MAE  (cyc)   : {best_test['val_mae_cycles']:.3f}\")\n",
    "    print(f\"  VAL  RMSE (norm)  : {best_test['val_rmse_norm']:.6f}\")\n",
    "    print(f\"  VAL  MAE  (norm)  : {best_test['val_mae_norm']:.6f}\")\n",
    "\n",
    "    print(\"\\n---------------- WIN-RATE (last_epoch vs best_by_val_norm) ----------------\")\n",
    "    print(f\"- {wr_val_rmse['metric']}: last wins={wr_val_rmse['wins_last']}, \"\n",
    "          f\"best wins={wr_val_rmse['wins_best']}, ties={wr_val_rmse['ties']} | \"\n",
    "          f\"mean(last-best)={wr_val_rmse['mean(last-best)']:.6f}\")\n",
    "    print(f\"- {wr_test_rmse['metric']}: last wins={wr_test_rmse['wins_last']}, \"\n",
    "          f\"best wins={wr_test_rmse['wins_best']}, ties={wr_test_rmse['ties']} | \"\n",
    "          f\"mean(last-best)={wr_test_rmse['mean(last-best)']:.6f}\")\n",
    "    print(f\"- {wr_val_rmse_norm['metric']}: last wins={wr_val_rmse_norm['wins_last']}, \"\n",
    "          f\"best wins={wr_val_rmse_norm['wins_best']}, ties={wr_val_rmse_norm['ties']} | \"\n",
    "          f\"mean(last-best)={wr_val_rmse_norm['mean(last-best)']:.6f}\")\n",
    "    print(f\"- {wr_test_rmse_norm['metric']}: last wins={wr_test_rmse_norm['wins_last']}, \"\n",
    "          f\"best wins={wr_test_rmse_norm['wins_best']}, ties={wr_test_rmse_norm['ties']} | \"\n",
    "          f\"mean(last-best)={wr_test_rmse_norm['mean(last-best)']:.6f}\")\n",
    "    print(\"=====================================================\\n\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5) 기록 저장 (VAL 기준 best)\n",
    "    # -----------------------------\n",
    "    out_txt = os.path.join(TRIAL13_DIR, \"BEST_MODEL_BY_VAL.txt\")\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"BEST MODEL (Trial13) - Selected by VAL\\n\")\n",
    "        f.write(f\"seed={best_val_seed}\\n\")\n",
    "        f.write(f\"checkpoint={best_val_ckpt}\\n\")\n",
    "        f.write(f\"val_rmse_cycles={best_val['val_rmse_cycles']}\\n\")\n",
    "        f.write(f\"val_mae_cycles={best_val['val_mae_cycles']}\\n\")\n",
    "        f.write(f\"val_rmse_norm={best_val['val_rmse_norm']}\\n\")\n",
    "        f.write(f\"val_mae_norm={best_val['val_mae_norm']}\\n\")\n",
    "        f.write(f\"test_rmse_cycles={best_val['test_rmse_cycles']}\\n\")\n",
    "        f.write(f\"test_mae_cycles={best_val['test_mae_cycles']}\\n\")\n",
    "        f.write(f\"test_rmse_norm={best_val['test_rmse_norm']}\\n\")\n",
    "        f.write(f\"test_mae_norm={best_val['test_mae_norm']}\\n\")\n",
    "\n",
    "        # Trial13 run settings / provenance\n",
    "        for k in [\n",
    "            \"stopped_epoch\", \"best_val_rmse_norm\",\n",
    "            \"alpha\", \"ph_consecutive_m\", \"rep_method\", \"lambdas\", \"lambda_to_plot\",\n",
    "            \"feature_dim\", \"features\",\n",
    "        ]:\n",
    "            if k in best_val.index:\n",
    "                f.write(f\"{k}={best_val[k]}\\n\")\n",
    "\n",
    "    print(f\"Saved -> {out_txt}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecc2360d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ BEST MODEL (Trial13) ================\n",
      "[SELECTED BY VAL (norm)]  (recommended for model selection)\n",
      "  Seed             : 333\n",
      "  Checkpoint       : best_by_val_norm\n",
      "  VAL  RMSE (norm)  : 0.122585\n",
      "  VAL  MAE  (norm)  : 0.094985\n",
      "  VAL  RMSE (cyc)   : 2118.589\n",
      "  VAL  MAE  (cyc)   : 1439.521\n",
      "  TEST RMSE (norm)  : 0.148056\n",
      "  TEST MAE  (norm)  : 0.116003\n",
      "  TEST RMSE (cyc)   : 1556.094\n",
      "  TEST MAE  (cyc)   : 971.670\n",
      "\n",
      "[SELECTED BY TEST (norm)] (for reporting only; not for tuning)\n",
      "  Seed             : 9819123\n",
      "  Checkpoint       : best_by_val_norm\n",
      "  TEST RMSE (norm)  : 0.106261\n",
      "  TEST MAE  (norm)  : 0.080015\n",
      "  TEST RMSE (cyc)   : 1484.817\n",
      "  TEST MAE  (cyc)   : 1019.842\n",
      "  VAL  RMSE (norm)  : 0.150209\n",
      "  VAL  MAE  (norm)  : 0.124874\n",
      "  VAL  RMSE (cyc)   : 2808.762\n",
      "  VAL  MAE  (cyc)   : 2063.151\n",
      "\n",
      "---------------- WIN-RATE (last_epoch vs best_by_val_norm) ----------------\n",
      "- val_rmse_norm: last wins=0, best wins=5, ties=0 | mean(last-best)=0.011238\n",
      "- test_rmse_norm: last wins=3, best wins=2, ties=0 | mean(last-best)=0.003569\n",
      "- val_rmse_cycles: last wins=0, best wins=5, ties=0 | mean(last-best)=235.338697\n",
      "- test_rmse_cycles: last wins=3, best wins=2, ties=0 | mean(last-best)=41.808385\n",
      "=====================================================\n",
      "\n",
      "Saved -> ./Trial13\\BEST_MODEL_BY_VAL_NORM.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================\n",
    "# Trial13 paths\n",
    "# ============================\n",
    "TRIAL13_DIR = \"./Trial13\"\n",
    "SUMMARY_CSV = os.path.join(TRIAL13_DIR, \"summary_across_seeds.csv\")\n",
    "\n",
    "BEST_TAG = \"best_by_val_norm\"\n",
    "LAST_TAG = \"last_epoch\"\n",
    "\n",
    "\n",
    "def _require_cols(df: pd.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in summary CSV: {missing}\")\n",
    "\n",
    "\n",
    "def pick_best_row_by_norm(df: pd.DataFrame, metric_prefix: str = \"val\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    metric_prefix: \"val\" or \"test\"\n",
    "    Sort rule (lower is better):\n",
    "      1) <prefix>_rmse_norm   ascending\n",
    "      2) <prefix>_mae_norm    ascending\n",
    "      3) <prefix>_rmse_cycles ascending  (tie-break)\n",
    "      4) <prefix>_mae_cycles  ascending  (tie-break)\n",
    "    \"\"\"\n",
    "    rmse_norm = f\"{metric_prefix}_rmse_norm\"\n",
    "    mae_norm = f\"{metric_prefix}_mae_norm\"\n",
    "    rmse_cyc = f\"{metric_prefix}_rmse_cycles\"\n",
    "    mae_cyc = f\"{metric_prefix}_mae_cycles\"\n",
    "\n",
    "    _require_cols(df, [\"seed\", \"checkpoint\", rmse_norm, mae_norm, rmse_cyc, mae_cyc])\n",
    "\n",
    "    df_sorted = df.sort_values(\n",
    "        by=[rmse_norm, mae_norm, rmse_cyc, mae_cyc],\n",
    "        ascending=[True, True, True, True]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return df_sorted.iloc[0]\n",
    "\n",
    "\n",
    "def win_rate(df: pd.DataFrame, metric: str) -> dict:\n",
    "    \"\"\"\n",
    "    Compare BEST_TAG vs LAST_TAG within each seed on the given metric (lower is better).\n",
    "    Returns wins for last, wins for best, ties, and mean(last-best).\n",
    "    \"\"\"\n",
    "    _require_cols(df, [\"seed\", \"checkpoint\", metric])\n",
    "\n",
    "    wins_last = 0\n",
    "    wins_best = 0\n",
    "    ties = 0\n",
    "    diffs = []\n",
    "\n",
    "    for seed, g in df.groupby(\"seed\"):\n",
    "        ckpts = set(g[\"checkpoint\"].astype(str).values)\n",
    "        if not ({BEST_TAG, LAST_TAG} <= ckpts):\n",
    "            continue\n",
    "\n",
    "        b = float(g.loc[g[\"checkpoint\"] == BEST_TAG, metric].iloc[0])\n",
    "        l = float(g.loc[g[\"checkpoint\"] == LAST_TAG, metric].iloc[0])\n",
    "\n",
    "        if np.isfinite(b) and np.isfinite(l):\n",
    "            diffs.append(l - b)  # negative => last better\n",
    "            if l < b:\n",
    "                wins_last += 1\n",
    "            elif b < l:\n",
    "                wins_best += 1\n",
    "            else:\n",
    "                ties += 1\n",
    "\n",
    "    return {\n",
    "        \"metric\": metric,\n",
    "        \"wins_last\": wins_last,\n",
    "        \"wins_best\": wins_best,\n",
    "        \"ties\": ties,\n",
    "        \"mean(last-best)\": float(np.mean(diffs)) if diffs else float(\"nan\"),\n",
    "        \"std(last-best)\": float(np.std(diffs)) if diffs else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(SUMMARY_CSV):\n",
    "        raise FileNotFoundError(f\"Not found: {SUMMARY_CSV}\")\n",
    "\n",
    "    df = pd.read_csv(SUMMARY_CSV)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Trial13 summary columns sanity check\n",
    "    # -----------------------------\n",
    "    needed = [\n",
    "        \"seed\", \"checkpoint\",\n",
    "        \"train_rmse_cycles\", \"train_mae_cycles\", \"train_rmse_norm\", \"train_mae_norm\",\n",
    "        \"val_rmse_cycles\", \"val_mae_cycles\", \"val_rmse_norm\", \"val_mae_norm\",\n",
    "        \"test_rmse_cycles\", \"test_mae_cycles\", \"test_rmse_norm\", \"test_mae_norm\",\n",
    "        \"stopped_epoch\", \"best_val_rmse_norm\",\n",
    "        \"alpha\", \"ph_consecutive_m\", \"rep_method\", \"lambdas\", \"lambda_to_plot\",\n",
    "        \"feature_dim\", \"features\",\n",
    "    ]\n",
    "    _require_cols(df, needed)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) VAL 기준 best (norm metric 기준) ✅\n",
    "    # -----------------------------\n",
    "    best_val = pick_best_row_by_norm(df, metric_prefix=\"val\")\n",
    "    best_val_seed = int(best_val[\"seed\"])\n",
    "    best_val_ckpt = str(best_val[\"checkpoint\"])\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) TEST 기준 best (norm metric 기준) (보고용)\n",
    "    # -----------------------------\n",
    "    best_test = pick_best_row_by_norm(df, metric_prefix=\"test\")\n",
    "    best_test_seed = int(best_test[\"seed\"])\n",
    "    best_test_ckpt = str(best_test[\"checkpoint\"])\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) win-rate (seed별 last vs best 비교)\n",
    "    # -----------------------------\n",
    "    wr_val_rmse = win_rate(df, \"val_rmse_cycles\")\n",
    "    wr_test_rmse = win_rate(df, \"test_rmse_cycles\")\n",
    "    wr_val_rmse_norm = win_rate(df, \"val_rmse_norm\")\n",
    "    wr_test_rmse_norm = win_rate(df, \"test_rmse_norm\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4) 출력\n",
    "    # -----------------------------\n",
    "    print(\"\\n================ BEST MODEL (Trial13) ================\")\n",
    "    print(\"[SELECTED BY VAL (norm)]  (recommended for model selection)\")\n",
    "    print(f\"  Seed             : {best_val_seed}\")\n",
    "    print(f\"  Checkpoint       : {best_val_ckpt}\")\n",
    "    print(f\"  VAL  RMSE (norm)  : {best_val['val_rmse_norm']:.6f}\")\n",
    "    print(f\"  VAL  MAE  (norm)  : {best_val['val_mae_norm']:.6f}\")\n",
    "    print(f\"  VAL  RMSE (cyc)   : {best_val['val_rmse_cycles']:.3f}\")\n",
    "    print(f\"  VAL  MAE  (cyc)   : {best_val['val_mae_cycles']:.3f}\")\n",
    "    print(f\"  TEST RMSE (norm)  : {best_val['test_rmse_norm']:.6f}\")\n",
    "    print(f\"  TEST MAE  (norm)  : {best_val['test_mae_norm']:.6f}\")\n",
    "    print(f\"  TEST RMSE (cyc)   : {best_val['test_rmse_cycles']:.3f}\")\n",
    "    print(f\"  TEST MAE  (cyc)   : {best_val['test_mae_cycles']:.3f}\")\n",
    "\n",
    "    print(\"\\n[SELECTED BY TEST (norm)] (for reporting only; not for tuning)\")\n",
    "    print(f\"  Seed             : {best_test_seed}\")\n",
    "    print(f\"  Checkpoint       : {best_test_ckpt}\")\n",
    "    print(f\"  TEST RMSE (norm)  : {best_test['test_rmse_norm']:.6f}\")\n",
    "    print(f\"  TEST MAE  (norm)  : {best_test['test_mae_norm']:.6f}\")\n",
    "    print(f\"  TEST RMSE (cyc)   : {best_test['test_rmse_cycles']:.3f}\")\n",
    "    print(f\"  TEST MAE  (cyc)   : {best_test['test_mae_cycles']:.3f}\")\n",
    "    print(f\"  VAL  RMSE (norm)  : {best_test['val_rmse_norm']:.6f}\")\n",
    "    print(f\"  VAL  MAE  (norm)  : {best_test['val_mae_norm']:.6f}\")\n",
    "    print(f\"  VAL  RMSE (cyc)   : {best_test['val_rmse_cycles']:.3f}\")\n",
    "    print(f\"  VAL  MAE  (cyc)   : {best_test['val_mae_cycles']:.3f}\")\n",
    "\n",
    "    print(\"\\n---------------- WIN-RATE (last_epoch vs best_by_val_norm) ----------------\")\n",
    "    print(f\"- {wr_val_rmse_norm['metric']}: last wins={wr_val_rmse_norm['wins_last']}, \"\n",
    "          f\"best wins={wr_val_rmse_norm['wins_best']}, ties={wr_val_rmse_norm['ties']} | \"\n",
    "          f\"mean(last-best)={wr_val_rmse_norm['mean(last-best)']:.6f}\")\n",
    "    print(f\"- {wr_test_rmse_norm['metric']}: last wins={wr_test_rmse_norm['wins_last']}, \"\n",
    "          f\"best wins={wr_test_rmse_norm['wins_best']}, ties={wr_test_rmse_norm['ties']} | \"\n",
    "          f\"mean(last-best)={wr_test_rmse_norm['mean(last-best)']:.6f}\")\n",
    "    print(f\"- {wr_val_rmse['metric']}: last wins={wr_val_rmse['wins_last']}, \"\n",
    "          f\"best wins={wr_val_rmse['wins_best']}, ties={wr_val_rmse['ties']} | \"\n",
    "          f\"mean(last-best)={wr_val_rmse['mean(last-best)']:.6f}\")\n",
    "    print(f\"- {wr_test_rmse['metric']}: last wins={wr_test_rmse['wins_last']}, \"\n",
    "          f\"best wins={wr_test_rmse['wins_best']}, ties={wr_test_rmse['ties']} | \"\n",
    "          f\"mean(last-best)={wr_test_rmse['mean(last-best)']:.6f}\")\n",
    "    print(\"=====================================================\\n\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5) 기록 저장 (VAL(norm) 기준 best)\n",
    "    # -----------------------------\n",
    "    out_txt = os.path.join(TRIAL13_DIR, \"BEST_MODEL_BY_VAL_NORM.txt\")\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"BEST MODEL (Trial13) - Selected by VAL (norm)\\n\")\n",
    "        f.write(f\"seed={best_val_seed}\\n\")\n",
    "        f.write(f\"checkpoint={best_val_ckpt}\\n\")\n",
    "        f.write(f\"val_rmse_norm={best_val['val_rmse_norm']}\\n\")\n",
    "        f.write(f\"val_mae_norm={best_val['val_mae_norm']}\\n\")\n",
    "        f.write(f\"val_rmse_cycles={best_val['val_rmse_cycles']}\\n\")\n",
    "        f.write(f\"val_mae_cycles={best_val['val_mae_cycles']}\\n\")\n",
    "        f.write(f\"test_rmse_norm={best_val['test_rmse_norm']}\\n\")\n",
    "        f.write(f\"test_mae_norm={best_val['test_mae_norm']}\\n\")\n",
    "        f.write(f\"test_rmse_cycles={best_val['test_rmse_cycles']}\\n\")\n",
    "        f.write(f\"test_mae_cycles={best_val['test_mae_cycles']}\\n\")\n",
    "\n",
    "        for k in [\n",
    "            \"stopped_epoch\", \"best_val_rmse_norm\",\n",
    "            \"alpha\", \"ph_consecutive_m\", \"rep_method\", \"lambdas\", \"lambda_to_plot\",\n",
    "            \"feature_dim\", \"features\",\n",
    "        ]:\n",
    "            if k in best_val.index:\n",
    "                f.write(f\"{k}={best_val[k]}\\n\")\n",
    "\n",
    "    print(f\"Saved -> {out_txt}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f019e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] train: λ=0.20:0.700, λ=0.40:0.629, λ=0.60:0.557, λ=0.80:0.443 | mean_all=0.582\n",
      "[OK] val: λ=0.20:0.600, λ=0.40:0.700, λ=0.60:0.700, λ=0.80:0.450 | mean_all=0.612\n",
      "[OK] test: λ=0.20:0.800, λ=0.40:0.700, λ=0.60:0.800, λ=0.80:0.600 | mean_all=0.725\n",
      "\n",
      "==================== DONE ====================\n",
      "Saved:\n",
      " - ./Trial13\\seed_333\\best_by_val_norm\\alpha_lambda_eval\\alpha_lambda_summary_seed333_best_by_val_norm.csv\n",
      " - ./Trial13\\seed_333\\best_by_val_norm\\alpha_lambda_eval\\alpha_lambda_per_file_seed333_best_by_val_norm.csv\n",
      "==============================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# USER CONFIG (Trial13)\n",
    "# =========================\n",
    "TRIAL13_DIR = r\"./Trial13\"          # ✅ Trial13 루트 폴더\n",
    "SEED = 333                          # 선택된 seed\n",
    "CKPT = \"best_by_val_norm\"           # \"best_by_val_norm\" or \"last_epoch\"\n",
    "SPLITS = [\"train\", \"val\", \"test\"]   # 평가할 split\n",
    "LAM_STRS = [\"0.20\", \"0.40\", \"0.60\", \"0.80\"]\n",
    "\n",
    "# (선택) 후반 λ를 더 중요하게 보고 싶으면 가중치 사용\n",
    "# 예: λ=0.2,0.4,0.6,0.8 가중치 = 1,1,2,3\n",
    "LAMBDA_WEIGHTS = None  # 또는 {\"0.20\":1, \"0.40\":1, \"0.60\":2, \"0.80\":3}\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def _require_cols(df: pd.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "\n",
    "def compute_alpha_lambda_rates(dfm: pd.DataFrame, lam_strs, weights=None) -> dict:\n",
    "    \"\"\"\n",
    "    dfm: <split>_prognostics_metrics_per_file.csv  (per-file summary)\n",
    "    Returns:\n",
    "      - per-lambda success rate (mean of alpha_lambda_ok_{lam})\n",
    "      - overall mean rate (simple mean or weighted mean)\n",
    "    \"\"\"\n",
    "    rates = {}\n",
    "\n",
    "    for ls in lam_strs:\n",
    "        col = f\"alpha_lambda_ok_{ls}\"\n",
    "        if col in dfm.columns:\n",
    "            rates[f\"rate_{ls}\"] = float(dfm[col].mean())  # 0/1 평균 = 성공률\n",
    "        else:\n",
    "            rates[f\"rate_{ls}\"] = np.nan\n",
    "\n",
    "    # overall score\n",
    "    if weights is None:\n",
    "        vals = [rates[f\"rate_{ls}\"] for ls in lam_strs if np.isfinite(rates[f\"rate_{ls}\"])]\n",
    "        rates[\"rate_mean_all\"] = float(np.mean(vals)) if vals else np.nan\n",
    "    else:\n",
    "        num = 0.0\n",
    "        den = 0.0\n",
    "        for ls in lam_strs:\n",
    "            v = rates[f\"rate_{ls}\"]\n",
    "            w = float(weights.get(ls, 0.0))\n",
    "            if np.isfinite(v) and w > 0:\n",
    "                num += w * v\n",
    "                den += w\n",
    "        rates[\"rate_weighted_all\"] = (num / den) if den > 0 else np.nan\n",
    "\n",
    "    return rates\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Trial13 구조: ./Trial13/seed_<seed>/<ckpt>/\n",
    "    seed_dir = os.path.join(TRIAL13_DIR, f\"seed_{SEED}\", CKPT)\n",
    "    if not os.path.isdir(seed_dir):\n",
    "        raise FileNotFoundError(f\"Not found: {seed_dir}\")\n",
    "\n",
    "    out_dir = os.path.join(seed_dir, \"alpha_lambda_eval\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    summary_rows = []\n",
    "    per_file_rows = []  # (선택) 파일별 ok 값도 모아서 저장\n",
    "\n",
    "    for split in SPLITS:\n",
    "        mpath = os.path.join(seed_dir, f\"{split}_prognostics_metrics_per_file.csv\")\n",
    "        if not os.path.exists(mpath):\n",
    "            print(f\"[SKIP] Missing: {mpath}\")\n",
    "            continue\n",
    "\n",
    "        dfm = pd.read_csv(mpath)\n",
    "        _require_cols(dfm, [\"file\"])  # 최소 file은 있어야 함\n",
    "\n",
    "        # split 요약(성공률)\n",
    "        rates = compute_alpha_lambda_rates(dfm, LAM_STRS, weights=LAMBDA_WEIGHTS)\n",
    "        row = {\n",
    "            \"seed\": SEED,\n",
    "            \"checkpoint\": CKPT,\n",
    "            \"split\": split,\n",
    "            \"n_files\": int(len(dfm)),\n",
    "            **rates\n",
    "        }\n",
    "        summary_rows.append(row)\n",
    "\n",
    "        # (선택) 파일별 pass/fail + t_lambda 저장\n",
    "        keep_cols = [\"file\"]\n",
    "        for ls in LAM_STRS:\n",
    "            c_ok = f\"alpha_lambda_ok_{ls}\"\n",
    "            c_tl = f\"t_lambda_{ls}\"\n",
    "            if c_ok in dfm.columns:\n",
    "                keep_cols.append(c_ok)\n",
    "            if c_tl in dfm.columns:\n",
    "                keep_cols.append(c_tl)\n",
    "\n",
    "        sub = dfm[keep_cols].copy()\n",
    "        sub.insert(0, \"split\", split)\n",
    "        sub.insert(0, \"checkpoint\", CKPT)\n",
    "        sub.insert(0, \"seed\", SEED)\n",
    "        per_file_rows.append(sub)\n",
    "\n",
    "        # 콘솔 출력\n",
    "        msg_parts = []\n",
    "        for ls in LAM_STRS:\n",
    "            v = row.get(f\"rate_{ls}\", np.nan)\n",
    "            if np.isfinite(v):\n",
    "                msg_parts.append(f\"λ={ls}:{v:.3f}\")\n",
    "        msg = \", \".join(msg_parts) if msg_parts else \"no lambda columns found\"\n",
    "        tail = \"\"\n",
    "        if \"rate_weighted_all\" in row and np.isfinite(row[\"rate_weighted_all\"]):\n",
    "            tail = f\" | weighted_all={row['rate_weighted_all']:.3f}\"\n",
    "        elif \"rate_mean_all\" in row and np.isfinite(row[\"rate_mean_all\"]):\n",
    "            tail = f\" | mean_all={row['rate_mean_all']:.3f}\"\n",
    "        print(f\"[OK] {split}: {msg}{tail}\")\n",
    "\n",
    "    # 저장: split별 요약\n",
    "    df_summary = pd.DataFrame(summary_rows)\n",
    "    out_summary = os.path.join(out_dir, f\"alpha_lambda_summary_seed{SEED}_{CKPT}.csv\")\n",
    "    df_summary.to_csv(out_summary, index=False)\n",
    "\n",
    "    # 저장: 파일별 pass/fail (선택)\n",
    "    out_pf = None\n",
    "    if per_file_rows:\n",
    "        df_pf = pd.concat(per_file_rows, axis=0, ignore_index=True)\n",
    "        out_pf = os.path.join(out_dir, f\"alpha_lambda_per_file_seed{SEED}_{CKPT}.csv\")\n",
    "        df_pf.to_csv(out_pf, index=False)\n",
    "\n",
    "    print(\"\\n==================== DONE ====================\")\n",
    "    print(\"Saved:\")\n",
    "    print(\" -\", out_summary)\n",
    "    if out_pf:\n",
    "        print(\" -\", out_pf)\n",
    "    print(\"==============================================\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e69bd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] ./Trial13\\seed_333\\best_by_val_norm\\alpha_lambda_eval\\alpha_lambda_curve_seed333_best_by_val_norm.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# USER CONFIG (Trial13)\n",
    "# =========================\n",
    "TRIAL13_DIR = r\"./Trial13\"\n",
    "SEED = 333\n",
    "CKPT = \"best_by_val_norm\"   # or \"last_epoch\"\n",
    "\n",
    "LAM_STRS = [\"0.20\", \"0.40\", \"0.60\", \"0.80\"]\n",
    "LAM = [float(x) for x in LAM_STRS]\n",
    "SPLITS_ORDER = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# =========================\n",
    "# Paths\n",
    "# =========================\n",
    "AL_DIR = os.path.join(\n",
    "    TRIAL13_DIR, f\"seed_{SEED}\", CKPT, \"alpha_lambda_eval\"\n",
    ")\n",
    "SUMMARY_CSV = os.path.join(\n",
    "    AL_DIR, f\"alpha_lambda_summary_seed{SEED}_{CKPT}.csv\"\n",
    ")\n",
    "OUT_PNG = os.path.join(\n",
    "    AL_DIR, f\"alpha_lambda_curve_seed{SEED}_{CKPT}.png\"\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Load\n",
    "# =========================\n",
    "if not os.path.exists(SUMMARY_CSV):\n",
    "    raise FileNotFoundError(f\"Not found: {SUMMARY_CSV}\")\n",
    "\n",
    "df = pd.read_csv(SUMMARY_CSV)\n",
    "\n",
    "# =========================\n",
    "# Plot\n",
    "# =========================\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "for split in SPLITS_ORDER:\n",
    "    sub = df[df[\"split\"] == split]\n",
    "    if sub.empty:\n",
    "        print(f\"[SKIP] no data for split={split}\")\n",
    "        continue\n",
    "\n",
    "    row = sub.iloc[0]\n",
    "    rates = []\n",
    "    for ls in LAM_STRS:\n",
    "        v = row.get(f\"rate_{ls}\", np.nan)\n",
    "        rates.append(float(v) if np.isfinite(v) else np.nan)\n",
    "\n",
    "    plt.plot(\n",
    "        LAM,\n",
    "        rates,\n",
    "        marker=\"o\",\n",
    "        linewidth=2,\n",
    "        label=split\n",
    "    )\n",
    "\n",
    "plt.xticks(LAM, [f\"{x:.2f}\" for x in LAM])\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.xlabel(\"λ (life fraction)\")\n",
    "plt.ylabel(\"α–λ success rate\")\n",
    "plt.title(f\"α–λ Success Rate Curve\\nSeed {SEED} | {CKPT} | Trial13\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# =========================\n",
    "# Save\n",
    "# =========================\n",
    "os.makedirs(os.path.dirname(OUT_PNG), exist_ok=True)\n",
    "plt.savefig(OUT_PNG, dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"[SAVE] {OUT_PNG}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3515342c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] DONE -> ./Trial13\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\\train\n",
      "[val] DONE -> ./Trial13\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\\val\n",
      "[test] DONE -> ./Trial13\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\\test\n",
      "\n",
      "ALL DONE.\n",
      "Saved under: ./Trial13\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================\n",
    "# USER CONFIG\n",
    "# ============================================================\n",
    "TRIAL_DIR = r\"./Trial13\"              # Trial13 폴더\n",
    "SEED = 333                           # seed 선택\n",
    "CKPT = \"best_by_val_norm\"            # \"best_by_val_norm\" or \"last_epoch\"\n",
    "\n",
    "SPLITS = [\"train\", \"val\", \"test\"]    # ✅ 여러 split 한 번에\n",
    "\n",
    "ALPHA = 0.20\n",
    "SEQ_LEN = 100                        # (참고) eval 구간에서 t_s=seq_len-1을 이미 metrics가 갖고 있어 직접 쓰진 않음\n",
    "LAMBDA_TO_PLOT = 0.60                # α–λ 그림에 표시할 λ\n",
    "\n",
    "MAX_FILES = None                     # None=모두, 아니면 예: 10\n",
    "\n",
    "# 저장 폴더 루트\n",
    "OUT_ROOT = os.path.join(TRIAL_DIR, f\"seed_{SEED}\", CKPT, \"paper_figures_bookstyle\")\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "def safe_name(s: str) -> str:\n",
    "    return s.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\")\n",
    "\n",
    "def load_cycle_seq_and_metrics(seed_dir: str, split: str):\n",
    "    \"\"\"\n",
    "    Trial9가 이미 만들어 둔 파일들:\n",
    "      - <split>_cycle_sequence_mean.csv\n",
    "      - <split>_prognostics_metrics_per_file.csv\n",
    "    \"\"\"\n",
    "    seq_csv = os.path.join(seed_dir, f\"{split}_cycle_sequence_mean.csv\")\n",
    "    met_csv = os.path.join(seed_dir, f\"{split}_prognostics_metrics_per_file.csv\")\n",
    "\n",
    "    if not os.path.exists(seq_csv):\n",
    "        raise FileNotFoundError(f\"Missing: {seq_csv}\")\n",
    "    if not os.path.exists(met_csv):\n",
    "        raise FileNotFoundError(f\"Missing: {met_csv}\")\n",
    "\n",
    "    df_seq = pd.read_csv(seq_csv)\n",
    "    df_met = pd.read_csv(met_csv)\n",
    "    return df_seq, df_met\n",
    "\n",
    "def get_eval_segment(df_one_file: pd.DataFrame, t_s: int, t_e: int) -> pd.DataFrame:\n",
    "    df = df_one_file.sort_values(\"cycle\").copy()\n",
    "    df = df[(df[\"cycle\"] >= t_s) & (df[\"cycle\"] <= t_e)].copy()\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "# ============================================================\n",
    "# Plotters\n",
    "# ============================================================\n",
    "def plot_ph_alpha_absolute_band(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    alpha: float,\n",
    "    out_path: str,\n",
    "    ph_start: Optional[float] = None,\n",
    "    title_prefix: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    BOOK Fig.2.9(a) 스타일: PH용 α-zone은 '절대 폭(평행 밴드)'\n",
    "      alphaZone = alpha * EOL_true\n",
    "      zone = RUL_true ± alphaZone\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    x = df_eval[\"cycle\"].to_numpy()\n",
    "    y_true = df_eval[\"RUL_true\"].to_numpy()\n",
    "    y_pred = df_eval[\"RUL_pred\"].to_numpy()\n",
    "\n",
    "    last_cycle = int(df_eval[\"cycle\"].max())\n",
    "    eol_true = last_cycle + 1\n",
    "\n",
    "    alpha_zone = alpha * float(eol_true)  # ✅ book-style 핵심 (평행 밴드)\n",
    "    upper = y_true + alpha_zone\n",
    "    lower = y_true - alpha_zone\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y_true, \"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, \"r\", label=\"Prediction (cycles)\")\n",
    "    plt.plot(x, upper, \"b--\", label=f\"+{alpha:.2f} α-zone (±α·EOL)\")\n",
    "    plt.plot(x, lower, \"b--\", label=f\"-{alpha:.2f} α-zone (±α·EOL)\")\n",
    "\n",
    "    if ph_start is not None and np.isfinite(ph_start):\n",
    "        plt.axvline(int(ph_start), color=\"g\", linestyle=\"-.\", label=\"PH start\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title_prefix} | BOOK-STYLE α+PH (absolute band)\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_alpha_lambda_relative_band(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    alpha: float,\n",
    "    lambda_to_plot: float,\n",
    "    t_lambda: Optional[int],\n",
    "    out_path: str,\n",
    "    title_prefix: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    BOOK Fig.2.9(b) 스타일: α–λ는 '상대 폭(수렴 밴드)'\n",
    "      zone = RUL_true*(1±alpha), 그리고 t >= t_lambda 구간만 표시\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    x = df_eval[\"cycle\"].to_numpy()\n",
    "    y_true = df_eval[\"RUL_true\"].to_numpy()\n",
    "    y_pred = df_eval[\"RUL_pred\"].to_numpy()\n",
    "\n",
    "    upper = y_true * (1.0 + alpha)\n",
    "    lower = y_true * (1.0 - alpha)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y_true, \"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, \"r\", label=\"Prediction (cycles)\")\n",
    "\n",
    "    if t_lambda is not None and np.isfinite(t_lambda):\n",
    "        t_lambda = int(t_lambda)\n",
    "        plt.axvline(t_lambda, color=\"g\", linestyle=\":\", label=f\"t_λ (λ={lambda_to_plot:.2f})\")\n",
    "\n",
    "        mask = x >= t_lambda\n",
    "        if np.any(mask):\n",
    "            plt.plot(x[mask], upper[mask], \"b--\", label=f\"+{alpha:.2f} α–λ zone\")\n",
    "            plt.plot(x[mask], lower[mask], \"b--\", label=f\"-{alpha:.2f} α–λ zone\")\n",
    "        else:\n",
    "            plt.plot(x, upper, \"b--\", label=f\"+{alpha:.2f} α zone\")\n",
    "            plt.plot(x, lower, \"b--\", label=f\"-{alpha:.2f} α zone\")\n",
    "    else:\n",
    "        plt.plot(x, upper, \"b--\", label=f\"+{alpha:.2f} α zone\")\n",
    "        plt.plot(x, lower, \"b--\", label=f\"-{alpha:.2f} α zone\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title_prefix} | BOOK-STYLE α–λ (relative band)\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# Main (multi-split)\n",
    "# ============================================================\n",
    "def run_for_one_split(seed_dir: str, split: str):\n",
    "    df_seq, df_met = load_cycle_seq_and_metrics(seed_dir, split)\n",
    "\n",
    "    files = df_seq[\"file\"].unique().tolist()\n",
    "    if MAX_FILES is not None:\n",
    "        files = files[:MAX_FILES]\n",
    "\n",
    "    out_dir = os.path.join(OUT_ROOT, split)  # ✅ split별 폴더\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    lam_key = f\"t_lambda_{LAMBDA_TO_PLOT:.2f}\"\n",
    "    title_prefix = f\"SEED {SEED} | {CKPT.upper()} | {split}\"\n",
    "\n",
    "    for f in files:\n",
    "        sub = df_seq[df_seq[\"file\"] == f].copy()\n",
    "        mrow = df_met[df_met[\"file\"] == f]\n",
    "        if mrow.empty:\n",
    "            continue\n",
    "        mrow = mrow.iloc[0].to_dict()\n",
    "\n",
    "        t_s = int(mrow[\"t_s\"])\n",
    "        t_e = int(mrow[\"t_e\"])\n",
    "        ph_start = mrow.get(\"t_PH_start\", np.nan)\n",
    "        t_lambda = mrow.get(lam_key, np.nan)\n",
    "\n",
    "        df_eval = get_eval_segment(sub, t_s, t_e)\n",
    "        if df_eval.empty:\n",
    "            continue\n",
    "\n",
    "        sname = safe_name(f)\n",
    "\n",
    "        out_a = os.path.join(out_dir, f\"FIG_A_BOOKSTYLE_alpha_PH__{sname}.png\")\n",
    "        plot_ph_alpha_absolute_band(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            alpha=ALPHA,\n",
    "            out_path=out_a,\n",
    "            ph_start=ph_start if np.isfinite(ph_start) else None,\n",
    "            title_prefix=title_prefix,\n",
    "        )\n",
    "\n",
    "        out_b = os.path.join(out_dir, f\"FIG_B_BOOKSTYLE_alpha_lambda__lam{LAMBDA_TO_PLOT:.2f}__{sname}.png\")\n",
    "        plot_alpha_lambda_relative_band(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            alpha=ALPHA,\n",
    "            lambda_to_plot=LAMBDA_TO_PLOT,\n",
    "            t_lambda=int(t_lambda) if np.isfinite(t_lambda) else None,\n",
    "            out_path=out_b,\n",
    "            title_prefix=title_prefix,\n",
    "        )\n",
    "\n",
    "    print(f\"[{split}] DONE -> {out_dir}\")\n",
    "\n",
    "def main():\n",
    "    seed_dir = os.path.join(TRIAL_DIR, f\"seed_{SEED}\", CKPT)\n",
    "\n",
    "    for split in SPLITS:\n",
    "        run_for_one_split(seed_dir, split)\n",
    "\n",
    "    print(\"\\nALL DONE.\")\n",
    "    print(\"Saved under:\", OUT_ROOT)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019da075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] DONE -> ./Trial13\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\\train\n",
      "[val] DONE -> ./Trial13\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\\val\n",
      "[test] DONE -> ./Trial13\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\\test\n",
      "\n",
      "ALL DONE.\n",
      "Saved under: ./Trial13\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================\n",
    "# USER CONFIG (Trial13)\n",
    "# ============================================================\n",
    "TRIAL_DIR = r\"./Trial13\"             # ✅ Trial13 루트 폴더\n",
    "SEED = 333                           # seed 선택\n",
    "CKPT = \"best_by_val_norm\"            # \"best_by_val_norm\" or \"last_epoch\"\n",
    "\n",
    "SPLITS = [\"train\", \"val\", \"test\"]    # ✅ 여러 split 한 번에\n",
    "\n",
    "ALPHA = 0.20\n",
    "LAMBDA_TO_PLOT = 0.60                # α–λ 그림에 표시할 λ\n",
    "MAX_FILES = None                     # None=모두, 아니면 예: 10\n",
    "\n",
    "# ============================================================\n",
    "# Paths\n",
    "# Trial13 구조:\n",
    "#   ./Trial13/seed_<seed>/<ckpt>/\n",
    "#       train_cycle_sequence_mean.csv\n",
    "#       train_prognostics_metrics_per_file.csv\n",
    "#       ...\n",
    "# ============================================================\n",
    "SEED_DIR = os.path.join(TRIAL_DIR, f\"seed_{SEED}\", CKPT)\n",
    "\n",
    "# 저장 폴더 루트 (원하면 이름 바꿔도 됨)\n",
    "OUT_ROOT = os.path.join(SEED_DIR, \"paper_figures_bookstyle\")\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "def safe_name(s: str) -> str:\n",
    "    return s.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\")\n",
    "\n",
    "\n",
    "def load_cycle_seq_and_metrics(seed_dir: str, split: str):\n",
    "    \"\"\"\n",
    "    Trial12가 만들어 둔 파일들:\n",
    "      - <split>_cycle_sequence_mean.csv\n",
    "      - <split>_prognostics_metrics_per_file.csv\n",
    "    (Trial12 export_ckpt()가 sub_dir(=seed_dir/tag) 아래에 저장함)\n",
    "    \"\"\"\n",
    "    seq_csv = os.path.join(seed_dir, f\"{split}_cycle_sequence_mean.csv\")\n",
    "    met_csv = os.path.join(seed_dir, f\"{split}_prognostics_metrics_per_file.csv\")\n",
    "\n",
    "    if not os.path.exists(seq_csv):\n",
    "        raise FileNotFoundError(f\"Missing: {seq_csv}\")\n",
    "    if not os.path.exists(met_csv):\n",
    "        raise FileNotFoundError(f\"Missing: {met_csv}\")\n",
    "\n",
    "    df_seq = pd.read_csv(seq_csv)\n",
    "    df_met = pd.read_csv(met_csv)\n",
    "    return df_seq, df_met\n",
    "\n",
    "\n",
    "def get_eval_segment(df_one_file: pd.DataFrame, t_s: int, t_e: int) -> pd.DataFrame:\n",
    "    df = df_one_file.sort_values(\"cycle\").copy()\n",
    "    df = df[(df[\"cycle\"] >= t_s) & (df[\"cycle\"] <= t_e)].copy()\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Plotters\n",
    "# ============================================================\n",
    "def plot_ph_alpha_absolute_band(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    alpha: float,\n",
    "    out_path: str,\n",
    "    ph_start: Optional[float] = None,\n",
    "    title_prefix: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    BOOK Fig.2.9(a) 스타일: PH용 α-zone은 '절대 폭(평행 밴드)'\n",
    "      alphaZone = alpha * EOL_true\n",
    "      zone = RUL_true ± alphaZone\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    x = df_eval[\"cycle\"].to_numpy()\n",
    "    y_true = df_eval[\"RUL_true\"].to_numpy()\n",
    "    y_pred = df_eval[\"RUL_pred\"].to_numpy()\n",
    "\n",
    "    last_cycle = int(df_eval[\"cycle\"].max())\n",
    "    eol_true = last_cycle + 1\n",
    "\n",
    "    alpha_zone = alpha * float(eol_true)  # ✅ book-style 핵심 (평행 밴드)\n",
    "    upper = y_true + alpha_zone\n",
    "    lower = y_true - alpha_zone\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y_true, \"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, \"r\", label=\"Prediction (cycles)\")\n",
    "    plt.plot(x, upper, \"b--\", label=f\"+{alpha:.2f} α-zone (±α·EOL)\")\n",
    "    plt.plot(x, lower, \"b--\", label=f\"-{alpha:.2f} α-zone (±α·EOL)\")\n",
    "\n",
    "    if ph_start is not None and np.isfinite(ph_start):\n",
    "        plt.axvline(int(ph_start), color=\"g\", linestyle=\"-.\", label=\"PH start\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title_prefix} | BOOK-STYLE α+PH (absolute band)\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_alpha_lambda_relative_band(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    alpha: float,\n",
    "    lambda_to_plot: float,\n",
    "    t_lambda: Optional[int],\n",
    "    out_path: str,\n",
    "    title_prefix: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    BOOK Fig.2.9(b) 스타일: α–λ는 '상대 폭(수렴 밴드)'\n",
    "      zone = RUL_true*(1±alpha), 그리고 t >= t_lambda 구간만 표시\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    x = df_eval[\"cycle\"].to_numpy()\n",
    "    y_true = df_eval[\"RUL_true\"].to_numpy()\n",
    "    y_pred = df_eval[\"RUL_pred\"].to_numpy()\n",
    "\n",
    "    upper = y_true * (1.0 + alpha)\n",
    "    lower = y_true * (1.0 - alpha)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y_true, \"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, \"r\", label=\"Prediction (cycles)\")\n",
    "\n",
    "    if t_lambda is not None and np.isfinite(t_lambda):\n",
    "        t_lambda = int(t_lambda)\n",
    "        plt.axvline(t_lambda, color=\"g\", linestyle=\":\", label=f\"t_λ (λ={lambda_to_plot:.2f})\")\n",
    "\n",
    "        mask = x >= t_lambda\n",
    "        if np.any(mask):\n",
    "            plt.plot(x[mask], upper[mask], \"b--\", label=f\"+{alpha:.2f} α–λ zone\")\n",
    "            plt.plot(x[mask], lower[mask], \"b--\", label=f\"-{alpha:.2f} α–λ zone\")\n",
    "        else:\n",
    "            plt.plot(x, upper, \"b--\", label=f\"+{alpha:.2f} α zone\")\n",
    "            plt.plot(x, lower, \"b--\", label=f\"-{alpha:.2f} α zone\")\n",
    "    else:\n",
    "        plt.plot(x, upper, \"b--\", label=f\"+{alpha:.2f} α zone\")\n",
    "        plt.plot(x, lower, \"b--\", label=f\"-{alpha:.2f} α zone\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title_prefix} | BOOK-STYLE α–λ (relative band)\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Main (multi-split)\n",
    "# ============================================================\n",
    "def run_for_one_split(seed_dir: str, split: str):\n",
    "    df_seq, df_met = load_cycle_seq_and_metrics(seed_dir, split)\n",
    "\n",
    "    files = df_seq[\"file\"].unique().tolist()\n",
    "    if MAX_FILES is not None:\n",
    "        files = files[:MAX_FILES]\n",
    "\n",
    "    out_dir = os.path.join(OUT_ROOT, split)  # ✅ split별 폴더\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    lam_key = f\"t_lambda_{LAMBDA_TO_PLOT:.2f}\"\n",
    "    title_prefix = f\"SEED {SEED} | {CKPT.upper()} | {split} | Trial12\"\n",
    "\n",
    "    for f in files:\n",
    "        sub = df_seq[df_seq[\"file\"] == f].copy()\n",
    "        mrow = df_met[df_met[\"file\"] == f]\n",
    "        if mrow.empty:\n",
    "            continue\n",
    "        mrow = mrow.iloc[0].to_dict()\n",
    "\n",
    "        t_s = int(mrow[\"t_s\"])\n",
    "        t_e = int(mrow[\"t_e\"])\n",
    "        ph_start = mrow.get(\"t_PH_start\", np.nan)\n",
    "        t_lambda = mrow.get(lam_key, np.nan)\n",
    "\n",
    "        df_eval = get_eval_segment(sub, t_s, t_e)\n",
    "        if df_eval.empty:\n",
    "            continue\n",
    "\n",
    "        sname = safe_name(f)\n",
    "\n",
    "        out_a = os.path.join(out_dir, f\"FIG_A_BOOKSTYLE_alpha_PH__{sname}.png\")\n",
    "        plot_ph_alpha_absolute_band(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            alpha=ALPHA,\n",
    "            out_path=out_a,\n",
    "            ph_start=ph_start if np.isfinite(ph_start) else None,\n",
    "            title_prefix=title_prefix,\n",
    "        )\n",
    "\n",
    "        out_b = os.path.join(out_dir, f\"FIG_B_BOOKSTYLE_alpha_lambda__lam{LAMBDA_TO_PLOT:.2f}__{sname}.png\")\n",
    "        plot_alpha_lambda_relative_band(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            alpha=ALPHA,\n",
    "            lambda_to_plot=LAMBDA_TO_PLOT,\n",
    "            t_lambda=int(t_lambda) if np.isfinite(t_lambda) else None,\n",
    "            out_path=out_b,\n",
    "            title_prefix=title_prefix,\n",
    "        )\n",
    "\n",
    "    print(f\"[{split}] DONE -> {out_dir}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not os.path.isdir(SEED_DIR):\n",
    "        raise FileNotFoundError(f\"Not found: {SEED_DIR}\")\n",
    "\n",
    "    # split별 실행\n",
    "    for split in SPLITS:\n",
    "        run_for_one_split(SEED_DIR, split)\n",
    "\n",
    "    print(\"\\nALL DONE.\")\n",
    "    print(\"Saved under:\", OUT_ROOT)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d4c1224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] ./Compare_Trial9_vs_Trial13\\01_seed_level_metrics_all.csv\n",
      "[Saved] ./Compare_Trial9_vs_Trial13\\02_trial_level_summary_mean_std.csv\n",
      "[Saved] ./Compare_Trial9_vs_Trial13\\03_winrate_best_vs_last.csv\n",
      "[Saved] ./Compare_Trial9_vs_Trial13\\04_trial9_vs_trial13_paired_by_seed.csv\n",
      "[Saved] ./Compare_Trial9_vs_Trial13\\06_prognostics_summary_by_trial_seed.csv\n",
      "[Saved] ./Compare_Trial9_vs_Trial13\\05_prognostics_summary_by_trial.csv\n",
      "[Saved] ./Compare_Trial9_vs_Trial13\\07_alpha_lambda_success_rates.csv\n",
      "[Saved] ./Compare_Trial9_vs_Trial13\\08_worst_files_comparison_topK.csv\n",
      "[Saved] ./Compare_Trial9_vs_Trial13\\compare_report.txt\n",
      "\n",
      "DONE. Check: ./Compare_Trial9_vs_Trial13\n"
     ]
    }
   ],
   "source": [
    "# Trial 9 / 13 Comparison Summary Analyzer\n",
    "# ============================================================\n",
    "# Trial9 vs Trial13 COMPARISON PACK (Full Version)\n",
    "# - Builds comparison tables (mean±std across seeds)\n",
    "# - Win-rate (per-seed best vs last) + Trial9 vs Trial13 paired-by-seed\n",
    "# - Prognostics summary: PH / CRA / Convergence + α–λ success rates\n",
    "# - Optional: file-level \"worst ranking\" comparison (top-K hard files)\n",
    "#\n",
    "# Output (default):\n",
    "#   ./Compare_Trial9_vs_Trial13/\n",
    "#     01_seed_level_metrics_all.csv\n",
    "#     02_trial_level_summary_mean_std.csv\n",
    "#     03_winrate_best_vs_last.csv\n",
    "#     04_trial9_vs_trial13_paired_by_seed.csv\n",
    "#     05_prognostics_summary_by_trial.csv\n",
    "#     06_prognostics_summary_by_trial_seed.csv\n",
    "#     07_alpha_lambda_success_rates.csv\n",
    "#     08_worst_files_comparison_topK.csv\n",
    "#     compare_report.txt\n",
    "#\n",
    "# Usage:\n",
    "#   1) Run Trial9 and Trial13 (seed sweep) so folders exist:\n",
    "#        Trial9/seed_<seed>/best_by_val_norm/...\n",
    "#        Trial13/seed_<seed>/best_by_val_norm/...\n",
    "#   2) Run this script.\n",
    "#\n",
    "# Notes:\n",
    "# - Works even if some seeds are missing (skips gracefully).\n",
    "# - Uses \"best_by_val_norm\" and \"last_epoch\" (both).\n",
    "# - Prognostics uses <split>_prognostics_metrics_per_file.csv\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# USER CONFIG\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    TRIAL9_DIR: str = r\"./Trial9\"\n",
    "    TRIAL13_DIR: str = r\"./Trial13\"\n",
    "\n",
    "    # seeds you attempted (it will skip missing)\n",
    "    SEEDS: Tuple[int, ...] = (9819123, 111, 222, 333, 444)\n",
    "\n",
    "    # compare both checkpoints\n",
    "    CKPTS: Tuple[str, ...] = (\"best_by_val_norm\", \"last_epoch\")\n",
    "\n",
    "    # which split to evaluate for prognostics\n",
    "    SPLIT: str = \"test\"  # \"train\"/\"val\"/\"test\"\n",
    "\n",
    "    # alpha-lambda keys (must match your saved csv col names)\n",
    "    LAMBDAS: Tuple[float, ...] = (0.2, 0.4, 0.6, 0.8)\n",
    "\n",
    "    # for worst-file analysis (optional)\n",
    "    TOPK_WORST_FILES: int = 15\n",
    "\n",
    "    OUT_DIR: str = r\"./Compare_Trial9_vs_Trial13\"\n",
    "\n",
    "\n",
    "cfg = Cfg()\n",
    "os.makedirs(cfg.OUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "def _exists(p: str) -> bool:\n",
    "    return os.path.exists(p)\n",
    "\n",
    "\n",
    "def _read_csv_safe(path: str) -> Optional[pd.DataFrame]:\n",
    "    if not _exists(path):\n",
    "        return None\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _trial_seed_ckpt_dir(trial_dir: str, seed: int, ckpt: str) -> str:\n",
    "    return os.path.join(trial_dir, f\"seed_{seed}\", ckpt)\n",
    "\n",
    "\n",
    "def _metrics_summary_path(trial_dir: str, seed: int, ckpt: str) -> str:\n",
    "    return os.path.join(_trial_seed_ckpt_dir(trial_dir, seed, ckpt), \"metrics_summary.csv\")\n",
    "\n",
    "\n",
    "def _prognostics_path(trial_dir: str, seed: int, ckpt: str, split: str) -> str:\n",
    "    return os.path.join(_trial_seed_ckpt_dir(trial_dir, seed, ckpt), f\"{split}_prognostics_metrics_per_file.csv\")\n",
    "\n",
    "\n",
    "def _cycle_seq_path(trial_dir: str, seed: int, ckpt: str, split: str) -> str:\n",
    "    return os.path.join(_trial_seed_ckpt_dir(trial_dir, seed, ckpt), f\"{split}_cycle_sequence_mean.csv\")\n",
    "\n",
    "\n",
    "def _format_mean_std(series: pd.Series, digits: int = 4) -> str:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
    "    if len(s) == 0:\n",
    "        return \"nan ± nan\"\n",
    "    return f\"{s.mean():.{digits}f} ± {s.std(ddof=0):.{digits}f}\"\n",
    "\n",
    "\n",
    "def _to_num(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Load seed-level metrics_summary.csv from Trial9 and Trial13\n",
    "# ============================================================\n",
    "def collect_seed_level_metrics(trial_name: str, trial_dir: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for seed in cfg.SEEDS:\n",
    "        for ckpt in cfg.CKPTS:\n",
    "            ms_path = _metrics_summary_path(trial_dir, seed, ckpt)\n",
    "            df = _read_csv_safe(ms_path)\n",
    "            if df is None or df.empty:\n",
    "                continue\n",
    "\n",
    "            r = df.iloc[0].to_dict()\n",
    "            r[\"trial\"] = trial_name\n",
    "            r[\"trial_dir\"] = trial_dir\n",
    "            r[\"seed\"] = int(seed)\n",
    "            r[\"checkpoint\"] = ckpt\n",
    "            r[\"metrics_summary_path\"] = ms_path\n",
    "            rows.append(r)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "\n",
    "    # normalize numeric\n",
    "    num_cols = [\n",
    "        \"train_rmse_cycles\",\"train_mae_cycles\",\"train_rmse_norm\",\"train_mae_norm\",\n",
    "        \"val_rmse_cycles\",\"val_mae_cycles\",\"val_rmse_norm\",\"val_mae_norm\",\n",
    "        \"test_rmse_cycles\",\"test_mae_cycles\",\"test_rmse_norm\",\"test_mae_norm\",\n",
    "        \"best_val_rmse_norm\",\"stopped_epoch\",\"feature_dim\"\n",
    "    ]\n",
    "    out = _to_num(out, [c for c in num_cols if c in out.columns])\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "df9 = collect_seed_level_metrics(\"Trial9\", cfg.TRIAL9_DIR)\n",
    "df13 = collect_seed_level_metrics(\"Trial13\", cfg.TRIAL13_DIR)\n",
    "\n",
    "df_all = pd.concat([df9, df13], axis=0, ignore_index=True)\n",
    "seed_metrics_path = os.path.join(cfg.OUT_DIR, \"01_seed_level_metrics_all.csv\")\n",
    "df_all.to_csv(seed_metrics_path, index=False)\n",
    "\n",
    "print(\"[Saved]\", seed_metrics_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Trial-level summary mean±std (across seeds), by checkpoint\n",
    "# ============================================================\n",
    "def build_trial_level_summary(df_all: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df_all.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    metrics = [\n",
    "        \"test_rmse_cycles\", \"test_mae_cycles\", \"test_rmse_norm\", \"test_mae_norm\",\n",
    "        \"val_rmse_norm\", \"val_rmse_cycles\",\n",
    "        \"train_rmse_cycles\", \"train_rmse_norm\",\n",
    "    ]\n",
    "    rows = []\n",
    "    for trial in sorted(df_all[\"trial\"].unique()):\n",
    "        for ckpt in cfg.CKPTS:\n",
    "            sub = df_all[(df_all[\"trial\"] == trial) & (df_all[\"checkpoint\"] == ckpt)].copy()\n",
    "            if sub.empty:\n",
    "                continue\n",
    "            row = {\"trial\": trial, \"checkpoint\": ckpt, \"n_seeds\": int(sub[\"seed\"].nunique())}\n",
    "            for m in metrics:\n",
    "                if m in sub.columns:\n",
    "                    row[m] = _format_mean_std(sub[m], digits=4)\n",
    "            rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "trial_summary = build_trial_level_summary(df_all)\n",
    "trial_summary_path = os.path.join(cfg.OUT_DIR, \"02_trial_level_summary_mean_std.csv\")\n",
    "trial_summary.to_csv(trial_summary_path, index=False)\n",
    "print(\"[Saved]\", trial_summary_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Win-rate: best_by_val_norm vs last_epoch within each trial (per seed)\n",
    "# ============================================================\n",
    "def win_rate_best_vs_last(df_all: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df_all.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # lower is better\n",
    "    metrics = [\"test_rmse_cycles\", \"test_mae_cycles\", \"test_rmse_norm\", \"test_mae_norm\"]\n",
    "    rows = []\n",
    "\n",
    "    for trial in sorted(df_all[\"trial\"].unique()):\n",
    "        for metric in metrics:\n",
    "            wins_last = 0\n",
    "            wins_best = 0\n",
    "            ties = 0\n",
    "            diffs = []\n",
    "\n",
    "            for seed in sorted(df_all[df_all[\"trial\"] == trial][\"seed\"].unique()):\n",
    "                b = df_all[(df_all[\"trial\"] == trial) & (df_all[\"seed\"] == seed) & (df_all[\"checkpoint\"] == \"best_by_val_norm\")]\n",
    "                l = df_all[(df_all[\"trial\"] == trial) & (df_all[\"seed\"] == seed) & (df_all[\"checkpoint\"] == \"last_epoch\")]\n",
    "                if b.empty or l.empty:\n",
    "                    continue\n",
    "                bv = pd.to_numeric(b.iloc[0].get(metric, np.nan), errors=\"coerce\")\n",
    "                lv = pd.to_numeric(l.iloc[0].get(metric, np.nan), errors=\"coerce\")\n",
    "                if not np.isfinite(bv) or not np.isfinite(lv):\n",
    "                    continue\n",
    "                diffs.append(float(lv - bv))\n",
    "                if lv < bv:\n",
    "                    wins_last += 1\n",
    "                elif bv < lv:\n",
    "                    wins_best += 1\n",
    "                else:\n",
    "                    ties += 1\n",
    "\n",
    "            rows.append({\n",
    "                \"trial\": trial,\n",
    "                \"metric\": metric,\n",
    "                \"wins_last\": wins_last,\n",
    "                \"wins_best\": wins_best,\n",
    "                \"ties\": ties,\n",
    "                \"mean(last-best)\": float(np.mean(diffs)) if diffs else np.nan,\n",
    "                \"std(last-best)\": float(np.std(diffs, ddof=0)) if diffs else np.nan,\n",
    "                \"n_compared\": int(len(diffs)),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "wr = win_rate_best_vs_last(df_all)\n",
    "wr_path = os.path.join(cfg.OUT_DIR, \"03_winrate_best_vs_last.csv\")\n",
    "wr.to_csv(wr_path, index=False)\n",
    "print(\"[Saved]\", wr_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Trial9 vs Trial13 paired-by-seed comparison (same seed & same ckpt)\n",
    "# ============================================================\n",
    "def paired_seed_comparison(df_all: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df_all.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    metrics = [\"test_rmse_cycles\", \"test_mae_cycles\", \"test_rmse_norm\", \"test_mae_norm\"]\n",
    "    rows = []\n",
    "\n",
    "    for ckpt in cfg.CKPTS:\n",
    "        for seed in cfg.SEEDS:\n",
    "            a = df_all[(df_all[\"trial\"] == \"Trial9\") & (df_all[\"checkpoint\"] == ckpt) & (df_all[\"seed\"] == seed)]\n",
    "            b = df_all[(df_all[\"trial\"] == \"Trial13\") & (df_all[\"checkpoint\"] == ckpt) & (df_all[\"seed\"] == seed)]\n",
    "            if a.empty or b.empty:\n",
    "                continue\n",
    "\n",
    "            row = {\"seed\": int(seed), \"checkpoint\": ckpt}\n",
    "            for m in metrics:\n",
    "                av = pd.to_numeric(a.iloc[0].get(m, np.nan), errors=\"coerce\")\n",
    "                bv = pd.to_numeric(b.iloc[0].get(m, np.nan), errors=\"coerce\")\n",
    "                row[f\"Trial9_{m}\"] = av\n",
    "                row[f\"Trial13_{m}\"] = bv\n",
    "                row[f\"delta(13-9)_{m}\"] = bv - av if np.isfinite(av) and np.isfinite(bv) else np.nan\n",
    "            rows.append(row)\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "\n",
    "    # also add summary rows (mean delta)\n",
    "    if not out.empty:\n",
    "        summary = {\"seed\": \"MEAN\", \"checkpoint\": \"ALL\"}\n",
    "        for m in metrics:\n",
    "            dcol = f\"delta(13-9)_{m}\"\n",
    "            summary[dcol] = pd.to_numeric(out[dcol], errors=\"coerce\").mean()\n",
    "        out = pd.concat([out, pd.DataFrame([summary])], ignore_index=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "paired = paired_seed_comparison(df_all)\n",
    "paired_path = os.path.join(cfg.OUT_DIR, \"04_trial9_vs_trial13_paired_by_seed.csv\")\n",
    "paired.to_csv(paired_path, index=False)\n",
    "print(\"[Saved]\", paired_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Prognostics summary from per-file CSVs (PH/CRA/Convergence + α–λ success)\n",
    "# ============================================================\n",
    "def collect_prognostics(trial_name: str, trial_dir: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for seed in cfg.SEEDS:\n",
    "        for ckpt in cfg.CKPTS:\n",
    "            path = _prognostics_path(trial_dir, seed, ckpt, cfg.SPLIT)\n",
    "            df = _read_csv_safe(path)\n",
    "            if df is None or df.empty:\n",
    "                continue\n",
    "\n",
    "            df = df.copy()\n",
    "            df[\"trial\"] = trial_name\n",
    "            df[\"seed\"] = int(seed)\n",
    "            df[\"checkpoint\"] = ckpt\n",
    "            df[\"prognostics_path\"] = path\n",
    "\n",
    "            # numeric coerce\n",
    "            numeric_cols = [\"PH\", \"CRA\", \"Convergence_cycles\", \"t_PH_start\"]\n",
    "            for lam in cfg.LAMBDAS:\n",
    "                numeric_cols += [f\"alpha_lambda_ok_{lam:.2f}\", f\"t_lambda_{lam:.2f}\"]\n",
    "            df = _to_num(df, [c for c in numeric_cols if c in df.columns])\n",
    "\n",
    "            rows.append(df)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    return pd.concat(rows, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "prog9 = collect_prognostics(\"Trial9\", cfg.TRIAL9_DIR)\n",
    "prog13 = collect_prognostics(\"Trial13\", cfg.TRIAL13_DIR)\n",
    "prog_all = pd.concat([prog9, prog13], axis=0, ignore_index=True)\n",
    "\n",
    "prog_seed_path = os.path.join(cfg.OUT_DIR, \"06_prognostics_summary_by_trial_seed.csv\")\n",
    "if not prog_all.empty:\n",
    "    prog_all.to_csv(prog_seed_path, index=False)\n",
    "    print(\"[Saved]\", prog_seed_path)\n",
    "else:\n",
    "    print(\"[Warn] No prognostics CSVs found. Skipping prognostics summary.\")\n",
    "\n",
    "\n",
    "def summarize_prognostics(prog_all: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    if prog_all.empty:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Core stats (per trial & ckpt)\n",
    "    core_metrics = [\"PH\", \"CRA\", \"Convergence_cycles\"]\n",
    "    rows = []\n",
    "    for trial in sorted(prog_all[\"trial\"].unique()):\n",
    "        for ckpt in cfg.CKPTS:\n",
    "            sub = prog_all[(prog_all[\"trial\"] == trial) & (prog_all[\"checkpoint\"] == ckpt)].copy()\n",
    "            if sub.empty:\n",
    "                continue\n",
    "\n",
    "            row = {\n",
    "                \"trial\": trial,\n",
    "                \"checkpoint\": ckpt,\n",
    "                \"n_files\": int(sub[\"file\"].nunique()) if \"file\" in sub.columns else int(len(sub)),\n",
    "            }\n",
    "\n",
    "            for m in core_metrics:\n",
    "                if m in sub.columns:\n",
    "                    s = pd.to_numeric(sub[m], errors=\"coerce\")\n",
    "                    row[f\"{m}_mean\"] = float(s.mean())\n",
    "                    row[f\"{m}_std\"] = float(s.std(ddof=0))\n",
    "                    row[f\"{m}_median\"] = float(s.median())\n",
    "                    row[f\"{m}_nan_rate\"] = float(np.mean(~np.isfinite(s.values)))\n",
    "            rows.append(row)\n",
    "\n",
    "    df_core = pd.DataFrame(rows)\n",
    "\n",
    "    # α–λ success rates (mean of 0/1)\n",
    "    rows2 = []\n",
    "    for trial in sorted(prog_all[\"trial\"].unique()):\n",
    "        for ckpt in cfg.CKPTS:\n",
    "            sub = prog_all[(prog_all[\"trial\"] == trial) & (prog_all[\"checkpoint\"] == ckpt)].copy()\n",
    "            if sub.empty:\n",
    "                continue\n",
    "            row = {\"trial\": trial, \"checkpoint\": ckpt}\n",
    "            for lam in cfg.LAMBDAS:\n",
    "                col = f\"alpha_lambda_ok_{lam:.2f}\"\n",
    "                if col in sub.columns:\n",
    "                    v = pd.to_numeric(sub[col], errors=\"coerce\")\n",
    "                    # success rate among non-nan\n",
    "                    ok = v.dropna()\n",
    "                    row[f\"lambda_{lam:.2f}_success_rate\"] = float(ok.mean()) if len(ok) else np.nan\n",
    "                    row[f\"lambda_{lam:.2f}_n\"] = int(len(ok))\n",
    "            rows2.append(row)\n",
    "\n",
    "    df_lam = pd.DataFrame(rows2)\n",
    "    return df_core, df_lam\n",
    "\n",
    "\n",
    "df_core, df_lam = summarize_prognostics(prog_all)\n",
    "\n",
    "core_path = os.path.join(cfg.OUT_DIR, \"05_prognostics_summary_by_trial.csv\")\n",
    "lam_path = os.path.join(cfg.OUT_DIR, \"07_alpha_lambda_success_rates.csv\")\n",
    "df_core.to_csv(core_path, index=False)\n",
    "df_lam.to_csv(lam_path, index=False)\n",
    "print(\"[Saved]\", core_path)\n",
    "print(\"[Saved]\", lam_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) Optional: Worst files comparison (top-K by mean abs error from cycle sequence)\n",
    "# ============================================================\n",
    "def worst_files_from_cycle_seq(trial_name: str, trial_dir: str, seed: int, ckpt: str, split: str, topk: int) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Uses <split>_cycle_sequence_mean.csv:\n",
    "      columns: file, cycle, RUL_true, RUL_pred, ...\n",
    "    Build per-file MAE_cycles and return top-K worst.\n",
    "    \"\"\"\n",
    "    path = _cycle_seq_path(trial_dir, seed, ckpt, split)\n",
    "    df = _read_csv_safe(path)\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "\n",
    "    # robust numeric\n",
    "    for c in [\"RUL_true\", \"RUL_pred\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    if \"file\" not in df.columns or \"RUL_true\" not in df.columns or \"RUL_pred\" not in df.columns:\n",
    "        return None\n",
    "\n",
    "    df = df.dropna(subset=[\"file\", \"RUL_true\", \"RUL_pred\"]).copy()\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    df[\"abs_err\"] = np.abs(df[\"RUL_pred\"].values - df[\"RUL_true\"].values)\n",
    "\n",
    "    g = df.groupby(\"file\", as_index=False).agg(\n",
    "        mae_cycles=(\"abs_err\", \"mean\"),\n",
    "        rmse_cycles=(\"abs_err\", lambda x: float(np.sqrt(np.mean(np.square(x.values))))),\n",
    "        n_points=(\"abs_err\", \"count\"),\n",
    "    )\n",
    "    g = g.sort_values(\"mae_cycles\", ascending=False).head(int(topk)).copy()\n",
    "    g[\"trial\"] = trial_name\n",
    "    g[\"seed\"] = int(seed)\n",
    "    g[\"checkpoint\"] = ckpt\n",
    "    g[\"cycle_seq_path\"] = path\n",
    "    return g\n",
    "\n",
    "\n",
    "rows_worst = []\n",
    "for seed in cfg.SEEDS:\n",
    "    for ckpt in cfg.CKPTS:\n",
    "        w9 = worst_files_from_cycle_seq(\"Trial9\", cfg.TRIAL9_DIR, seed, ckpt, cfg.SPLIT, cfg.TOPK_WORST_FILES)\n",
    "        w13 = worst_files_from_cycle_seq(\"Trial13\", cfg.TRIAL13_DIR, seed, ckpt, cfg.SPLIT, cfg.TOPK_WORST_FILES)\n",
    "        if w9 is not None:\n",
    "            rows_worst.append(w9)\n",
    "        if w13 is not None:\n",
    "            rows_worst.append(w13)\n",
    "\n",
    "if rows_worst:\n",
    "    df_worst = pd.concat(rows_worst, axis=0, ignore_index=True)\n",
    "    worst_path = os.path.join(cfg.OUT_DIR, \"08_worst_files_comparison_topK.csv\")\n",
    "    df_worst.to_csv(worst_path, index=False)\n",
    "    print(\"[Saved]\", worst_path)\n",
    "else:\n",
    "    worst_path = None\n",
    "    print(\"[Info] No cycle_sequence_mean CSVs found for worst-file analysis (optional).\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) Human-readable report.txt\n",
    "# ============================================================\n",
    "def write_report(df_all: pd.DataFrame, trial_summary: pd.DataFrame, wr: pd.DataFrame,\n",
    "                 paired: pd.DataFrame, df_core: pd.DataFrame, df_lam: pd.DataFrame,\n",
    "                 out_path: str) -> None:\n",
    "    lines = []\n",
    "    lines.append(\"=== Trial9 vs Trial13 Comparison Report ===\\n\")\n",
    "\n",
    "    # seed-level availability\n",
    "    lines.append(\">> Seed-level metrics availability:\")\n",
    "    if df_all.empty:\n",
    "        lines.append(\"  - No metrics_summary.csv found for either trial.\")\n",
    "    else:\n",
    "        for trial in [\"Trial9\", \"Trial13\"]:\n",
    "            sub = df_all[df_all[\"trial\"] == trial]\n",
    "            lines.append(f\"  - {trial}: rows={len(sub)} (seed x ckpt), seeds={sorted(sub['seed'].unique().tolist())}\")\n",
    "\n",
    "    lines.append(\"\\n>> Trial-level mean±std (seed aggregation):\")\n",
    "    if trial_summary.empty:\n",
    "        lines.append(\"  - (none)\")\n",
    "    else:\n",
    "        lines.append(trial_summary.to_string(index=False))\n",
    "\n",
    "    lines.append(\"\\n>> Win-rate (best_by_val_norm vs last_epoch) per trial:\")\n",
    "    if wr.empty:\n",
    "        lines.append(\"  - (none)\")\n",
    "    else:\n",
    "        lines.append(wr.to_string(index=False))\n",
    "\n",
    "    lines.append(\"\\n>> Trial9 vs Trial13 paired-by-seed deltas (13-9):\")\n",
    "    if paired.empty:\n",
    "        lines.append(\"  - (none)\")\n",
    "    else:\n",
    "        lines.append(paired.to_string(index=False))\n",
    "\n",
    "    lines.append(\"\\n>> Prognostics summary (PH/CRA/Convergence):\")\n",
    "    if df_core.empty:\n",
    "        lines.append(\"  - (none) (missing prognostics CSVs?)\")\n",
    "    else:\n",
    "        lines.append(df_core.to_string(index=False))\n",
    "\n",
    "    lines.append(\"\\n>> α–λ success rates:\")\n",
    "    if df_lam.empty:\n",
    "        lines.append(\"  - (none)\")\n",
    "    else:\n",
    "        lines.append(df_lam.to_string(index=False))\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "\n",
    "report_path = os.path.join(cfg.OUT_DIR, \"compare_report.txt\")\n",
    "write_report(df_all, trial_summary, wr, paired, df_core, df_lam, report_path)\n",
    "print(\"[Saved]\", report_path)\n",
    "\n",
    "print(\"\\nDONE. Check:\", cfg.OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9245711c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] ./Compare_Trial12_vs_Trial13\\01_seed_level_metrics_all.csv\n",
      "[Saved] ./Compare_Trial12_vs_Trial13\\02_trial_level_summary_mean_std.csv\n",
      "[Saved] ./Compare_Trial12_vs_Trial13\\03_winrate_best_vs_last.csv\n",
      "[Saved] ./Compare_Trial12_vs_Trial13\\04_trial12_vs_trial13_paired_by_seed.csv\n",
      "[Saved] ./Compare_Trial12_vs_Trial13\\06_prognostics_summary_by_trial_seed.csv\n",
      "[Saved] ./Compare_Trial12_vs_Trial13\\05_prognostics_summary_by_trial.csv\n",
      "[Saved] ./Compare_Trial12_vs_Trial13\\07_alpha_lambda_success_rates.csv\n",
      "[Saved] ./Compare_Trial12_vs_Trial13\\08_worst_files_comparison_topK.csv\n",
      "[Saved] ./Compare_Trial12_vs_Trial13\\compare_report.txt\n",
      "\n",
      "DONE. Check: ./Compare_Trial12_vs_Trial13\n"
     ]
    }
   ],
   "source": [
    "# Trial 12 / 13 Comparison Summary Analyzer\n",
    "# ============================================================\n",
    "# Trial12 vs Trial13 COMPARISON PACK (Full Version)\n",
    "# - Builds comparison tables (mean±std across seeds)\n",
    "# - Win-rate (per-seed best vs last) + Trial12 vs Trial13 paired-by-seed\n",
    "# - Prognostics summary: PH / CRA / Convergence + α–λ success rates\n",
    "# - Optional: file-level \"worst ranking\" comparison (top-K hard files)\n",
    "#\n",
    "# Output (default):\n",
    "#   ./Compare_Trial12_vs_Trial13/\n",
    "#     01_seed_level_metrics_all.csv\n",
    "#     02_trial_level_summary_mean_std.csv\n",
    "#     03_winrate_best_vs_last.csv\n",
    "#     04_trial12_vs_trial13_paired_by_seed.csv\n",
    "#     05_prognostics_summary_by_trial.csv\n",
    "#     06_prognostics_summary_by_trial_seed.csv\n",
    "#     07_alpha_lambda_success_rates.csv\n",
    "#     08_worst_files_comparison_topK.csv\n",
    "#     compare_report.txt\n",
    "#\n",
    "# Usage:\n",
    "#   1) Run Trial12 and Trial13 (seed sweep) so folders exist:\n",
    "#        Trial12/seed_<seed>/best_by_val_norm/...\n",
    "#        Trial13/seed_<seed>/best_by_val_norm/...\n",
    "#   2) Run this script.\n",
    "#\n",
    "# Notes:\n",
    "# - Works even if some seeds are missing (skips gracefully).\n",
    "# - Uses \"best_by_val_norm\" and \"last_epoch\" (both).\n",
    "# - Prognostics uses <split>_prognostics_metrics_per_file.csv\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# USER CONFIG\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    TRIAL12_DIR: str = r\"./Trial12\"\n",
    "    TRIAL13_DIR: str = r\"./Trial13\"\n",
    "\n",
    "    # seeds you attempted (it will skip missing)\n",
    "    SEEDS: Tuple[int, ...] = (9819123, 111, 222, 333, 444)\n",
    "\n",
    "    # compare both checkpoints\n",
    "    CKPTS: Tuple[str, ...] = (\"best_by_val_norm\", \"last_epoch\")\n",
    "\n",
    "    # which split to evaluate for prognostics\n",
    "    SPLIT: str = \"test\"  # \"train\"/\"val\"/\"test\"\n",
    "\n",
    "    # alpha-lambda keys (must match your saved csv col names)\n",
    "    LAMBDAS: Tuple[float, ...] = (0.2, 0.4, 0.6, 0.8)\n",
    "\n",
    "    # for worst-file analysis (optional)\n",
    "    TOPK_WORST_FILES: int = 15\n",
    "\n",
    "    OUT_DIR: str = r\"./Compare_Trial12_vs_Trial13\"\n",
    "\n",
    "\n",
    "cfg = Cfg()\n",
    "os.makedirs(cfg.OUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "def _exists(p: str) -> bool:\n",
    "    return os.path.exists(p)\n",
    "\n",
    "\n",
    "def _read_csv_safe(path: str) -> Optional[pd.DataFrame]:\n",
    "    if not _exists(path):\n",
    "        return None\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _trial_seed_ckpt_dir(trial_dir: str, seed: int, ckpt: str) -> str:\n",
    "    return os.path.join(trial_dir, f\"seed_{seed}\", ckpt)\n",
    "\n",
    "\n",
    "def _metrics_summary_path(trial_dir: str, seed: int, ckpt: str) -> str:\n",
    "    return os.path.join(_trial_seed_ckpt_dir(trial_dir, seed, ckpt), \"metrics_summary.csv\")\n",
    "\n",
    "\n",
    "def _prognostics_path(trial_dir: str, seed: int, ckpt: str, split: str) -> str:\n",
    "    return os.path.join(_trial_seed_ckpt_dir(trial_dir, seed, ckpt), f\"{split}_prognostics_metrics_per_file.csv\")\n",
    "\n",
    "\n",
    "def _cycle_seq_path(trial_dir: str, seed: int, ckpt: str, split: str) -> str:\n",
    "    return os.path.join(_trial_seed_ckpt_dir(trial_dir, seed, ckpt), f\"{split}_cycle_sequence_mean.csv\")\n",
    "\n",
    "\n",
    "def _format_mean_std(series: pd.Series, digits: int = 4) -> str:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
    "    if len(s) == 0:\n",
    "        return \"nan ± nan\"\n",
    "    return f\"{s.mean():.{digits}f} ± {s.std(ddof=0):.{digits}f}\"\n",
    "\n",
    "\n",
    "def _to_num(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Load seed-level metrics_summary.csv from Trial12 and Trial13\n",
    "# ============================================================\n",
    "def collect_seed_level_metrics(trial_name: str, trial_dir: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for seed in cfg.SEEDS:\n",
    "        for ckpt in cfg.CKPTS:\n",
    "            ms_path = _metrics_summary_path(trial_dir, seed, ckpt)\n",
    "            df = _read_csv_safe(ms_path)\n",
    "            if df is None or df.empty:\n",
    "                continue\n",
    "\n",
    "            r = df.iloc[0].to_dict()\n",
    "            r[\"trial\"] = trial_name\n",
    "            r[\"trial_dir\"] = trial_dir\n",
    "            r[\"seed\"] = int(seed)\n",
    "            r[\"checkpoint\"] = ckpt\n",
    "            r[\"metrics_summary_path\"] = ms_path\n",
    "            rows.append(r)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "\n",
    "    # normalize numeric\n",
    "    num_cols = [\n",
    "        \"train_rmse_cycles\",\"train_mae_cycles\",\"train_rmse_norm\",\"train_mae_norm\",\n",
    "        \"val_rmse_cycles\",\"val_mae_cycles\",\"val_rmse_norm\",\"val_mae_norm\",\n",
    "        \"test_rmse_cycles\",\"test_mae_cycles\",\"test_rmse_norm\",\"test_mae_norm\",\n",
    "        \"best_val_rmse_norm\",\"stopped_epoch\",\"feature_dim\"\n",
    "    ]\n",
    "    out = _to_num(out, [c for c in num_cols if c in out.columns])\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "df12 = collect_seed_level_metrics(\"Trial12\", cfg.TRIAL12_DIR)\n",
    "df13 = collect_seed_level_metrics(\"Trial13\", cfg.TRIAL13_DIR)\n",
    "\n",
    "df_all = pd.concat([df12, df13], axis=0, ignore_index=True)\n",
    "seed_metrics_path = os.path.join(cfg.OUT_DIR, \"01_seed_level_metrics_all.csv\")\n",
    "df_all.to_csv(seed_metrics_path, index=False)\n",
    "\n",
    "print(\"[Saved]\", seed_metrics_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Trial-level summary mean±std (across seeds), by checkpoint\n",
    "# ============================================================\n",
    "def build_trial_level_summary(df_all: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df_all.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    metrics = [\n",
    "        \"test_rmse_cycles\", \"test_mae_cycles\", \"test_rmse_norm\", \"test_mae_norm\",\n",
    "        \"val_rmse_norm\", \"val_rmse_cycles\",\n",
    "        \"train_rmse_cycles\", \"train_rmse_norm\",\n",
    "    ]\n",
    "    rows = []\n",
    "    for trial in sorted(df_all[\"trial\"].unique()):\n",
    "        for ckpt in cfg.CKPTS:\n",
    "            sub = df_all[(df_all[\"trial\"] == trial) & (df_all[\"checkpoint\"] == ckpt)].copy()\n",
    "            if sub.empty:\n",
    "                continue\n",
    "            row = {\"trial\": trial, \"checkpoint\": ckpt, \"n_seeds\": int(sub[\"seed\"].nunique())}\n",
    "            for m in metrics:\n",
    "                if m in sub.columns:\n",
    "                    row[m] = _format_mean_std(sub[m], digits=4)\n",
    "            rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "trial_summary = build_trial_level_summary(df_all)\n",
    "trial_summary_path = os.path.join(cfg.OUT_DIR, \"02_trial_level_summary_mean_std.csv\")\n",
    "trial_summary.to_csv(trial_summary_path, index=False)\n",
    "print(\"[Saved]\", trial_summary_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Win-rate: best_by_val_norm vs last_epoch within each trial (per seed)\n",
    "# ============================================================\n",
    "def win_rate_best_vs_last(df_all: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df_all.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # lower is better\n",
    "    metrics = [\"test_rmse_cycles\", \"test_mae_cycles\", \"test_rmse_norm\", \"test_mae_norm\"]\n",
    "    rows = []\n",
    "\n",
    "    for trial in sorted(df_all[\"trial\"].unique()):\n",
    "        for metric in metrics:\n",
    "            wins_last = 0\n",
    "            wins_best = 0\n",
    "            ties = 0\n",
    "            diffs = []\n",
    "\n",
    "            for seed in sorted(df_all[df_all[\"trial\"] == trial][\"seed\"].unique()):\n",
    "                b = df_all[(df_all[\"trial\"] == trial) & (df_all[\"seed\"] == seed) & (df_all[\"checkpoint\"] == \"best_by_val_norm\")]\n",
    "                l = df_all[(df_all[\"trial\"] == trial) & (df_all[\"seed\"] == seed) & (df_all[\"checkpoint\"] == \"last_epoch\")]\n",
    "                if b.empty or l.empty:\n",
    "                    continue\n",
    "                bv = pd.to_numeric(b.iloc[0].get(metric, np.nan), errors=\"coerce\")\n",
    "                lv = pd.to_numeric(l.iloc[0].get(metric, np.nan), errors=\"coerce\")\n",
    "                if not np.isfinite(bv) or not np.isfinite(lv):\n",
    "                    continue\n",
    "                diffs.append(float(lv - bv))\n",
    "                if lv < bv:\n",
    "                    wins_last += 1\n",
    "                elif bv < lv:\n",
    "                    wins_best += 1\n",
    "                else:\n",
    "                    ties += 1\n",
    "\n",
    "            rows.append({\n",
    "                \"trial\": trial,\n",
    "                \"metric\": metric,\n",
    "                \"wins_last\": wins_last,\n",
    "                \"wins_best\": wins_best,\n",
    "                \"ties\": ties,\n",
    "                \"mean(last-best)\": float(np.mean(diffs)) if diffs else np.nan,\n",
    "                \"std(last-best)\": float(np.std(diffs, ddof=0)) if diffs else np.nan,\n",
    "                \"n_compared\": int(len(diffs)),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "wr = win_rate_best_vs_last(df_all)\n",
    "wr_path = os.path.join(cfg.OUT_DIR, \"03_winrate_best_vs_last.csv\")\n",
    "wr.to_csv(wr_path, index=False)\n",
    "print(\"[Saved]\", wr_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Trial12 vs Trial13 paired-by-seed comparison (same seed & same ckpt)\n",
    "# ============================================================\n",
    "def paired_seed_comparison(df_all: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df_all.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    metrics = [\"test_rmse_cycles\", \"test_mae_cycles\", \"test_rmse_norm\", \"test_mae_norm\"]\n",
    "    rows = []\n",
    "\n",
    "    for ckpt in cfg.CKPTS:\n",
    "        for seed in cfg.SEEDS:\n",
    "            a = df_all[(df_all[\"trial\"] == \"Trial12\") & (df_all[\"checkpoint\"] == ckpt) & (df_all[\"seed\"] == seed)]\n",
    "            b = df_all[(df_all[\"trial\"] == \"Trial13\") & (df_all[\"checkpoint\"] == ckpt) & (df_all[\"seed\"] == seed)]\n",
    "            if a.empty or b.empty:\n",
    "                continue\n",
    "\n",
    "            row = {\"seed\": int(seed), \"checkpoint\": ckpt}\n",
    "            for m in metrics:\n",
    "                av = pd.to_numeric(a.iloc[0].get(m, np.nan), errors=\"coerce\")\n",
    "                bv = pd.to_numeric(b.iloc[0].get(m, np.nan), errors=\"coerce\")\n",
    "                row[f\"Trial12_{m}\"] = av\n",
    "                row[f\"Trial13_{m}\"] = bv\n",
    "                row[f\"delta(13-12)_{m}\"] = bv - av if np.isfinite(av) and np.isfinite(bv) else np.nan\n",
    "            rows.append(row)\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "\n",
    "    # also add summary rows (mean delta)\n",
    "    if not out.empty:\n",
    "        summary = {\"seed\": \"MEAN\", \"checkpoint\": \"ALL\"}\n",
    "        for m in metrics:\n",
    "            dcol = f\"delta(13-12)_{m}\"\n",
    "            summary[dcol] = pd.to_numeric(out[dcol], errors=\"coerce\").mean()\n",
    "        out = pd.concat([out, pd.DataFrame([summary])], ignore_index=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "paired = paired_seed_comparison(df_all)\n",
    "paired_path = os.path.join(cfg.OUT_DIR, \"04_trial12_vs_trial13_paired_by_seed.csv\")\n",
    "paired.to_csv(paired_path, index=False)\n",
    "print(\"[Saved]\", paired_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Prognostics summary from per-file CSVs (PH/CRA/Convergence + α–λ success)\n",
    "# ============================================================\n",
    "def collect_prognostics(trial_name: str, trial_dir: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for seed in cfg.SEEDS:\n",
    "        for ckpt in cfg.CKPTS:\n",
    "            path = _prognostics_path(trial_dir, seed, ckpt, cfg.SPLIT)\n",
    "            df = _read_csv_safe(path)\n",
    "            if df is None or df.empty:\n",
    "                continue\n",
    "\n",
    "            df = df.copy()\n",
    "            df[\"trial\"] = trial_name\n",
    "            df[\"seed\"] = int(seed)\n",
    "            df[\"checkpoint\"] = ckpt\n",
    "            df[\"prognostics_path\"] = path\n",
    "\n",
    "            # numeric coerce\n",
    "            numeric_cols = [\"PH\", \"CRA\", \"Convergence_cycles\", \"t_PH_start\"]\n",
    "            for lam in cfg.LAMBDAS:\n",
    "                numeric_cols += [f\"alpha_lambda_ok_{lam:.2f}\", f\"t_lambda_{lam:.2f}\"]\n",
    "            df = _to_num(df, [c for c in numeric_cols if c in df.columns])\n",
    "\n",
    "            rows.append(df)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    return pd.concat(rows, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "prog12 = collect_prognostics(\"Trial12\", cfg.TRIAL12_DIR)\n",
    "prog13 = collect_prognostics(\"Trial13\", cfg.TRIAL13_DIR)\n",
    "prog_all = pd.concat([prog12, prog13], axis=0, ignore_index=True)\n",
    "\n",
    "prog_seed_path = os.path.join(cfg.OUT_DIR, \"06_prognostics_summary_by_trial_seed.csv\")\n",
    "if not prog_all.empty:\n",
    "    prog_all.to_csv(prog_seed_path, index=False)\n",
    "    print(\"[Saved]\", prog_seed_path)\n",
    "else:\n",
    "    print(\"[Warn] No prognostics CSVs found. Skipping prognostics summary.\")\n",
    "\n",
    "\n",
    "def summarize_prognostics(prog_all: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    if prog_all.empty:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Core stats (per trial & ckpt)\n",
    "    core_metrics = [\"PH\", \"CRA\", \"Convergence_cycles\"]\n",
    "    rows = []\n",
    "    for trial in sorted(prog_all[\"trial\"].unique()):\n",
    "        for ckpt in cfg.CKPTS:\n",
    "            sub = prog_all[(prog_all[\"trial\"] == trial) & (prog_all[\"checkpoint\"] == ckpt)].copy()\n",
    "            if sub.empty:\n",
    "                continue\n",
    "\n",
    "            row = {\n",
    "                \"trial\": trial,\n",
    "                \"checkpoint\": ckpt,\n",
    "                \"n_files\": int(sub[\"file\"].nunique()) if \"file\" in sub.columns else int(len(sub)),\n",
    "            }\n",
    "\n",
    "            for m in core_metrics:\n",
    "                if m in sub.columns:\n",
    "                    s = pd.to_numeric(sub[m], errors=\"coerce\")\n",
    "                    row[f\"{m}_mean\"] = float(s.mean())\n",
    "                    row[f\"{m}_std\"] = float(s.std(ddof=0))\n",
    "                    row[f\"{m}_median\"] = float(s.median())\n",
    "                    row[f\"{m}_nan_rate\"] = float(np.mean(~np.isfinite(s.values)))\n",
    "            rows.append(row)\n",
    "\n",
    "    df_core = pd.DataFrame(rows)\n",
    "\n",
    "    # α–λ success rates (mean of 0/1)\n",
    "    rows2 = []\n",
    "    for trial in sorted(prog_all[\"trial\"].unique()):\n",
    "        for ckpt in cfg.CKPTS:\n",
    "            sub = prog_all[(prog_all[\"trial\"] == trial) & (prog_all[\"checkpoint\"] == ckpt)].copy()\n",
    "            if sub.empty:\n",
    "                continue\n",
    "            row = {\"trial\": trial, \"checkpoint\": ckpt}\n",
    "            for lam in cfg.LAMBDAS:\n",
    "                col = f\"alpha_lambda_ok_{lam:.2f}\"\n",
    "                if col in sub.columns:\n",
    "                    v = pd.to_numeric(sub[col], errors=\"coerce\")\n",
    "                    ok = v.dropna()\n",
    "                    row[f\"lambda_{lam:.2f}_success_rate\"] = float(ok.mean()) if len(ok) else np.nan\n",
    "                    row[f\"lambda_{lam:.2f}_n\"] = int(len(ok))\n",
    "            rows2.append(row)\n",
    "\n",
    "    df_lam = pd.DataFrame(rows2)\n",
    "    return df_core, df_lam\n",
    "\n",
    "\n",
    "df_core, df_lam = summarize_prognostics(prog_all)\n",
    "\n",
    "core_path = os.path.join(cfg.OUT_DIR, \"05_prognostics_summary_by_trial.csv\")\n",
    "lam_path = os.path.join(cfg.OUT_DIR, \"07_alpha_lambda_success_rates.csv\")\n",
    "df_core.to_csv(core_path, index=False)\n",
    "df_lam.to_csv(lam_path, index=False)\n",
    "print(\"[Saved]\", core_path)\n",
    "print(\"[Saved]\", lam_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) Optional: Worst files comparison (top-K by mean abs error from cycle sequence)\n",
    "# ============================================================\n",
    "def worst_files_from_cycle_seq(trial_name: str, trial_dir: str, seed: int, ckpt: str, split: str, topk: int) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Uses <split>_cycle_sequence_mean.csv:\n",
    "      columns: file, cycle, RUL_true, RUL_pred, ...\n",
    "    Build per-file MAE_cycles and return top-K worst.\n",
    "    \"\"\"\n",
    "    path = _cycle_seq_path(trial_dir, seed, ckpt, split)\n",
    "    df = _read_csv_safe(path)\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "\n",
    "    for c in [\"RUL_true\", \"RUL_pred\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    if \"file\" not in df.columns or \"RUL_true\" not in df.columns or \"RUL_pred\" not in df.columns:\n",
    "        return None\n",
    "\n",
    "    df = df.dropna(subset=[\"file\", \"RUL_true\", \"RUL_pred\"]).copy()\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    df[\"abs_err\"] = np.abs(df[\"RUL_pred\"].values - df[\"RUL_true\"].values)\n",
    "\n",
    "    g = df.groupby(\"file\", as_index=False).agg(\n",
    "        mae_cycles=(\"abs_err\", \"mean\"),\n",
    "        rmse_cycles=(\"abs_err\", lambda x: float(np.sqrt(np.mean(np.square(x.values))))),\n",
    "        n_points=(\"abs_err\", \"count\"),\n",
    "    )\n",
    "    g = g.sort_values(\"mae_cycles\", ascending=False).head(int(topk)).copy()\n",
    "    g[\"trial\"] = trial_name\n",
    "    g[\"seed\"] = int(seed)\n",
    "    g[\"checkpoint\"] = ckpt\n",
    "    g[\"cycle_seq_path\"] = path\n",
    "    return g\n",
    "\n",
    "\n",
    "rows_worst = []\n",
    "for seed in cfg.SEEDS:\n",
    "    for ckpt in cfg.CKPTS:\n",
    "        w12 = worst_files_from_cycle_seq(\"Trial12\", cfg.TRIAL12_DIR, seed, ckpt, cfg.SPLIT, cfg.TOPK_WORST_FILES)\n",
    "        w13 = worst_files_from_cycle_seq(\"Trial13\", cfg.TRIAL13_DIR, seed, ckpt, cfg.SPLIT, cfg.TOPK_WORST_FILES)\n",
    "        if w12 is not None:\n",
    "            rows_worst.append(w12)\n",
    "        if w13 is not None:\n",
    "            rows_worst.append(w13)\n",
    "\n",
    "if rows_worst:\n",
    "    df_worst = pd.concat(rows_worst, axis=0, ignore_index=True)\n",
    "    worst_path = os.path.join(cfg.OUT_DIR, \"08_worst_files_comparison_topK.csv\")\n",
    "    df_worst.to_csv(worst_path, index=False)\n",
    "    print(\"[Saved]\", worst_path)\n",
    "else:\n",
    "    worst_path = None\n",
    "    print(\"[Info] No cycle_sequence_mean CSVs found for worst-file analysis (optional).\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) Human-readable report.txt\n",
    "# ============================================================\n",
    "def write_report(df_all: pd.DataFrame, trial_summary: pd.DataFrame, wr: pd.DataFrame,\n",
    "                 paired: pd.DataFrame, df_core: pd.DataFrame, df_lam: pd.DataFrame,\n",
    "                 out_path: str) -> None:\n",
    "    lines = []\n",
    "    lines.append(\"=== Trial12 vs Trial13 Comparison Report ===\\n\")\n",
    "\n",
    "    lines.append(\">> Seed-level metrics availability:\")\n",
    "    if df_all.empty:\n",
    "        lines.append(\"  - No metrics_summary.csv found for either trial.\")\n",
    "    else:\n",
    "        for trial in [\"Trial12\", \"Trial13\"]:\n",
    "            sub = df_all[df_all[\"trial\"] == trial]\n",
    "            lines.append(f\"  - {trial}: rows={len(sub)} (seed x ckpt), seeds={sorted(sub['seed'].unique().tolist())}\")\n",
    "\n",
    "    lines.append(\"\\n>> Trial-level mean±std (seed aggregation):\")\n",
    "    lines.append(trial_summary.to_string(index=False) if not trial_summary.empty else \"  - (none)\")\n",
    "\n",
    "    lines.append(\"\\n>> Win-rate (best_by_val_norm vs last_epoch) per trial:\")\n",
    "    lines.append(wr.to_string(index=False) if not wr.empty else \"  - (none)\")\n",
    "\n",
    "    lines.append(\"\\n>> Trial12 vs Trial13 paired-by-seed deltas (13-12):\")\n",
    "    lines.append(paired.to_string(index=False) if not paired.empty else \"  - (none)\")\n",
    "\n",
    "    lines.append(\"\\n>> Prognostics summary (PH/CRA/Convergence):\")\n",
    "    lines.append(df_core.to_string(index=False) if not df_core.empty else \"  - (none) (missing prognostics CSVs?)\")\n",
    "\n",
    "    lines.append(\"\\n>> α–λ success rates:\")\n",
    "    lines.append(df_lam.to_string(index=False) if not df_lam.empty else \"  - (none)\")\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "\n",
    "report_path = os.path.join(cfg.OUT_DIR, \"compare_report.txt\")\n",
    "write_report(df_all, trial_summary, wr, paired, df_core, df_lam, report_path)\n",
    "print(\"[Saved]\", report_path)\n",
    "\n",
    "print(\"\\nDONE. Check:\", cfg.OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "019edc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['min_vce', 'delta_1', 'delta_5', 'delta_20', 'delta_50', 'ema_10', 'ema_50', 'rollstd_10', 'win_mean', 'win_std', 'win_slope']\n"
     ]
    }
   ],
   "source": [
    "TRIAL_DIR = \"./Trial13\"\n",
    "SEED = 333\n",
    "\n",
    "seed_dir = os.path.join(TRIAL_DIR, f\"seed_{SEED}\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "print(\n",
    "    pd.read_csv(\n",
    "        os.path.join(seed_dir, \"scaler_x_mean_std.csv\")\n",
    "    )[\"feature\"].tolist()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "167f5a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "out_dir: ./Trial13\\seed_333\\best_by_val_norm\\perm_importance_pack\\test\n",
      "feature_dim: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_56060\\971626391.py:362: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BASE] rmse_cycles = 1569.133789\n",
      "[FEAT]    min_vce perm=1679.955526 delta=110.821737\n",
      "[FEAT]    delta_1 perm=1561.225667 delta=-7.908122\n",
      "[FEAT]    delta_5 perm=1579.987427 delta=10.853638\n",
      "[FEAT]   delta_20 perm=1577.429769 delta=8.295980\n",
      "[FEAT]   delta_50 perm=1543.012939 delta=-26.120850\n",
      "[FEAT]     ema_10 perm=1679.894409 delta=110.760620\n",
      "[FEAT]     ema_50 perm=1683.282430 delta=114.148641\n",
      "[FEAT] rollstd_10 perm=1571.307902 delta=2.174113\n",
      "[FEAT]   win_mean perm=1673.238770 delta=104.104980\n",
      "[FEAT]    win_std perm=1566.012899 delta=-3.120890\n",
      "[FEAT]  win_slope perm=1802.418905 delta=233.285116\n",
      "\n",
      "[DONE] Saved to: ./Trial13\\seed_333\\best_by_val_norm\\perm_importance_pack\\test\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Trial13 PERMUTATION IMPORTANCE PACK (Full Version)\n",
    "# - Feature permutation importance (11 features)\n",
    "# - Time permutation importance (permute a time index across batch)\n",
    "#\n",
    "# Output:\n",
    "#   ./Trial13/seed_<seed>/<ckpt>/perm_importance_pack/<split>/\n",
    "#     01_feature_permutation_importance.csv (+ png)\n",
    "#     02_time_permutation_importance.csv    (+ png)\n",
    "#     used_windows_sample.csv\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# USER CONFIG\n",
    "# ----------------------------\n",
    "DATA_DIR   = r\"C:\\Users\\11HOME_AHCI\\Desktop\\PoF\\Winter 2026\\Ref Data4\\100\"\n",
    "TRIAL_DIR  = r\"./Trial13\"\n",
    "SEED       = 333\n",
    "CKPT_TAG   = \"best_by_val_norm\"   # \"best_by_val_norm\" or \"last_epoch\"\n",
    "SPLIT      = \"test\"               # \"train\"/\"val\"/\"test\"\n",
    "\n",
    "SEQ_LEN      = 100\n",
    "STRIDE       = 5\n",
    "PRED_HORIZON = 0\n",
    "\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS  = 2\n",
    "DROPOUT     = 0.2\n",
    "\n",
    "# permutation eval batch\n",
    "BATCH_SIZE_EVAL = 256\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# subsample windows for speed (None = use all)\n",
    "MAX_WINDOWS = 6000\n",
    "\n",
    "# permutation repeats (stability)\n",
    "N_REPEATS_FEATURE = 3\n",
    "N_REPEATS_TIME    = 2\n",
    "\n",
    "# metric to evaluate (cycles RMSE or norm RMSE)\n",
    "METRIC = \"rmse_cycles\"  # \"rmse_cycles\" or \"rmse_norm\"\n",
    "\n",
    "OUT_DIR = os.path.join(TRIAL_DIR, f\"seed_{SEED}\", CKPT_TAG, \"perm_importance_pack\", SPLIT)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "print(\"out_dir:\", OUT_DIR)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# FIXED feature order (MUST match scaler_x_mean_std.csv you printed)\n",
    "# ----------------------------\n",
    "FEATURES = [\n",
    "    \"min_vce\",\n",
    "    \"delta_1\", \"delta_5\", \"delta_20\", \"delta_50\",\n",
    "    \"ema_10\", \"ema_50\",\n",
    "    \"rollstd_10\",\n",
    "    \"win_mean\", \"win_std\", \"win_slope\",\n",
    "]\n",
    "FEAT_DIM = len(FEATURES)\n",
    "print(\"feature_dim:\", FEAT_DIM)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def list_csv_files(data_dir: str) -> List[Path]:\n",
    "    p = Path(data_dir)\n",
    "    files = sorted([f for f in p.glob(\"*.csv\") if f.is_file()])\n",
    "    if len(files) == 0:\n",
    "        raise FileNotFoundError(f\"No CSV files found in: {data_dir}\")\n",
    "    return files\n",
    "\n",
    "\n",
    "def read_one_csv(csv_path: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    vce = df.iloc[:, 0].astype(np.float32).to_numpy()\n",
    "    rul = df.iloc[:, 1].astype(np.float32).to_numpy()\n",
    "    return vce, rul\n",
    "\n",
    "\n",
    "def compute_delta(v: np.ndarray, k: int) -> np.ndarray:\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    if k <= 0:\n",
    "        return out\n",
    "    out[k:] = v[k:] - v[:-k]\n",
    "    return out\n",
    "\n",
    "\n",
    "def ema(v: np.ndarray, span: int) -> np.ndarray:\n",
    "    # simple EMA via pandas (stable & easy)\n",
    "    return pd.Series(v).ewm(span=span, adjust=False).mean().astype(np.float32).to_numpy()\n",
    "\n",
    "\n",
    "def rolling_std(v: np.ndarray, w: int) -> np.ndarray:\n",
    "    # rolling std (min_periods=1 to avoid NaNs)\n",
    "    return pd.Series(v).rolling(window=w, min_periods=1).std().fillna(0.0).astype(np.float32).to_numpy()\n",
    "\n",
    "\n",
    "def window_stats(v: np.ndarray, seq_len: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    # for each t, compute stats on the last seq_len points ending at t (causal)\n",
    "    # mean/std/slope (linear fit)\n",
    "    T = len(v)\n",
    "    win_mean = np.zeros(T, dtype=np.float32)\n",
    "    win_std  = np.zeros(T, dtype=np.float32)\n",
    "    win_slope = np.zeros(T, dtype=np.float32)\n",
    "\n",
    "    for t in range(T):\n",
    "        s = max(0, t - (seq_len - 1))\n",
    "        wv = v[s:t+1]\n",
    "        win_mean[t] = float(np.mean(wv))\n",
    "        win_std[t]  = float(np.std(wv, ddof=0))\n",
    "        # slope with simple least squares on index\n",
    "        x = np.arange(len(wv), dtype=np.float32)\n",
    "        if len(wv) >= 2:\n",
    "            x_mean = float(np.mean(x))\n",
    "            y_mean = float(np.mean(wv))\n",
    "            num = float(np.sum((x - x_mean) * (wv - y_mean)))\n",
    "            den = float(np.sum((x - x_mean) ** 2)) + 1e-12\n",
    "            win_slope[t] = num / den\n",
    "        else:\n",
    "            win_slope[t] = 0.0\n",
    "\n",
    "    return win_mean, win_std, win_slope\n",
    "\n",
    "\n",
    "def load_scaler_from_csv(seed_dir: str) -> StandardScaler:\n",
    "    path = os.path.join(seed_dir, \"scaler_x_mean_std.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "    # sanity check order matches\n",
    "    got = df[\"feature\"].astype(str).tolist()\n",
    "    if got != FEATURES:\n",
    "        raise ValueError(\n",
    "            \"Feature order mismatch!\\n\"\n",
    "            f\"scaler_x_mean_std.csv: {got}\\n\"\n",
    "            f\"expected FEATURES     : {FEATURES}\\n\"\n",
    "            \"=> Trial13 feature generator order must match scaler order.\"\n",
    "        )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.mean_ = df[\"mean\"].values.astype(np.float64)\n",
    "    std = df[\"std\"].values.astype(np.float64)\n",
    "    scaler.var_ = (std ** 2)\n",
    "    scaler.scale_ = std\n",
    "    scaler.n_features_in_ = len(df)\n",
    "    return scaler\n",
    "\n",
    "\n",
    "def read_split_list(seed_dir: str, split: str) -> List[Path]:\n",
    "    p = os.path.join(seed_dir, f\"{split}_files.csv\")\n",
    "    names = pd.read_csv(p, header=None)[0].astype(str).tolist()\n",
    "    return [Path(DATA_DIR) / n for n in names]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Dataset (rebuild Trial13 features)\n",
    "# ============================================================\n",
    "class Trial13WindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "      x(seq_len, 11), y_norm(1), file, start_idx, cycle_target, y_cycles, rul0\n",
    "    \"\"\"\n",
    "    def __init__(self, file_list: List[Path], seq_len: int, stride: int, pred_horizon: int,\n",
    "                 scaler_x: StandardScaler):\n",
    "        self.file_list = file_list\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.scaler_x = scaler_x\n",
    "\n",
    "        self.series: List[Tuple[str, np.ndarray, np.ndarray, float]] = []\n",
    "        for fp in self.file_list:\n",
    "            vce, rul = read_one_csv(fp)\n",
    "            rul0 = float(rul[0])\n",
    "\n",
    "            # ---- build 11 features (per time index)\n",
    "            d1  = compute_delta(vce, 1)\n",
    "            d5  = compute_delta(vce, 5)\n",
    "            d20 = compute_delta(vce, 20)\n",
    "            d50 = compute_delta(vce, 50)\n",
    "\n",
    "            e10 = ema(vce, 10)\n",
    "            e50 = ema(vce, 50)\n",
    "\n",
    "            rs10 = rolling_std(vce, 10)\n",
    "\n",
    "            wmean, wstd, wslope = window_stats(vce, seq_len)\n",
    "\n",
    "            x = np.stack(\n",
    "                [vce, d1, d5, d20, d50, e10, e50, rs10, wmean, wstd, wslope],\n",
    "                axis=1\n",
    "            ).astype(np.float32)  # (T, 11)\n",
    "\n",
    "            self.series.append((fp.name, x, rul.astype(np.float32), rul0))\n",
    "\n",
    "        # index windows\n",
    "        self.index: List[Tuple[int, int]] = []\n",
    "        for fi, (_name, x, _rul, _rul0) in enumerate(self.series):\n",
    "            T = x.shape[0]\n",
    "            last_start = T - (seq_len + pred_horizon)\n",
    "            if last_start < 0:\n",
    "                continue\n",
    "            for s in range(0, last_start + 1, stride):\n",
    "                self.index.append((fi, s))\n",
    "\n",
    "        if len(self.index) == 0:\n",
    "            raise ValueError(\"No windows created. Check seq_len/pred_horizon vs file lengths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fi, s = self.index[idx]\n",
    "        name, x, rul, rul0 = self.series[fi]\n",
    "        xw = x[s:s + self.seq_len, :]\n",
    "        y_idx = s + self.seq_len - 1 + self.pred_horizon\n",
    "        y_cycles = float(rul[y_idx])\n",
    "        y_norm = np.array([y_cycles / rul0], dtype=np.float32)\n",
    "\n",
    "        xw = self.scaler_x.transform(xw).astype(np.float32)\n",
    "        cycle_target = int(s + (self.seq_len - 1) + self.pred_horizon)\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(xw),\n",
    "            torch.from_numpy(y_norm),\n",
    "            name,\n",
    "            torch.tensor(s, dtype=torch.long),\n",
    "            torch.tensor(cycle_target, dtype=torch.long),\n",
    "            torch.tensor(y_cycles, dtype=torch.float32),\n",
    "            torch.tensor(rul0, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Model\n",
    "# ============================================================\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.head(last)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_metric(model, loader, device, metric: str) -> float:\n",
    "    model.eval()\n",
    "    errs_norm = []\n",
    "    errs_cyc = []\n",
    "\n",
    "    for x, y_norm, _name, _s, _cycle_target, y_cycles, rul0 in loader:  # ✅ 7개로 맞춤\n",
    "        x = x.to(device)\n",
    "        y_norm = y_norm.to(device)\n",
    "        y_cycles = y_cycles.to(device).view(-1, 1)\n",
    "        rul0 = rul0.to(device).view(-1, 1)\n",
    "\n",
    "        pred_norm = model(x)\n",
    "\n",
    "        err_norm = (pred_norm - y_norm).detach().cpu().numpy().reshape(-1)\n",
    "        pred_cyc = (pred_norm * rul0).detach().cpu().numpy().reshape(-1)\n",
    "        err_cyc  = (pred_cyc - y_cycles.detach().cpu().numpy().reshape(-1))\n",
    "\n",
    "        errs_norm.append(err_norm)\n",
    "        errs_cyc.append(err_cyc)\n",
    "\n",
    "    if len(errs_norm) == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    e_norm = np.concatenate(errs_norm)\n",
    "    e_cyc  = np.concatenate(errs_cyc)\n",
    "\n",
    "    if metric == \"rmse_norm\":\n",
    "        return float(np.sqrt(np.mean(e_norm ** 2)))\n",
    "    elif metric == \"rmse_cycles\":\n",
    "        return float(np.sqrt(np.mean(e_cyc ** 2)))\n",
    "    else:\n",
    "        raise ValueError(\"metric must be rmse_norm or rmse_cycles\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    set_seed(0)\n",
    "\n",
    "    seed_dir = os.path.join(TRIAL_DIR, f\"seed_{SEED}\")\n",
    "    ckpt_path = os.path.join(seed_dir, f\"{CKPT_TAG}.pt\")\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        raise FileNotFoundError(f\"Missing ckpt: {ckpt_path}\")\n",
    "\n",
    "    # load scaler + build dataset\n",
    "    scaler_x = load_scaler_from_csv(seed_dir)\n",
    "    files = read_split_list(seed_dir, SPLIT)\n",
    "\n",
    "    ds = Trial13WindowDataset(files, SEQ_LEN, STRIDE, PRED_HORIZON, scaler_x=scaler_x)\n",
    "\n",
    "    # optional subsample windows (for speed)\n",
    "    if MAX_WINDOWS is not None and len(ds) > MAX_WINDOWS:\n",
    "        # create a small wrapper index\n",
    "        idxs = np.random.RandomState(0).choice(len(ds), size=int(MAX_WINDOWS), replace=False)\n",
    "        idxs = np.sort(idxs)\n",
    "\n",
    "        class Subset(Dataset):\n",
    "            def __init__(self, base, idxs):\n",
    "                self.base = base\n",
    "                self.idxs = idxs\n",
    "            def __len__(self): return len(self.idxs)\n",
    "            def __getitem__(self, i): return self.base[int(self.idxs[i])]\n",
    "\n",
    "        ds_used = Subset(ds, idxs)\n",
    "    else:\n",
    "        ds_used = ds\n",
    "\n",
    "    loader = DataLoader(ds_used, batch_size=BATCH_SIZE_EVAL, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # save which windows were used (debug)\n",
    "    rows = []\n",
    "    for i in range(min(len(ds_used), 2000)):\n",
    "        x, y_norm, name, s, cyc, y_cyc, r0 = ds_used[i]\n",
    "        rows.append({\"file\": str(name), \"start_idx\": int(s), \"cycle\": int(cyc), \"y_true\": float(y_cyc)})\n",
    "    pd.DataFrame(rows).to_csv(os.path.join(OUT_DIR, \"used_windows_sample.csv\"), index=False)\n",
    "\n",
    "    # build model with correct input_size=11\n",
    "    model = LSTMRegressor(FEAT_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT).to(device)\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    base = eval_metric(model, loader, device, METRIC)\n",
    "    print(f\"[BASE] {METRIC} = {base:.6f}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # (1) Feature permutation importance\n",
    "    # ============================================================\n",
    "    feat_imp = []\n",
    "    X_cache = []\n",
    "\n",
    "    # cache all batches once (so repeated permutations are consistent)\n",
    "    for batch in loader:\n",
    "        xb = batch[0].to(device)  # (B,T,F)\n",
    "        y_norm = batch[1].to(device)\n",
    "        y_cyc  = batch[5].to(device)  # (B,)\n",
    "        r0     = batch[6].to(device)\n",
    "        X_cache.append((xb, y_norm, y_cyc, r0))\n",
    "\n",
    "    for j, feat_name in enumerate(FEATURES):\n",
    "        scores = []\n",
    "        for rep in range(N_REPEATS_FEATURE):\n",
    "            # evaluate with feature-j permuted across batch (keep time structure)\n",
    "            errs_norm = []\n",
    "            errs_cyc = []\n",
    "            for xb, y_norm, y_cyc, r0 in X_cache:\n",
    "                xperm = xb.clone()\n",
    "                perm_idx = torch.randperm(xperm.size(0), device=device)\n",
    "                xperm[:, :, j] = xperm[perm_idx, :, j]\n",
    "\n",
    "                pred_norm = model(xperm)\n",
    "                err_norm = (pred_norm - y_norm).detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "                pred_cyc = (pred_norm * r0.view(-1,1)).detach().cpu().numpy().reshape(-1)\n",
    "                err_cyc  = (pred_cyc - y_cyc.detach().cpu().numpy().reshape(-1))\n",
    "\n",
    "                errs_norm.append(err_norm)\n",
    "                errs_cyc.append(err_cyc)\n",
    "\n",
    "            e_norm = np.concatenate(errs_norm)\n",
    "            e_cyc  = np.concatenate(errs_cyc)\n",
    "\n",
    "            if METRIC == \"rmse_norm\":\n",
    "                score = float(np.sqrt(np.mean(e_norm**2)))\n",
    "            else:\n",
    "                score = float(np.sqrt(np.mean(e_cyc**2)))\n",
    "\n",
    "            scores.append(score)\n",
    "\n",
    "        mean_score = float(np.mean(scores))\n",
    "        delta = mean_score - base\n",
    "        feat_imp.append({\"feature\": feat_name, \"perm_metric_mean\": mean_score, \"delta(+worse)\": delta})\n",
    "        print(f\"[FEAT] {feat_name:>10s} perm={mean_score:.6f} delta={delta:.6f}\")\n",
    "\n",
    "    df_feat = pd.DataFrame(feat_imp).sort_values(\"delta(+worse)\", ascending=False)\n",
    "    df_feat.to_csv(os.path.join(OUT_DIR, \"01_feature_permutation_importance.csv\"), index=False)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.bar(df_feat[\"feature\"].values, df_feat[\"delta(+worse)\"].values)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylabel(f\"Δ {METRIC} (permuted - base)\")\n",
    "    plt.title(f\"Trial13 Feature Permutation Importance | {SPLIT} | {CKPT_TAG} | seed={SEED}\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(OUT_DIR, \"01_feature_permutation_importance.png\"), dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # ============================================================\n",
    "    # (2) Time permutation importance (permute one time index across batch)\n",
    "    # ============================================================\n",
    "    time_imp = []\n",
    "    for t in range(SEQ_LEN):\n",
    "        scores = []\n",
    "        for rep in range(N_REPEATS_TIME):\n",
    "            errs_norm = []\n",
    "            errs_cyc = []\n",
    "            for xb, y_norm, y_cyc, r0 in X_cache:\n",
    "                xperm = xb.clone()\n",
    "                perm_idx = torch.randperm(xperm.size(0), device=device)\n",
    "                # permute ALL features at time t across batch\n",
    "                xperm[:, t, :] = xperm[perm_idx, t, :]\n",
    "\n",
    "                pred_norm = model(xperm)\n",
    "                err_norm = (pred_norm - y_norm).detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "                pred_cyc = (pred_norm * r0.view(-1,1)).detach().cpu().numpy().reshape(-1)\n",
    "                err_cyc  = (pred_cyc - y_cyc.detach().cpu().numpy().reshape(-1))\n",
    "\n",
    "                errs_norm.append(err_norm)\n",
    "                errs_cyc.append(err_cyc)\n",
    "\n",
    "            e_norm = np.concatenate(errs_norm)\n",
    "            e_cyc  = np.concatenate(errs_cyc)\n",
    "\n",
    "            if METRIC == \"rmse_norm\":\n",
    "                score = float(np.sqrt(np.mean(e_norm**2)))\n",
    "            else:\n",
    "                score = float(np.sqrt(np.mean(e_cyc**2)))\n",
    "            scores.append(score)\n",
    "\n",
    "        mean_score = float(np.mean(scores))\n",
    "        delta = mean_score - base\n",
    "        time_imp.append({\"t_in_window\": t, \"perm_metric_mean\": mean_score, \"delta(+worse)\": delta})\n",
    "\n",
    "    df_time = pd.DataFrame(time_imp).sort_values(\"t_in_window\")\n",
    "    df_time.to_csv(os.path.join(OUT_DIR, \"02_time_permutation_importance.csv\"), index=False)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(df_time[\"t_in_window\"].values, df_time[\"delta(+worse)\"].values)\n",
    "    plt.xlabel(\"t in window\")\n",
    "    plt.ylabel(f\"Δ {METRIC} (permuted - base)\")\n",
    "    plt.title(f\"Trial13 Time Permutation Importance | {SPLIT} | {CKPT_TAG} | seed={SEED}\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(OUT_DIR, \"02_time_permutation_importance.png\"), dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(\"\\n[DONE] Saved to:\", OUT_DIR)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e42aa1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "OUT_DIR: ./Trial13\\seed_333\\best_by_val_norm\\perm_importance_pack\\test\n",
      "feature_dim: 11\n",
      "features: ['min_vce', 'delta_1', 'delta_5', 'delta_20', 'delta_50', 'ema_10', 'ema_50', 'rollstd_10', 'win_mean', 'win_std', 'win_slope']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_56060\\1962916574.py:448: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BASE] rmse_cycles = 1569.555786\n",
      "[Saved] 01_feature_permutation_importance.csv (+png)\n",
      "[Saved] 02_time_permutation_importance.csv (+png)\n",
      "\n",
      "[DONE] Saved to: ./Trial13\\seed_333\\best_by_val_norm\\perm_importance_pack\\test\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Trial13 Permutation Importance Pack (FULL)\n",
    "# - Feature permutation importance\n",
    "# - Time permutation importance (permute time indices inside window)\n",
    "#\n",
    "# Outputs:\n",
    "#  ./Trial13/seed_<seed>/<ckpt>/perm_importance_pack/<split>/\n",
    "#    00_base_metric.txt\n",
    "#    01_feature_permutation_importance.csv (+png)\n",
    "#    02_time_permutation_importance.csv (+png)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# USER CONFIG\n",
    "# ----------------------------\n",
    "DATA_DIR   = r\"C:\\Users\\11HOME_AHCI\\Desktop\\PoF\\Winter 2026\\Ref Data4\\100\"\n",
    "TRIAL_DIR  = r\"./Trial13\"\n",
    "SEED       = 333\n",
    "CKPT_TAG   = \"best_by_val_norm\"     # or \"last_epoch\"\n",
    "SPLIT      = \"test\"                 # train/val/test\n",
    "\n",
    "SEQ_LEN      = 100\n",
    "STRIDE       = 5\n",
    "PRED_HORIZON = 0\n",
    "\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS  = 2\n",
    "DROPOUT     = 0.2\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# permutation settings\n",
    "N_SAMPLES = 2000          # evaluate on at most N windows (speed)\n",
    "TIME_PERM_TRIALS = 3      # repeat time permutation to reduce randomness\n",
    "\n",
    "# metric to evaluate (lower is better)\n",
    "METRIC = \"rmse_cycles\"    # \"mae_cycles\" or \"rmse_cycles\"\n",
    "\n",
    "OUT_DIR = os.path.join(TRIAL_DIR, f\"seed_{SEED}\", CKPT_TAG, \"perm_importance_pack\", SPLIT)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helpers: scaler + features\n",
    "# ============================================================\n",
    "def load_scaler_and_features(seed_dir: str) -> Tuple[StandardScaler, List[str]]:\n",
    "    path = os.path.join(seed_dir, \"scaler_x_mean_std.csv\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Missing scaler file: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    feats = df[\"feature\"].astype(str).tolist()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.mean_ = df[\"mean\"].values.astype(np.float64)\n",
    "    std = df[\"std\"].values.astype(np.float64)\n",
    "    scaler.var_ = (std ** 2)\n",
    "    scaler.scale_ = std\n",
    "    scaler.n_features_in_ = len(feats)\n",
    "    return scaler, feats\n",
    "\n",
    "\n",
    "def read_one_csv(csv_path: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    vce = df.iloc[:, 0].astype(np.float32).to_numpy()\n",
    "    rul = df.iloc[:, 1].astype(np.float32).to_numpy()\n",
    "    return vce, rul\n",
    "\n",
    "\n",
    "def compute_deltas(v: np.ndarray, ks: List[int]) -> Dict[int, np.ndarray]:\n",
    "    out = {}\n",
    "    for k in ks:\n",
    "        d = np.zeros_like(v, dtype=np.float32)\n",
    "        d[k:] = v[k:] - v[:-k]\n",
    "        out[k] = d\n",
    "    return out\n",
    "\n",
    "\n",
    "def ema(v: np.ndarray, span: int) -> np.ndarray:\n",
    "    # simple EMA (pandas-like)\n",
    "    alpha = 2.0 / (span + 1.0)\n",
    "    y = np.zeros_like(v, dtype=np.float32)\n",
    "    y[0] = v[0]\n",
    "    for i in range(1, len(v)):\n",
    "        y[i] = alpha * v[i] + (1 - alpha) * y[i - 1]\n",
    "    return y\n",
    "\n",
    "\n",
    "def rolling_std(v: np.ndarray, win: int) -> np.ndarray:\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    for i in range(len(v)):\n",
    "        s = max(0, i - win + 1)\n",
    "        out[i] = np.std(v[s:i+1], ddof=0).astype(np.float32)\n",
    "    return out\n",
    "\n",
    "\n",
    "def window_stats(v_window: np.ndarray) -> Tuple[float, float, float]:\n",
    "    # mean, std, slope (linear fit)\n",
    "    x = np.arange(len(v_window), dtype=np.float32)\n",
    "    y = v_window.astype(np.float32)\n",
    "    m = float(np.mean(y))\n",
    "    s = float(np.std(y, ddof=0))\n",
    "    # slope via least squares\n",
    "    x_mean = float(np.mean(x))\n",
    "    y_mean = float(np.mean(y))\n",
    "    denom = float(np.sum((x - x_mean) ** 2)) + 1e-12\n",
    "    slope = float(np.sum((x - x_mean) * (y - y_mean)) / denom)\n",
    "    return m, s, slope\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Dataset: build the EXACT Trial13 feature set\n",
    "#   feature order must match scaler_x_mean_std.csv\n",
    "# ============================================================\n",
    "class WindowedTrial13Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    returns a batch tuple with at least:\n",
    "      x(T,F), y_norm(1), name, start_idx, cycle_target, y_cycles, rul0\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 file_list: List[Path],\n",
    "                 seq_len: int,\n",
    "                 stride: int,\n",
    "                 pred_horizon: int,\n",
    "                 scaler_x: StandardScaler,\n",
    "                 feature_names: List[str]):\n",
    "\n",
    "        self.file_list = file_list\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.scaler_x = scaler_x\n",
    "        self.feature_names = feature_names\n",
    "\n",
    "        # config inferred from names\n",
    "        self.delta_ks = [1,5,20,50]\n",
    "        self.ema_spans = [10,50]\n",
    "        self.rollstd_win = 10\n",
    "\n",
    "        self.series = []  # (name, X(T,F), rul(T), rul0)\n",
    "        for fp in self.file_list:\n",
    "            vce, rul = read_one_csv(fp)\n",
    "            rul0 = float(rul[0])\n",
    "            if rul0 <= 0:\n",
    "                continue\n",
    "\n",
    "            # base signals\n",
    "            d = compute_deltas(vce, self.delta_ks)\n",
    "            ema10 = ema(vce, 10)\n",
    "            ema50 = ema(vce, 50)\n",
    "            rs10 = rolling_std(vce, self.rollstd_win)\n",
    "\n",
    "            # build per-time features first (some window stats appended per-time)\n",
    "            feats_by_name = {\n",
    "                \"min_vce\": vce,\n",
    "                \"delta_1\": d[1],\n",
    "                \"delta_5\": d[5],\n",
    "                \"delta_20\": d[20],\n",
    "                \"delta_50\": d[50],\n",
    "                \"ema_10\": ema10,\n",
    "                \"ema_50\": ema50,\n",
    "                \"rollstd_10\": rs10,\n",
    "            }\n",
    "\n",
    "            # window stats features depend on window → easiest is to compute later per window\n",
    "            # but we want x(T,F) per sample, so we will compute X on the fly in __getitem__\n",
    "            # store raw arrays\n",
    "            self.series.append((fp.name, vce, feats_by_name, rul.astype(np.float32), rul0))\n",
    "\n",
    "        # index windows\n",
    "        self.index = []\n",
    "        for fi, (_name, vce, _fbn, rul, _r0) in enumerate(self.series):\n",
    "            T = len(vce)\n",
    "            last_start = T - (seq_len + pred_horizon)\n",
    "            if last_start < 0:\n",
    "                continue\n",
    "            for s in range(0, last_start + 1, stride):\n",
    "                self.index.append((fi, s))\n",
    "\n",
    "        if len(self.index) == 0:\n",
    "            raise ValueError(\"No windows created (check seq_len/stride).\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fi, s = self.index[idx]\n",
    "        name, vce, fbn, rul, rul0 = self.series[fi]\n",
    "\n",
    "        y_idx = s + self.seq_len - 1 + self.pred_horizon\n",
    "        y_cycles = float(rul[y_idx])\n",
    "        y_norm = np.array([y_cycles / rul0], dtype=np.float32)\n",
    "        cycle_target = int(y_idx)\n",
    "\n",
    "        # window stats (computed from raw min_vce window)\n",
    "        v_win = vce[s:s+self.seq_len]\n",
    "        w_mean, w_std, w_slope = window_stats(v_win)\n",
    "\n",
    "        # assemble X in feature_names order\n",
    "        X = np.zeros((self.seq_len, len(self.feature_names)), dtype=np.float32)\n",
    "        for j, fn in enumerate(self.feature_names):\n",
    "            if fn in fbn:\n",
    "                X[:, j] = fbn[fn][s:s+self.seq_len]\n",
    "            elif fn == \"win_mean\":\n",
    "                X[:, j] = w_mean\n",
    "            elif fn == \"win_std\":\n",
    "                X[:, j] = w_std\n",
    "            elif fn == \"win_slope\":\n",
    "                X[:, j] = w_slope\n",
    "            else:\n",
    "                raise KeyError(f\"Unknown feature name: {fn}\")\n",
    "\n",
    "        # scale\n",
    "        X = self.scaler_x.transform(X).astype(np.float32)\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(X),\n",
    "            torch.from_numpy(y_norm),\n",
    "            name,\n",
    "            torch.tensor(s, dtype=torch.long),\n",
    "            torch.tensor(cycle_target, dtype=torch.long),\n",
    "            torch.tensor(y_cycles, dtype=torch.float32),\n",
    "            torch.tensor(rul0, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Model\n",
    "# ============================================================\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.head(last)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Eval metric\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def eval_metric(model: nn.Module, loader: DataLoader, device, metric: str) -> float:\n",
    "    model.eval()\n",
    "    abs_errs = []\n",
    "    sq_errs = []\n",
    "\n",
    "    for batch in loader:\n",
    "        x = batch[0].to(device)\n",
    "        y_norm = batch[1].to(device)\n",
    "        y_cycles = batch[-2].to(device).view(-1, 1)\n",
    "        rul0 = batch[-1].to(device).view(-1, 1)\n",
    "\n",
    "        pred_norm = model(x)\n",
    "        pred_cycles = pred_norm * rul0\n",
    "\n",
    "        err = (pred_cycles - y_cycles).detach().cpu().numpy().reshape(-1)\n",
    "        abs_errs.append(np.abs(err))\n",
    "        sq_errs.append(err**2)\n",
    "\n",
    "    abs_err = np.concatenate(abs_errs) if abs_errs else np.array([], dtype=np.float32)\n",
    "    sq_err = np.concatenate(sq_errs) if sq_errs else np.array([], dtype=np.float32)\n",
    "\n",
    "    if metric == \"mae_cycles\":\n",
    "        return float(np.mean(abs_err)) if len(abs_err) else np.nan\n",
    "    if metric == \"rmse_cycles\":\n",
    "        return float(np.sqrt(np.mean(sq_err))) if len(sq_err) else np.nan\n",
    "    raise ValueError(\"metric must be 'mae_cycles' or 'rmse_cycles'\")\n",
    "\n",
    "\n",
    "def subsample_loader(loader: DataLoader, max_n: int) -> DataLoader:\n",
    "    # easiest: just stop early in evaluation by wrapping; but we need same loader object\n",
    "    # We'll instead create a new loader with a Subset of dataset indices.\n",
    "    ds = loader.dataset\n",
    "    n = len(ds)\n",
    "    if n <= max_n:\n",
    "        return loader\n",
    "    idx = np.random.RandomState(0).choice(n, size=max_n, replace=False)\n",
    "    subset = torch.utils.data.Subset(ds, idx.tolist())\n",
    "    return DataLoader(subset, batch_size=loader.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Permutation importance\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def feature_permutation_importance(model, loader, device, feature_names: List[str], base: float) -> pd.DataFrame:\n",
    "    ds = loader.dataset\n",
    "    F = len(feature_names)\n",
    "\n",
    "    rows = []\n",
    "    for fj in range(F):\n",
    "        metric_vals = []\n",
    "\n",
    "        for batch in loader:\n",
    "            x = batch[0].to(device)           # (B,T,F)\n",
    "            y_norm = batch[1].to(device)\n",
    "            y_cycles = batch[-2].to(device).view(-1, 1)\n",
    "            rul0 = batch[-1].to(device).view(-1, 1)\n",
    "\n",
    "            # permute feature fj across batch (keeps time structure)\n",
    "            xp = x.clone()\n",
    "            perm = torch.randperm(x.size(0), device=device)\n",
    "            xp[:, :, fj] = xp[perm, :, fj]\n",
    "\n",
    "            pred_norm = model(xp)\n",
    "            pred_cycles = pred_norm * rul0\n",
    "            err = (pred_cycles - y_cycles).detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "            if METRIC == \"mae_cycles\":\n",
    "                metric_vals.append(float(np.mean(np.abs(err))))\n",
    "            else:\n",
    "                metric_vals.append(float(np.sqrt(np.mean(err**2))))\n",
    "\n",
    "        perm_metric = float(np.mean(metric_vals)) if metric_vals else np.nan\n",
    "        rows.append({\n",
    "            \"feature\": feature_names[fj],\n",
    "            \"perm_metric_mean\": perm_metric,\n",
    "            \"delta(+worse)\": perm_metric - base,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values(\"delta(+worse)\", ascending=False).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def time_permutation_importance(model, loader, device, base: float, trials: int = 3) -> pd.DataFrame:\n",
    "    # permute time order inside window (same permutation for all features)\n",
    "    T = SEQ_LEN\n",
    "    deltas = []\n",
    "\n",
    "    for t in range(T):\n",
    "        # here: we disrupt one timestep position by shuffling its values with another random timestep\n",
    "        # simpler & stable: shuffle the values at time t across batch (keeps other times intact)\n",
    "        trial_vals = []\n",
    "        for _ in range(trials):\n",
    "            metric_vals = []\n",
    "            for batch in loader:\n",
    "                x = batch[0].to(device)  # (B,T,F)\n",
    "                y_cycles = batch[-2].to(device).view(-1, 1)\n",
    "                rul0 = batch[-1].to(device).view(-1, 1)\n",
    "\n",
    "                xp = x.clone()\n",
    "                perm = torch.randperm(x.size(0), device=device)\n",
    "                xp[:, t, :] = xp[perm, t, :]   # shuffle that timestep across batch\n",
    "\n",
    "                pred_norm = model(xp)\n",
    "                pred_cycles = pred_norm * rul0\n",
    "                err = (pred_cycles - y_cycles).detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "                if METRIC == \"mae_cycles\":\n",
    "                    metric_vals.append(float(np.mean(np.abs(err))))\n",
    "                else:\n",
    "                    metric_vals.append(float(np.sqrt(np.mean(err**2))))\n",
    "\n",
    "            trial_vals.append(float(np.mean(metric_vals)) if metric_vals else np.nan)\n",
    "\n",
    "        perm_metric = float(np.nanmean(trial_vals))\n",
    "        deltas.append(perm_metric - base)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"t_in_window\": np.arange(T),\n",
    "        \"perm_metric_mean\": (np.array(deltas) + base),\n",
    "        \"delta(+worse)\": np.array(deltas),\n",
    "    }).sort_values(\"delta(+worse)\", ascending=False).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_bar(df: pd.DataFrame, xcol: str, ycol: str, out_png: str, title: str, topk: Optional[int] = None):\n",
    "    os.makedirs(os.path.dirname(out_png), exist_ok=True)\n",
    "    d = df.copy()\n",
    "    if topk is not None:\n",
    "        d = d.head(topk)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(d[xcol].astype(str).values, d[ycol].values)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ycol)\n",
    "    plt.grid(True, axis=\"y\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "def main():\n",
    "    seed_dir = os.path.join(TRIAL_DIR, f\"seed_{SEED}\")\n",
    "    ckpt_path = os.path.join(seed_dir, f\"{CKPT_TAG}.pt\")\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        raise FileNotFoundError(f\"Missing ckpt: {ckpt_path}\")\n",
    "\n",
    "    scaler_x, feature_names = load_scaler_and_features(seed_dir)\n",
    "    input_dim = len(feature_names)\n",
    "    print(\"feature_dim:\", input_dim)\n",
    "    print(\"features:\", feature_names)\n",
    "\n",
    "    # split file list\n",
    "    split_list_path = os.path.join(seed_dir, f\"{SPLIT}_files.csv\")\n",
    "    names = pd.read_csv(split_list_path, header=None)[0].astype(str).tolist()\n",
    "    file_list = [Path(DATA_DIR) / n for n in names]\n",
    "\n",
    "    ds = WindowedTrial13Dataset(\n",
    "        file_list=file_list,\n",
    "        seq_len=SEQ_LEN,\n",
    "        stride=STRIDE,\n",
    "        pred_horizon=PRED_HORIZON,\n",
    "        scaler_x=scaler_x,\n",
    "        feature_names=feature_names,\n",
    "    )\n",
    "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    loader = subsample_loader(loader, N_SAMPLES)\n",
    "\n",
    "    # model\n",
    "    model = LSTMRegressor(input_dim, HIDDEN_SIZE, NUM_LAYERS, DROPOUT).to(device)\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # baseline metric\n",
    "    base = eval_metric(model, loader, device, METRIC)\n",
    "    with open(os.path.join(OUT_DIR, \"00_base_metric.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"METRIC={METRIC}\\nBASE={base:.6f}\\n\")\n",
    "    print(f\"[BASE] {METRIC} = {base:.6f}\")\n",
    "\n",
    "    # (1) Feature permutation\n",
    "    df_feat = feature_permutation_importance(model, loader, device, feature_names, base)\n",
    "    df_feat.to_csv(os.path.join(OUT_DIR, \"01_feature_permutation_importance.csv\"), index=False)\n",
    "    save_bar(\n",
    "        df_feat, \"feature\", \"delta(+worse)\",\n",
    "        os.path.join(OUT_DIR, \"01_feature_permutation_importance.png\"),\n",
    "        title=f\"Feature Permutation Importance (delta worse) | {SPLIT} | {CKPT_TAG}\",\n",
    "        topk=None\n",
    "    )\n",
    "    print(\"[Saved] 01_feature_permutation_importance.csv (+png)\")\n",
    "\n",
    "    # (2) Time permutation\n",
    "    df_time = time_permutation_importance(model, loader, device, base, trials=TIME_PERM_TRIALS)\n",
    "    df_time.to_csv(os.path.join(OUT_DIR, \"02_time_permutation_importance.csv\"), index=False)\n",
    "    save_bar(\n",
    "        df_time, \"t_in_window\", \"delta(+worse)\",\n",
    "        os.path.join(OUT_DIR, \"02_time_permutation_importance.png\"),\n",
    "        title=f\"Time Permutation Importance (delta worse) | {SPLIT} | {CKPT_TAG} (top time steps)\",\n",
    "        topk=30\n",
    "    )\n",
    "    print(\"[Saved] 02_time_permutation_importance.csv (+png)\")\n",
    "\n",
    "    print(\"\\n[DONE] Saved to:\", OUT_DIR)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igbt_rnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
