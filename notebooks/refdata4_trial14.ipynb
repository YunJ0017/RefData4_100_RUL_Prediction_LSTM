{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0998504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "[SEED 9819123] device=cuda\n",
      "[SEED 9819123] out=./Trial14\\seed_9819123\n",
      "==============================\n",
      "[SEED 9819123] [001/300] train_mse_norm=0.022296 | val_rmse_norm=0.164454 | val_mae_cycles=2304.912 | best_val_rmse_norm=0.164454\n",
      "[SEED 9819123] [010/300] train_mse_norm=0.016629 | val_rmse_norm=0.159979 | val_mae_cycles=2244.534 | best_val_rmse_norm=0.158172\n",
      "[SEED 9819123] [020/300] train_mse_norm=0.016705 | val_rmse_norm=0.157696 | val_mae_cycles=2175.083 | best_val_rmse_norm=0.157696\n",
      "[SEED 9819123] [030/300] train_mse_norm=0.016088 | val_rmse_norm=0.163209 | val_mae_cycles=2224.854 | best_val_rmse_norm=0.157696\n",
      "[SEED 9819123] [040/300] train_mse_norm=0.012689 | val_rmse_norm=0.162197 | val_mae_cycles=2132.374 | best_val_rmse_norm=0.157378\n",
      "[SEED 9819123] [050/300] train_mse_norm=0.002280 | val_rmse_norm=0.179738 | val_mae_cycles=2325.509 | best_val_rmse_norm=0.157378\n",
      "[SEED 9819123] [060/300] train_mse_norm=0.000608 | val_rmse_norm=0.177955 | val_mae_cycles=2354.379 | best_val_rmse_norm=0.157378\n",
      "[SEED 9819123] Early stopping at epoch 69.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_31012\\1448923074.py:977: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 9819123] best_by_val_norm: TEST mae_cycles=1033.154 | rmse_cycles=1599.707 | rmse_norm=0.111484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_31012\\1448923074.py:977: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 9819123] last_epoch: TEST mae_cycles=1174.077 | rmse_cycles=1787.581 | rmse_norm=0.123437\n",
      "\n",
      "==============================\n",
      "[SEED 111] device=cuda\n",
      "[SEED 111] out=./Trial14\\seed_111\n",
      "==============================\n",
      "[SEED 111] [001/300] train_mse_norm=0.042643 | val_rmse_norm=0.134463 | val_mae_cycles=2576.141 | best_val_rmse_norm=0.134463\n",
      "[SEED 111] [010/300] train_mse_norm=0.018061 | val_rmse_norm=0.146745 | val_mae_cycles=2855.620 | best_val_rmse_norm=0.134463\n",
      "[SEED 111] [020/300] train_mse_norm=0.015565 | val_rmse_norm=0.144623 | val_mae_cycles=2824.658 | best_val_rmse_norm=0.132038\n",
      "[SEED 111] [030/300] train_mse_norm=0.012059 | val_rmse_norm=0.159950 | val_mae_cycles=3085.744 | best_val_rmse_norm=0.132038\n",
      "[SEED 111] [040/300] train_mse_norm=0.002024 | val_rmse_norm=0.163257 | val_mae_cycles=3282.342 | best_val_rmse_norm=0.132038\n",
      "[SEED 111] Early stopping at epoch 41.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_31012\\1448923074.py:977: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 111] best_by_val_norm: TEST mae_cycles=1809.596 | rmse_cycles=2851.311 | rmse_norm=0.151585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_31012\\1448923074.py:977: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 111] last_epoch: TEST mae_cycles=1585.218 | rmse_cycles=2529.418 | rmse_norm=0.144033\n",
      "\n",
      "==============================\n",
      "[SEED 222] device=cuda\n",
      "[SEED 222] out=./Trial14\\seed_222\n",
      "==============================\n",
      "[SEED 222] [001/300] train_mse_norm=0.022733 | val_rmse_norm=0.139011 | val_mae_cycles=1575.178 | best_val_rmse_norm=0.139011\n",
      "[SEED 222] [010/300] train_mse_norm=0.018254 | val_rmse_norm=0.141020 | val_mae_cycles=1557.519 | best_val_rmse_norm=0.136616\n",
      "[SEED 222] [020/300] train_mse_norm=0.017861 | val_rmse_norm=0.140039 | val_mae_cycles=1558.372 | best_val_rmse_norm=0.136616\n",
      "[SEED 222] [030/300] train_mse_norm=0.009804 | val_rmse_norm=0.154699 | val_mae_cycles=1661.187 | best_val_rmse_norm=0.136616\n",
      "[SEED 222] Early stopping at epoch 35.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_31012\\1448923074.py:977: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 222] best_by_val_norm: TEST mae_cycles=2127.510 | rmse_cycles=3145.102 | rmse_norm=0.128156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_31012\\1448923074.py:977: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 222] last_epoch: TEST mae_cycles=2404.082 | rmse_cycles=3605.987 | rmse_norm=0.145702\n",
      "\n",
      "==============================\n",
      "[SEED 333] device=cuda\n",
      "[SEED 333] out=./Trial14\\seed_333\n",
      "==============================\n",
      "[SEED 333] [001/300] train_mse_norm=0.023093 | val_rmse_norm=0.123515 | val_mae_cycles=1496.794 | best_val_rmse_norm=0.123515\n",
      "[SEED 333] [010/300] train_mse_norm=0.018690 | val_rmse_norm=0.125106 | val_mae_cycles=1483.803 | best_val_rmse_norm=0.123360\n",
      "[SEED 333] [020/300] train_mse_norm=0.018895 | val_rmse_norm=0.126090 | val_mae_cycles=1485.783 | best_val_rmse_norm=0.121881\n",
      "[SEED 333] [030/300] train_mse_norm=0.013758 | val_rmse_norm=0.131944 | val_mae_cycles=1493.846 | best_val_rmse_norm=0.121881\n",
      "[SEED 333] [040/300] train_mse_norm=0.002140 | val_rmse_norm=0.143231 | val_mae_cycles=1695.928 | best_val_rmse_norm=0.121881\n",
      "[SEED 333] Early stopping at epoch 41.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_31012\\1448923074.py:977: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 333] best_by_val_norm: TEST mae_cycles=971.831 | rmse_cycles=1544.697 | rmse_norm=0.146952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_31012\\1448923074.py:977: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 333] last_epoch: TEST mae_cycles=962.831 | rmse_cycles=1647.031 | rmse_norm=0.157552\n",
      "\n",
      "==============================\n",
      "[SEED 444] device=cuda\n",
      "[SEED 444] out=./Trial14\\seed_444\n",
      "==============================\n",
      "[SEED 444] [001/300] train_mse_norm=0.035879 | val_rmse_norm=0.143768 | val_mae_cycles=2073.267 | best_val_rmse_norm=0.143768\n",
      "[SEED 444] [010/300] train_mse_norm=0.017693 | val_rmse_norm=0.144096 | val_mae_cycles=2125.801 | best_val_rmse_norm=0.138893\n",
      "[SEED 444] [020/300] train_mse_norm=0.017420 | val_rmse_norm=0.139118 | val_mae_cycles=2021.584 | best_val_rmse_norm=0.138893\n",
      "[SEED 444] [030/300] train_mse_norm=0.017252 | val_rmse_norm=0.141903 | val_mae_cycles=2121.118 | best_val_rmse_norm=0.138715\n",
      "[SEED 444] [040/300] train_mse_norm=0.011903 | val_rmse_norm=0.149142 | val_mae_cycles=2161.659 | best_val_rmse_norm=0.135164\n",
      "[SEED 444] [050/300] train_mse_norm=0.001577 | val_rmse_norm=0.157321 | val_mae_cycles=2259.819 | best_val_rmse_norm=0.135164\n",
      "[SEED 444] [060/300] train_mse_norm=0.000483 | val_rmse_norm=0.157743 | val_mae_cycles=2259.042 | best_val_rmse_norm=0.135164\n",
      "[SEED 444] Early stopping at epoch 65.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_31012\\1448923074.py:977: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 444] best_by_val_norm: TEST mae_cycles=1321.993 | rmse_cycles=1926.283 | rmse_norm=0.132900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_31012\\1448923074.py:977: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 444] last_epoch: TEST mae_cycles=1421.075 | rmse_cycles=2184.222 | rmse_norm=0.145460\n",
      "=== WIN-RATE SUMMARY (TEST; lower is better) ===\n",
      "- test_mae_cycles: last wins=2, best wins=3, ties=0 | mean(last-best)=56.639665, std(last-best)=167.663100\n",
      "- test_rmse_cycles: last wins=1, best wins=4, ties=0 | mean(last-best)=137.428133, std(last-best)=258.418174\n",
      "- test_mae_norm: last wins=1, best wins=4, ties=0 | mean(last-best)=0.003877, std(last-best)=0.007330\n",
      "- test_rmse_norm: last wins=1, best wins=4, ties=0 | mean(last-best)=0.009022, std(last-best)=0.008613\n",
      "\n",
      "=== MEAN ± STD across seeds (TEST) ===\n",
      "                 test_mae_cycles             test_rmse_cycles              \\\n",
      "                            mean         std             mean         std   \n",
      "checkpoint                                                                  \n",
      "best_by_val_norm     1452.816845  501.766993      2213.419915  738.447356   \n",
      "last_epoch           1509.456509  553.440342      2350.848048  782.249073   \n",
      "\n",
      "                 test_mae_norm           test_rmse_norm            \n",
      "                          mean       std           mean       std  \n",
      "checkpoint                                                         \n",
      "best_by_val_norm      0.102368  0.014668       0.134215  0.015962  \n",
      "last_epoch            0.106245  0.009949       0.143237  0.012335  \n",
      "\n",
      "Saved:\n",
      " - ./Trial14\\summary_across_seeds.csv\n",
      " - ./Trial14\\win_rate_summary.csv\n",
      " - ./Trial14\\win_rate_summary.txt\n",
      "\n",
      "DONE. Check Trial14 folder:\n",
      " - per seed results: Trial14/seed_<seed>/...\n",
      " - figures (paper-style): seed_<seed>/<ckpt>/paper_figures/<split>/\n",
      " - cycle sequence mean CSV: <ckpt>/<split>_cycle_sequence_mean.csv\n",
      " - PH/α–λ metrics CSV: <ckpt>/<split>_prognostics_metrics_per_file.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Trial14: Trial13 + (A) Temporal Pooling + (B) Late-step Masking\n",
    "#\n",
    "# Motivation:\n",
    "# - Your time permutation importance showed strong reliance on late timesteps.\n",
    "# - Instead of using ONLY the last hidden state, we pool across time (mean_last_k / attention).\n",
    "# - We add \"late-step masking\" (actually early-step masking) during training:\n",
    "#     early timesteps are masked more often -> model learns to rely more on late dynamics.\n",
    "#\n",
    "# Keeps Trial9 evaluation pack: PH / α–λ / CRA / convergence + paper figures\n",
    "#\n",
    "# Folder:\n",
    "#   ./Trial14/seed_<seed>/best_by_val_norm/...\n",
    "#   ./Trial14/seed_<seed>/last_epoch/...\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0) Reproducibility\n",
    "# ============================================================\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Config\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    data_dir: str = r\"C:\\Users\\11HOME_AHCI\\Desktop\\PoF\\Winter 2026\\Ref Data4\\100\"\n",
    "    out_dir: str = r\"./Trial14\"\n",
    "\n",
    "    # seeds to sweep\n",
    "    seeds: Tuple[int, ...] = (9819123, 111, 222, 333, 444)\n",
    "\n",
    "    # sliding window\n",
    "    seq_len: int = 100\n",
    "    stride: int = 5\n",
    "    pred_horizon: int = 0\n",
    "\n",
    "    # split by FILE\n",
    "    train_ratio: float = 0.7\n",
    "    val_ratio: float = 0.2\n",
    "    test_ratio: float = 0.1\n",
    "\n",
    "    # training\n",
    "    batch_size: int = 512\n",
    "    epochs: int = 300\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 0.0\n",
    "    patience: int = 30\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # model\n",
    "    hidden_size: int = 512\n",
    "    num_layers: int = 2\n",
    "    dropout: float = 0.2\n",
    "\n",
    "    # data loading\n",
    "    num_workers: int = 0\n",
    "\n",
    "    # output controls\n",
    "    save_figures: bool = True\n",
    "    max_files_to_plot: Optional[int] = None  # None=all\n",
    "\n",
    "    # ===========================\n",
    "    # Trial9-style Evaluation settings\n",
    "    # ===========================\n",
    "    alpha: float = 0.20\n",
    "    ph_consecutive_m: int = 5\n",
    "    rep_method: str = \"mean\"\n",
    "    lambdas: Tuple[float, ...] = (0.2, 0.4, 0.6, 0.8)\n",
    "    lambda_to_plot: float = 0.6\n",
    "    eps_rul: float = 1e-8\n",
    "\n",
    "    # ===========================\n",
    "    # Trial13: Feature engineering from min_vce ONLY\n",
    "    # ===========================\n",
    "    delta_steps: Tuple[int, ...] = (1, 5, 20, 50)\n",
    "    ema_spans: Tuple[int, ...] = (10, 50)\n",
    "    roll_std_window: int = 10\n",
    "    add_window_stats: bool = True\n",
    "\n",
    "    # ===========================\n",
    "    # Trial14-A: Temporal Pooling\n",
    "    # ===========================\n",
    "    # pooling: \"last\" | \"mean_last_k\" | \"attn\"\n",
    "    pooling: str = \"mean_last_k\"\n",
    "    pool_last_k: int = 10  # used when pooling=\"mean_last_k\"\n",
    "    attn_dim: int = 128    # used when pooling=\"attn\"\n",
    "\n",
    "    # ===========================\n",
    "    # Trial14-B: Late-step masking (early-step masking)\n",
    "    # ===========================\n",
    "    # Apply ONLY during training (train_ds).\n",
    "    time_mask_enable: bool = True\n",
    "    time_mask_pmax: float = 0.35  # max mask prob at earliest timestep\n",
    "    time_mask_gamma: float = 2.0  # larger => even more early-heavy masking\n",
    "    time_mask_value: float = 0.0  # masked feature value after scaling\n",
    "    # Optional: mask the whole timestep (all features) vs mask each feature independently\n",
    "    time_mask_mode: str = \"timestep\"  # \"timestep\" or \"feature\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Data utils\n",
    "# ============================================================\n",
    "def list_csv_files(data_dir: str) -> List[Path]:\n",
    "    p = Path(data_dir)\n",
    "    files = sorted([f for f in p.glob(\"*.csv\") if f.is_file()])\n",
    "    if len(files) == 0:\n",
    "        raise FileNotFoundError(f\"No CSV files found in: {data_dir}\")\n",
    "    return files\n",
    "\n",
    "\n",
    "def read_one_csv(csv_path: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(f\"{csv_path.name}: expected at least 2 columns, got {df.shape[1]}\")\n",
    "    vce = df.iloc[:, 0].astype(np.float32).to_numpy()\n",
    "    rul = df.iloc[:, 1].astype(np.float32).to_numpy()\n",
    "\n",
    "    if len(vce) != len(rul):\n",
    "        raise ValueError(f\"{csv_path.name}: length mismatch vce={len(vce)}, rul={len(rul)}\")\n",
    "    if len(vce) < 5:\n",
    "        raise ValueError(f\"{csv_path.name}: too short sequence length={len(vce)}\")\n",
    "    return vce, rul\n",
    "\n",
    "\n",
    "def split_files(\n",
    "    files: List[Path],\n",
    "    train_ratio: float,\n",
    "    val_ratio: float,\n",
    "    test_ratio: float,\n",
    "    seed: int\n",
    ") -> Dict[str, List[Path]]:\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-9\n",
    "\n",
    "    rng = random.Random(seed)\n",
    "    files_shuffled = files[:]\n",
    "    rng.shuffle(files_shuffled)\n",
    "\n",
    "    n = len(files_shuffled)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_val = int(n * val_ratio)\n",
    "\n",
    "    train_files = files_shuffled[:n_train]\n",
    "    val_files = files_shuffled[n_train:n_train + n_val]\n",
    "    test_files = files_shuffled[n_train + n_val:]\n",
    "\n",
    "    return {\"train\": train_files, \"val\": val_files, \"test\": test_files}\n",
    "\n",
    "\n",
    "def delta_k(v: np.ndarray, k: int) -> np.ndarray:\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    if k <= 0:\n",
    "        return out\n",
    "    out[k:] = v[k:] - v[:-k]\n",
    "    return out\n",
    "\n",
    "\n",
    "def ema(v: np.ndarray, span: int) -> np.ndarray:\n",
    "    if span <= 1:\n",
    "        return v.astype(np.float32).copy()\n",
    "    a = 2.0 / (float(span) + 1.0)\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    out[0] = v[0]\n",
    "    for i in range(1, len(v)):\n",
    "        out[i] = a * v[i] + (1.0 - a) * out[i - 1]\n",
    "    return out\n",
    "\n",
    "\n",
    "def rolling_std(v: np.ndarray, w: int) -> np.ndarray:\n",
    "    w = int(w)\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    for i in range(len(v)):\n",
    "        j0 = max(0, i - w + 1)\n",
    "        out[i] = float(np.std(v[j0:i + 1], ddof=0))\n",
    "    return out\n",
    "\n",
    "\n",
    "def feature_names(cfg: Config) -> List[str]:\n",
    "    names = [\"min_vce\"]\n",
    "    for k in cfg.delta_steps:\n",
    "        names.append(f\"delta_{k}\")\n",
    "    for s in cfg.ema_spans:\n",
    "        names.append(f\"ema_{s}\")\n",
    "    if cfg.roll_std_window and cfg.roll_std_window > 1:\n",
    "        names.append(f\"rollstd_{cfg.roll_std_window}\")\n",
    "    if cfg.add_window_stats:\n",
    "        names += [\"win_mean\", \"win_std\", \"win_slope\"]\n",
    "    return names\n",
    "\n",
    "\n",
    "def build_features_from_min_vce(vce: np.ndarray, cfg: Config) -> np.ndarray:\n",
    "    feats = [vce.astype(np.float32)]\n",
    "\n",
    "    for k in cfg.delta_steps:\n",
    "        feats.append(delta_k(vce, int(k)))\n",
    "\n",
    "    for s in cfg.ema_spans:\n",
    "        feats.append(ema(vce, int(s)))\n",
    "\n",
    "    if cfg.roll_std_window and cfg.roll_std_window > 1:\n",
    "        feats.append(rolling_std(vce, int(cfg.roll_std_window)))\n",
    "\n",
    "    X = np.stack(feats, axis=1).astype(np.float32)  # (T, F_base)\n",
    "    return X\n",
    "\n",
    "\n",
    "def _window_slope(seg: np.ndarray) -> float:\n",
    "    L = len(seg)\n",
    "    if L <= 1:\n",
    "        return 0.0\n",
    "    t = np.arange(L, dtype=np.float32)\n",
    "    denom = float(np.var(t) + 1e-12)\n",
    "    if denom <= 0:\n",
    "        return 0.0\n",
    "    # cov(t, seg)/var(t)\n",
    "    return float(np.cov(t, seg, ddof=0)[0, 1] / denom)\n",
    "\n",
    "\n",
    "def apply_late_step_masking(x: np.ndarray, cfg: Config, rng: np.random.RandomState) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    x: (L, F) float32 (already scaled)\n",
    "    Early-heavy masking:\n",
    "      p(t) = pmax * (1 - t/(L-1))^gamma\n",
    "      t=0 => pmax, t=L-1 => ~0\n",
    "    \"\"\"\n",
    "    if (not cfg.time_mask_enable) or cfg.time_mask_pmax <= 0:\n",
    "        return x\n",
    "\n",
    "    L, F = x.shape\n",
    "    if L <= 1:\n",
    "        return x\n",
    "\n",
    "    t = np.arange(L, dtype=np.float32)\n",
    "    frac = t / float(L - 1)\n",
    "    p = float(cfg.time_mask_pmax) * np.power((1.0 - frac), float(cfg.time_mask_gamma))  # (L,)\n",
    "\n",
    "    x2 = x.copy()\n",
    "\n",
    "    if cfg.time_mask_mode == \"timestep\":\n",
    "        m = rng.rand(L) < p  # (L,)\n",
    "        if np.any(m):\n",
    "            x2[m, :] = float(cfg.time_mask_value)\n",
    "    elif cfg.time_mask_mode == \"feature\":\n",
    "        # each feature independently\n",
    "        M = rng.rand(L, F) < p.reshape(-1, 1)\n",
    "        x2[M] = float(cfg.time_mask_value)\n",
    "    else:\n",
    "        # fallback\n",
    "        m = rng.rand(L) < p\n",
    "        if np.any(m):\n",
    "            x2[m, :] = float(cfg.time_mask_value)\n",
    "\n",
    "    return x2\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Dataset\n",
    "# ============================================================\n",
    "class WindowedRULDatasetNormMinVCE_Trial14(Dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x: (seq_len, F)\n",
    "      y_norm: (1,)\n",
    "      name, start_idx, y_cycles, rul0\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_list: List[Path],\n",
    "        cfg: Config,\n",
    "        scaler_x: StandardScaler = None,\n",
    "        fit_scaler: bool = False,\n",
    "        is_train: bool = False,\n",
    "    ):\n",
    "        self.file_list = file_list\n",
    "        self.cfg = cfg\n",
    "        self.seq_len = cfg.seq_len\n",
    "        self.stride = cfg.stride\n",
    "        self.pred_horizon = cfg.pred_horizon\n",
    "        self.scaler_x = scaler_x if scaler_x is not None else StandardScaler()\n",
    "        self.is_train = bool(is_train)\n",
    "\n",
    "        # deterministic rng for masking (per dataset instance)\n",
    "        self._rng = np.random.RandomState(1234 if self.is_train else 4321)\n",
    "\n",
    "        # store: (name, Xbase(T,Fbase), vce(T,), rul(T,), rul0)\n",
    "        self.series: List[Tuple[str, np.ndarray, np.ndarray, np.ndarray, float]] = []\n",
    "        for fp in self.file_list:\n",
    "            vce, rul = read_one_csv(fp)\n",
    "            rul0 = float(rul[0])\n",
    "            if rul0 <= 0:\n",
    "                raise ValueError(f\"{fp.name}: RUL0 must be > 0, got {rul0}\")\n",
    "\n",
    "            Xbase = build_features_from_min_vce(vce, cfg).astype(np.float32)\n",
    "            self.series.append((fp.name, Xbase, vce.astype(np.float32), rul.astype(np.float32), rul0))\n",
    "\n",
    "        # Fit scaler\n",
    "        if fit_scaler:\n",
    "            if not cfg.add_window_stats:\n",
    "                all_x = np.concatenate([Xbase for _, Xbase, _, _, _ in self.series], axis=0)\n",
    "                self.scaler_x.fit(all_x)\n",
    "            else:\n",
    "                rng = np.random.RandomState(0)\n",
    "                rows = []\n",
    "                max_windows_for_scaler = 5000\n",
    "                for (_name, Xbase, vce_raw, _rul, _rul0) in self.series:\n",
    "                    T = Xbase.shape[0]\n",
    "                    last_start = T - (self.seq_len + self.pred_horizon)\n",
    "                    if last_start < 0:\n",
    "                        continue\n",
    "                    starts = list(range(0, last_start + 1, self.stride))\n",
    "                    if len(starts) == 0:\n",
    "                        continue\n",
    "                    if len(starts) > 200:\n",
    "                        starts = rng.choice(starts, size=200, replace=False).tolist()\n",
    "                    for s in starts:\n",
    "                        xw = Xbase[s:s + self.seq_len, :]\n",
    "                        seg = vce_raw[s:s + self.seq_len]\n",
    "                        wmean = float(np.mean(seg))\n",
    "                        wstd = float(np.std(seg, ddof=0))\n",
    "                        slope = _window_slope(seg)\n",
    "                        stats = np.array([wmean, wstd, slope], dtype=np.float32).reshape(1, 3)\n",
    "                        stats_rep = np.repeat(stats, repeats=self.seq_len, axis=0)\n",
    "                        xfull = np.concatenate([xw, stats_rep], axis=1)\n",
    "                        rows.append(xfull)\n",
    "                        if len(rows) >= max_windows_for_scaler:\n",
    "                            break\n",
    "                    if len(rows) >= max_windows_for_scaler:\n",
    "                        break\n",
    "                if len(rows) == 0:\n",
    "                    raise ValueError(\"Scaler fitting failed: no windows sampled. Check seq_len/stride.\")\n",
    "                fit_mat = np.concatenate(rows, axis=0)\n",
    "                self.scaler_x.fit(fit_mat)\n",
    "\n",
    "        # window index\n",
    "        self.index: List[Tuple[int, int]] = []\n",
    "        for fi, (_name, Xbase, _vce, _rul, _rul0) in enumerate(self.series):\n",
    "            T = Xbase.shape[0]\n",
    "            last_start = T - (self.seq_len + self.pred_horizon)\n",
    "            if last_start < 0:\n",
    "                continue\n",
    "            for s in range(0, last_start + 1, self.stride):\n",
    "                self.index.append((fi, s))\n",
    "\n",
    "        if len(self.index) == 0:\n",
    "            raise ValueError(\"No windows were created. Check seq_len/pred_horizon vs file lengths.\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fi, s = self.index[idx]\n",
    "        name, Xbase, vce_raw, rul, rul0 = self.series[fi]\n",
    "\n",
    "        x = Xbase[s:s + self.seq_len, :]  # (L,Fbase)\n",
    "\n",
    "        if self.cfg.add_window_stats:\n",
    "            seg = vce_raw[s:s + self.seq_len]\n",
    "            wmean = float(np.mean(seg))\n",
    "            wstd = float(np.std(seg, ddof=0))\n",
    "            slope = _window_slope(seg)\n",
    "            stats = np.array([wmean, wstd, slope], dtype=np.float32).reshape(1, 3)\n",
    "            stats_rep = np.repeat(stats, repeats=self.seq_len, axis=0)\n",
    "            x = np.concatenate([x, stats_rep], axis=1).astype(np.float32)  # (L,F)\n",
    "\n",
    "        y_idx = s + self.seq_len - 1 + self.pred_horizon\n",
    "        y_cycles = float(rul[y_idx])\n",
    "        y_norm = np.array([y_cycles / rul0], dtype=np.float32)\n",
    "\n",
    "        # scale first\n",
    "        x = self.scaler_x.transform(x).astype(np.float32)\n",
    "\n",
    "        # Trial14-B: apply masking ONLY for training dataset\n",
    "        if self.is_train and self.cfg.time_mask_enable:\n",
    "            x = apply_late_step_masking(x, self.cfg, self._rng).astype(np.float32)\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x),\n",
    "            torch.from_numpy(y_norm),\n",
    "            name,\n",
    "            torch.tensor(s, dtype=torch.long),\n",
    "            torch.tensor(y_cycles, dtype=torch.float32),\n",
    "            torch.tensor(rul0, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Model (Temporal pooling)\n",
    "# ============================================================\n",
    "class TemporalPool(nn.Module):\n",
    "    def __init__(self, mode: str, hidden_size: int, last_k: int = 10, attn_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.mode = str(mode)\n",
    "        self.last_k = int(last_k)\n",
    "        self.attn_dim = int(attn_dim)\n",
    "\n",
    "        if self.mode == \"attn\":\n",
    "            self.proj = nn.Sequential(\n",
    "                nn.Linear(hidden_size, self.attn_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(self.attn_dim, 1),\n",
    "            )\n",
    "\n",
    "    def forward(self, h: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        h: (B, L, H)\n",
    "        returns pooled: (B, H)\n",
    "        \"\"\"\n",
    "        if self.mode == \"last\":\n",
    "            return h[:, -1, :]\n",
    "\n",
    "        if self.mode == \"mean_last_k\":\n",
    "            L = h.size(1)\n",
    "            k = min(self.last_k, L)\n",
    "            return torch.mean(h[:, -k:, :], dim=1)\n",
    "\n",
    "        if self.mode == \"attn\":\n",
    "            # scores: (B, L, 1) -> weights: (B, L, 1)\n",
    "            scores = self.proj(h)\n",
    "            w = torch.softmax(scores, dim=1)\n",
    "            return torch.sum(w * h, dim=1)\n",
    "\n",
    "        # fallback\n",
    "        return h[:, -1, :]\n",
    "\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout: float,\n",
    "                 pooling: str, pool_last_k: int, attn_dim: int):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.pool = TemporalPool(pooling, hidden_size, last_k=pool_last_k, attn_dim=attn_dim)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, _ = self.lstm(x)           # (B, L, H)\n",
    "        z = self.pool(h)              # (B, H)\n",
    "        return self.head(z)           # (B, 1) norm-scale\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Basic Eval + Save window-level predictions\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def evaluate_basic(model, loader, device) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "\n",
    "    mae_norm_list, mse_norm_list = [], []\n",
    "    mae_cyc_list, mse_cyc_list = [], []\n",
    "\n",
    "    for x, y_norm, _name, _s, y_cycles, rul0 in loader:\n",
    "        x = x.to(device)\n",
    "        y_norm = y_norm.to(device)\n",
    "        y_cycles = y_cycles.to(device).view(-1, 1)\n",
    "        rul0 = rul0.to(device).view(-1, 1)\n",
    "\n",
    "        pred_norm = model(x)\n",
    "\n",
    "        err_norm = pred_norm - y_norm\n",
    "        mae_norm_list.append(torch.mean(torch.abs(err_norm)).item())\n",
    "        mse_norm_list.append(torch.mean(err_norm ** 2).item())\n",
    "\n",
    "        pred_cycles = pred_norm * rul0\n",
    "        err_cyc = pred_cycles - y_cycles\n",
    "        mae_cyc_list.append(torch.mean(torch.abs(err_cyc)).item())\n",
    "        mse_cyc_list.append(torch.mean(err_cyc ** 2).item())\n",
    "\n",
    "    return {\n",
    "        \"mae_norm\": float(np.mean(mae_norm_list)) if mae_norm_list else float(\"nan\"),\n",
    "        \"rmse_norm\": float(np.sqrt(np.mean(mse_norm_list))) if mse_norm_list else float(\"nan\"),\n",
    "        \"mae_cycles\": float(np.mean(mae_cyc_list)) if mae_cyc_list else float(\"nan\"),\n",
    "        \"rmse_cycles\": float(np.sqrt(np.mean(mse_cyc_list))) if mse_cyc_list else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_predictions_windows_csv(model, loader, device, out_csv: str, seq_len: int) -> None:\n",
    "    model.eval()\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for x, y_norm, name, s, y_cycles, rul0 in loader:\n",
    "        x = x.to(device)\n",
    "        y_norm = y_norm.to(device)\n",
    "        y_cycles = y_cycles.to(device).view(-1, 1)\n",
    "        rul0 = rul0.to(device).view(-1, 1)\n",
    "\n",
    "        pred_norm = model(x)\n",
    "        pred_cycles = pred_norm * rul0\n",
    "\n",
    "        pred_norm_np = pred_norm.cpu().numpy().reshape(-1)\n",
    "        y_norm_np = y_norm.cpu().numpy().reshape(-1)\n",
    "        pred_cyc_np = pred_cycles.cpu().numpy().reshape(-1)\n",
    "        y_cyc_np = y_cycles.cpu().numpy().reshape(-1)\n",
    "\n",
    "        rul0_np = rul0.cpu().numpy().reshape(-1)\n",
    "        s_np = s.cpu().numpy().reshape(-1)\n",
    "        name_list = list(name)\n",
    "\n",
    "        for i in range(len(pred_norm_np)):\n",
    "            rows.append({\n",
    "                \"file\": name_list[i],\n",
    "                \"start_idx\": int(s_np[i]),\n",
    "                \"cycle\": int(s_np[i] + (seq_len - 1)),\n",
    "                \"rul0\": float(rul0_np[i]),\n",
    "                \"RUL_true\": float(y_cyc_np[i]),\n",
    "                \"RUL_pred\": float(pred_cyc_np[i]),\n",
    "                \"RUL_true_norm\": float(y_norm_np[i]),\n",
    "                \"RUL_pred_norm\": float(pred_norm_np[i]),\n",
    "            })\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) Window -> Cycle sequence (mean representative)\n",
    "# ============================================================\n",
    "def windows_to_cycle_sequence_mean(windows_csv: str) -> pd.DataFrame:\n",
    "    dfw = pd.read_csv(windows_csv)\n",
    "    if dfw.empty:\n",
    "        raise ValueError(f\"Empty windows csv: {windows_csv}\")\n",
    "\n",
    "    g = dfw.groupby([\"file\", \"cycle\"], as_index=False).agg(\n",
    "        rul0=(\"rul0\", \"first\"),\n",
    "        RUL_true=(\"RUL_true\", \"mean\"),\n",
    "        RUL_pred=(\"RUL_pred\", \"mean\"),\n",
    "        n_windows=(\"RUL_pred\", \"count\"),\n",
    "    )\n",
    "    return g\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) Prognostics metrics (same as Trial9)\n",
    "# ============================================================\n",
    "def compute_metrics_for_one_file(\n",
    "    df_seq_one_file: pd.DataFrame,\n",
    "    seq_len: int,\n",
    "    alpha: float,\n",
    "    ph_consecutive_m: int,\n",
    "    lambdas: Tuple[float, ...],\n",
    "    eps_rul: float,\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "\n",
    "    df = df_seq_one_file.sort_values(\"cycle\").reset_index(drop=True).copy()\n",
    "\n",
    "    t_s = seq_len - 1\n",
    "    last_cycle = int(df[\"cycle\"].max())\n",
    "    EOL_true = last_cycle + 1\n",
    "    t_e = EOL_true - 1\n",
    "\n",
    "    df_eval = df[(df[\"cycle\"] >= t_s) & (df[\"cycle\"] <= t_e)].copy()\n",
    "    df_eval.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if df_eval.empty:\n",
    "        summary = {\n",
    "            \"t_s\": t_s, \"t_e\": t_e, \"EOL_true\": EOL_true,\n",
    "            \"PH\": np.nan, \"t_PH_start\": np.nan,\n",
    "            \"CRA\": np.nan, \"Convergence_cycles\": np.nan,\n",
    "        }\n",
    "        for lam in lambdas:\n",
    "            summary[f\"t_lambda_{lam:.2f}\"] = np.nan\n",
    "            summary[f\"alpha_lambda_ok_{lam:.2f}\"] = np.nan\n",
    "        return df_eval, summary\n",
    "\n",
    "    denom = np.maximum(np.abs(df_eval[\"RUL_true\"].values), eps_rul)\n",
    "    rel_err = np.abs(df_eval[\"RUL_true\"].values - df_eval[\"RUL_pred\"].values) / denom\n",
    "    RA = 1.0 - rel_err\n",
    "\n",
    "    df_eval[\"rel_err\"] = rel_err\n",
    "    df_eval[\"RA\"] = RA\n",
    "    df_eval[\"in_alpha\"] = df_eval[\"rel_err\"] <= alpha\n",
    "\n",
    "    CRA = float(np.mean(df_eval[\"RA\"].values))\n",
    "\n",
    "    flags = df_eval[\"in_alpha\"].values.astype(np.int32)\n",
    "    t_PH_start = np.nan\n",
    "    if len(flags) >= ph_consecutive_m:\n",
    "        run = 0\n",
    "        for i, ok in enumerate(flags):\n",
    "            if ok:\n",
    "                run += 1\n",
    "                if run >= ph_consecutive_m:\n",
    "                    start_i = i - ph_consecutive_m + 1\n",
    "                    t_PH_start = int(df_eval.loc[start_i, \"cycle\"])\n",
    "                    break\n",
    "            else:\n",
    "                run = 0\n",
    "\n",
    "    if np.isfinite(t_PH_start):\n",
    "        PH = float(EOL_true - t_PH_start)\n",
    "        Convergence_cycles = float(t_PH_start - t_s)\n",
    "    else:\n",
    "        PH = np.nan\n",
    "        Convergence_cycles = np.nan\n",
    "\n",
    "    rul0 = float(df_eval[\"rul0\"].iloc[0])\n",
    "    lam_results = {}\n",
    "    for lam in lambdas:\n",
    "        target_rul = (1.0 - float(lam)) * rul0\n",
    "        idx = int(np.argmin(np.abs(df_eval[\"RUL_true\"].values - target_rul)))\n",
    "        t_lam = int(df_eval.loc[idx, \"cycle\"])\n",
    "        ok = bool(df_eval.loc[idx, \"rel_err\"] <= alpha)\n",
    "\n",
    "        lam_results[f\"t_lambda_{lam:.2f}\"] = t_lam\n",
    "        lam_results[f\"alpha_lambda_ok_{lam:.2f}\"] = int(ok)\n",
    "\n",
    "    summary = {\n",
    "        \"t_s\": int(t_s),\n",
    "        \"t_e\": int(t_e),\n",
    "        \"EOL_true\": int(EOL_true),\n",
    "        \"alpha\": float(alpha),\n",
    "        \"ph_consecutive_m\": int(ph_consecutive_m),\n",
    "        \"CRA\": CRA,\n",
    "        \"t_PH_start\": t_PH_start if np.isfinite(t_PH_start) else np.nan,\n",
    "        \"PH\": PH,\n",
    "        \"Convergence_cycles\": Convergence_cycles,\n",
    "        **lam_results\n",
    "    }\n",
    "    return df_eval, summary\n",
    "\n",
    "\n",
    "def compute_metrics_from_windows_csv(\n",
    "    windows_csv: str,\n",
    "    seq_len: int,\n",
    "    alpha: float,\n",
    "    ph_consecutive_m: int,\n",
    "    lambdas: Tuple[float, ...],\n",
    "    eps_rul: float,\n",
    "    out_dir: str,\n",
    "    split_name: str,\n",
    ") -> Tuple[str, str]:\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    df_seq = windows_to_cycle_sequence_mean(windows_csv)\n",
    "    seq_path = os.path.join(out_dir, f\"{split_name}_cycle_sequence_mean.csv\")\n",
    "    df_seq.to_csv(seq_path, index=False)\n",
    "\n",
    "    rows = []\n",
    "    for f in df_seq[\"file\"].unique():\n",
    "        sub = df_seq[df_seq[\"file\"] == f].copy()\n",
    "        _df_eval, summary = compute_metrics_for_one_file(\n",
    "            df_seq_one_file=sub,\n",
    "            seq_len=seq_len,\n",
    "            alpha=alpha,\n",
    "            ph_consecutive_m=ph_consecutive_m,\n",
    "            lambdas=lambdas,\n",
    "            eps_rul=eps_rul,\n",
    "        )\n",
    "        summary[\"file\"] = f\n",
    "        rows.append(summary)\n",
    "\n",
    "    dfm = pd.DataFrame(rows)\n",
    "    metrics_path = os.path.join(out_dir, f\"{split_name}_prognostics_metrics_per_file.csv\")\n",
    "    dfm.to_csv(metrics_path, index=False)\n",
    "\n",
    "    return seq_path, metrics_path\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) Plotters (same as Trial13)\n",
    "# ============================================================\n",
    "def _safe_name(s: str) -> str:\n",
    "    return s.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\")\n",
    "\n",
    "\n",
    "def plot_alpha_ph(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    title: str,\n",
    "    alpha: float,\n",
    "    PH_start: Optional[float],\n",
    "    out_path: str,\n",
    "    dpi: int = 200,\n",
    ") -> None:\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    plt.figure()\n",
    "\n",
    "    x = df_eval[\"cycle\"].values\n",
    "    y_true = df_eval[\"RUL_true\"].values\n",
    "    y_pred = df_eval[\"RUL_pred\"].values\n",
    "\n",
    "    upper = y_true * (1.0 + alpha)\n",
    "    lower = y_true * (1.0 - alpha)\n",
    "\n",
    "    plt.plot(x, y_true, color=\"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, color=\"r\", label=\"Prediction (cycles)\")\n",
    "    plt.plot(x, upper, color=\"b\", linestyle=\"--\", label=f\"+{alpha:.2f} alpha accuracy zone\")\n",
    "    plt.plot(x, lower, color=\"b\", linestyle=\"--\", label=f\"-{alpha:.2f} alpha accuracy zone\")\n",
    "\n",
    "    if PH_start is not None and np.isfinite(PH_start):\n",
    "        plt.axvline(int(PH_start), color=\"g\", linestyle=\"-.\", label=\"PH start\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title}\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_alpha_lambda(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    title: str,\n",
    "    alpha: float,\n",
    "    lambda_to_plot: float,\n",
    "    t_lambda: Optional[int],\n",
    "    out_path: str,\n",
    "    dpi: int = 200,\n",
    ") -> None:\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    x = df_eval[\"cycle\"].values\n",
    "    y_true = df_eval[\"RUL_true\"].values\n",
    "    y_pred = df_eval[\"RUL_pred\"].values\n",
    "\n",
    "    upper = y_true * (1.0 + alpha)\n",
    "    lower = y_true * (1.0 - alpha)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y_true, color=\"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, color=\"r\", label=\"Prediction (cycles)\")\n",
    "\n",
    "    if t_lambda is not None and np.isfinite(t_lambda):\n",
    "        t_lambda = int(t_lambda)\n",
    "        plt.axvline(t_lambda, linestyle=\":\", color=\"g\", label=f\"t_λ (λ={lambda_to_plot:.2f})\")\n",
    "        mask = x >= t_lambda\n",
    "        if np.any(mask):\n",
    "            plt.plot(x[mask], upper[mask], color=\"b\", linestyle=\"--\", label=f\"+{alpha:.2f} alpha–lambda zone\")\n",
    "            plt.plot(x[mask], lower[mask], color=\"b\", linestyle=\"--\", label=f\"-{alpha:.2f} alpha–lambda zone\")\n",
    "        else:\n",
    "            plt.plot(x, upper, color=\"b\", linestyle=\"--\", label=f\"+{alpha:.2f} α zone\")\n",
    "            plt.plot(x, lower, color=\"b\", linestyle=\"--\", label=f\"-{alpha:.2f} α zone\")\n",
    "    else:\n",
    "        plt.plot(x, upper, color=\"b\", linestyle=\"--\", label=f\"+{alpha:.2f} α zone\")\n",
    "        plt.plot(x, lower, color=\"b\", linestyle=\"--\", label=f\"-{alpha:.2f} α zone\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title}\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def make_paper_figures_for_split(\n",
    "    cycle_seq_csv: str,\n",
    "    metrics_per_file_csv: str,\n",
    "    out_fig_dir: str,\n",
    "    title_prefix: str,\n",
    "    alpha: float,\n",
    "    lambda_to_plot: float,\n",
    "    max_files: Optional[int] = None,\n",
    "    dpi: int = 200,\n",
    ") -> None:\n",
    "    df_seq = pd.read_csv(cycle_seq_csv)\n",
    "    dfm = pd.read_csv(metrics_per_file_csv)\n",
    "\n",
    "    files = df_seq[\"file\"].unique().tolist()\n",
    "    if max_files is not None:\n",
    "        files = files[:max_files]\n",
    "\n",
    "    os.makedirs(out_fig_dir, exist_ok=True)\n",
    "\n",
    "    lam_key = f\"t_lambda_{lambda_to_plot:.2f}\"\n",
    "\n",
    "    for f in files:\n",
    "        sub = df_seq[df_seq[\"file\"] == f].sort_values(\"cycle\").copy()\n",
    "        mrow = dfm[dfm[\"file\"] == f]\n",
    "        if mrow.empty:\n",
    "            continue\n",
    "        mrow = mrow.iloc[0].to_dict()\n",
    "\n",
    "        t_s = int(mrow[\"t_s\"])\n",
    "        t_e = int(mrow[\"t_e\"])\n",
    "        PH_start = mrow.get(\"t_PH_start\", np.nan)\n",
    "\n",
    "        df_eval = sub[(sub[\"cycle\"] >= t_s) & (sub[\"cycle\"] <= t_e)].copy()\n",
    "        if df_eval.empty:\n",
    "            continue\n",
    "\n",
    "        t_lambda = None\n",
    "        if lam_key in mrow and np.isfinite(mrow[lam_key]):\n",
    "            t_lambda = int(mrow[lam_key])\n",
    "\n",
    "        safe = _safe_name(f)\n",
    "\n",
    "        out1 = os.path.join(out_fig_dir, f\"FIG1_alpha_PH__{safe}.png\")\n",
    "        plot_alpha_ph(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            title=f\"{title_prefix} | α+PH\",\n",
    "            alpha=alpha,\n",
    "            PH_start=PH_start if np.isfinite(PH_start) else None,\n",
    "            out_path=out1,\n",
    "            dpi=dpi,\n",
    "        )\n",
    "\n",
    "        out2 = os.path.join(out_fig_dir, f\"FIG2_alpha_lambda__lam{lambda_to_plot:.2f}__{safe}.png\")\n",
    "        plot_alpha_lambda(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            title=f\"{title_prefix} | α–λ (λ={lambda_to_plot:.2f})\",\n",
    "            alpha=alpha,\n",
    "            lambda_to_plot=lambda_to_plot,\n",
    "            t_lambda=t_lambda,\n",
    "            out_path=out2,\n",
    "            dpi=dpi,\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) One seed run\n",
    "# ============================================================\n",
    "def run_one_seed(cfg: Config, seed: int) -> Dict[str, Any]:\n",
    "    set_seed(seed)\n",
    "\n",
    "    seed_dir = os.path.join(cfg.out_dir, f\"seed_{seed}\")\n",
    "    os.makedirs(seed_dir, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"[SEED {seed}] device={device}\")\n",
    "    print(f\"[SEED {seed}] out={seed_dir}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    # split\n",
    "    files = list_csv_files(cfg.data_dir)\n",
    "    splits = split_files(files, cfg.train_ratio, cfg.val_ratio, cfg.test_ratio, seed)\n",
    "\n",
    "    # save split lists\n",
    "    for k in [\"train\", \"val\", \"test\"]:\n",
    "        pd.Series([p.name for p in splits[k]]).to_csv(\n",
    "            os.path.join(seed_dir, f\"{k}_files.csv\"), index=False, header=False\n",
    "        )\n",
    "\n",
    "    # datasets (fit scaler on train only)\n",
    "    scaler_x = StandardScaler()\n",
    "    train_ds = WindowedRULDatasetNormMinVCE_Trial14(\n",
    "        splits[\"train\"], cfg, scaler_x=scaler_x, fit_scaler=True, is_train=True\n",
    "    )\n",
    "    val_ds = WindowedRULDatasetNormMinVCE_Trial14(\n",
    "        splits[\"val\"], cfg, scaler_x=train_ds.scaler_x, fit_scaler=False, is_train=False\n",
    "    )\n",
    "    test_ds = WindowedRULDatasetNormMinVCE_Trial14(\n",
    "        splits[\"test\"], cfg, scaler_x=train_ds.scaler_x, fit_scaler=False, is_train=False\n",
    "    )\n",
    "\n",
    "    feat_list = feature_names(cfg)\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feat_list,\n",
    "        \"mean\": train_ds.scaler_x.mean_.ravel(),\n",
    "        \"std\": np.sqrt(train_ds.scaler_x.var_).ravel(),\n",
    "    }).to_csv(os.path.join(seed_dir, \"scaler_x_mean_std.csv\"), index=False)\n",
    "\n",
    "    # loaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "\n",
    "    train_eval = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "    val_eval = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "    test_eval = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "\n",
    "    # model\n",
    "    input_size = len(feat_list)\n",
    "    model = LSTMRegressor(\n",
    "        input_size=input_size,\n",
    "        hidden_size=cfg.hidden_size,\n",
    "        num_layers=cfg.num_layers,\n",
    "        dropout=cfg.dropout,\n",
    "        pooling=cfg.pooling,\n",
    "        pool_last_k=cfg.pool_last_k,\n",
    "        attn_dim=cfg.attn_dim,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    best_by_val_norm = float(\"inf\")\n",
    "    best_path = os.path.join(seed_dir, \"best_by_val_norm.pt\")\n",
    "    last_path = os.path.join(seed_dir, \"last_epoch.pt\")\n",
    "\n",
    "    history: List[Dict[str, Any]] = []\n",
    "    bad_epochs = 0\n",
    "\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        model.train()\n",
    "        losses = []\n",
    "\n",
    "        for x, y_norm, *_ in train_loader:\n",
    "            x = x.to(device)\n",
    "            y_norm = y_norm.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred_norm = model(x)\n",
    "            loss = criterion(pred_norm, y_norm)\n",
    "            loss.backward()\n",
    "\n",
    "            if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        train_mse_norm = float(np.mean(losses)) if losses else float(\"nan\")\n",
    "        val_metrics = evaluate_basic(model, val_loader, device)\n",
    "\n",
    "        history.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_mse_norm\": train_mse_norm,\n",
    "            \"val_rmse_norm\": val_metrics[\"rmse_norm\"],\n",
    "            \"val_mae_norm\": val_metrics[\"mae_norm\"],\n",
    "            \"val_rmse_cycles\": val_metrics[\"rmse_cycles\"],\n",
    "            \"val_mae_cycles\": val_metrics[\"mae_cycles\"],\n",
    "        })\n",
    "\n",
    "        if val_metrics[\"rmse_norm\"] < best_by_val_norm:\n",
    "            best_by_val_norm = val_metrics[\"rmse_norm\"]\n",
    "            bad_epochs = 0\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(\n",
    "                f\"[SEED {seed}] [{epoch:03d}/{cfg.epochs}] \"\n",
    "                f\"train_mse_norm={train_mse_norm:.6f} | \"\n",
    "                f\"val_rmse_norm={val_metrics['rmse_norm']:.6f} | \"\n",
    "                f\"val_mae_cycles={val_metrics['mae_cycles']:.3f} | \"\n",
    "                f\"best_val_rmse_norm={best_by_val_norm:.6f}\"\n",
    "            )\n",
    "\n",
    "        if bad_epochs >= cfg.patience:\n",
    "            print(f\"[SEED {seed}] Early stopping at epoch {epoch}.\")\n",
    "            break\n",
    "\n",
    "    pd.DataFrame(history).to_csv(os.path.join(seed_dir, \"history.csv\"), index=False)\n",
    "    torch.save(model.state_dict(), last_path)\n",
    "\n",
    "    def export_ckpt(tag: str, ckpt_path: str) -> Dict[str, Any]:\n",
    "        sub_dir = os.path.join(seed_dir, tag)\n",
    "        os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "        model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "        model.eval()\n",
    "\n",
    "        tr = evaluate_basic(model, train_eval, device)\n",
    "        va = evaluate_basic(model, val_eval, device)\n",
    "        te = evaluate_basic(model, test_eval, device)\n",
    "\n",
    "        for split_name, loader in [(\"train\", train_eval), (\"val\", val_eval), (\"test\", test_eval)]:\n",
    "            win_csv = os.path.join(sub_dir, f\"{split_name}_predictions_windows.csv\")\n",
    "            save_predictions_windows_csv(model, loader, device, win_csv, seq_len=cfg.seq_len)\n",
    "\n",
    "            seq_csv, metrics_csv = compute_metrics_from_windows_csv(\n",
    "                windows_csv=win_csv,\n",
    "                seq_len=cfg.seq_len,\n",
    "                alpha=cfg.alpha,\n",
    "                ph_consecutive_m=cfg.ph_consecutive_m,\n",
    "                lambdas=cfg.lambdas,\n",
    "                eps_rul=cfg.eps_rul,\n",
    "                out_dir=sub_dir,\n",
    "                split_name=split_name,\n",
    "            )\n",
    "\n",
    "            if cfg.save_figures:\n",
    "                fig_dir = os.path.join(sub_dir, \"paper_figures\", split_name)\n",
    "                make_paper_figures_for_split(\n",
    "                    cycle_seq_csv=seq_csv,\n",
    "                    metrics_per_file_csv=metrics_csv,\n",
    "                    out_fig_dir=fig_dir,\n",
    "                    title_prefix=f\"SEED {seed} | {tag.upper()} | {split_name}\",\n",
    "                    alpha=cfg.alpha,\n",
    "                    lambda_to_plot=cfg.lambda_to_plot,\n",
    "                    max_files=cfg.max_files_to_plot,\n",
    "                )\n",
    "\n",
    "        ms = {\n",
    "            \"seed\": seed,\n",
    "            \"checkpoint\": tag,\n",
    "            \"train_rmse_cycles\": tr[\"rmse_cycles\"],\n",
    "            \"train_mae_cycles\": tr[\"mae_cycles\"],\n",
    "            \"train_rmse_norm\": tr[\"rmse_norm\"],\n",
    "            \"train_mae_norm\": tr[\"mae_norm\"],\n",
    "            \"val_rmse_cycles\": va[\"rmse_cycles\"],\n",
    "            \"val_mae_cycles\": va[\"mae_cycles\"],\n",
    "            \"val_rmse_norm\": va[\"rmse_norm\"],\n",
    "            \"val_mae_norm\": va[\"mae_norm\"],\n",
    "            \"test_rmse_cycles\": te[\"rmse_cycles\"],\n",
    "            \"test_mae_cycles\": te[\"mae_cycles\"],\n",
    "            \"test_rmse_norm\": te[\"rmse_norm\"],\n",
    "            \"test_mae_norm\": te[\"mae_norm\"],\n",
    "            \"stopped_epoch\": history[-1][\"epoch\"] if len(history) else None,\n",
    "            \"best_val_rmse_norm\": best_by_val_norm,\n",
    "            \"alpha\": cfg.alpha,\n",
    "            \"ph_consecutive_m\": cfg.ph_consecutive_m,\n",
    "            \"rep_method\": cfg.rep_method,\n",
    "            \"lambdas\": str(cfg.lambdas),\n",
    "            \"lambda_to_plot\": cfg.lambda_to_plot,\n",
    "            \"feature_dim\": input_size,\n",
    "            \"features\": \",\".join(feature_names(cfg)),\n",
    "            # Trial14 extras\n",
    "            \"pooling\": cfg.pooling,\n",
    "            \"pool_last_k\": cfg.pool_last_k,\n",
    "            \"time_mask_enable\": int(cfg.time_mask_enable),\n",
    "            \"time_mask_pmax\": cfg.time_mask_pmax,\n",
    "            \"time_mask_gamma\": cfg.time_mask_gamma,\n",
    "            \"time_mask_mode\": cfg.time_mask_mode,\n",
    "        }\n",
    "        pd.DataFrame([ms]).to_csv(os.path.join(sub_dir, \"metrics_summary.csv\"), index=False)\n",
    "\n",
    "        print(\n",
    "            f\"[SEED {seed}] {tag}: TEST mae_cycles={te['mae_cycles']:.3f} | \"\n",
    "            f\"rmse_cycles={te['rmse_cycles']:.3f} | rmse_norm={te['rmse_norm']:.6f}\"\n",
    "        )\n",
    "        return ms\n",
    "\n",
    "    ms_best = export_ckpt(\"best_by_val_norm\", best_path)\n",
    "    ms_last = export_ckpt(\"last_epoch\", last_path)\n",
    "\n",
    "    return {\"seed\": seed, \"seed_dir\": seed_dir, \"best\": ms_best, \"last\": ms_last}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10) Seed sweep + global comparison\n",
    "# ============================================================\n",
    "def summarize_across_seeds(cfg: Config, results: List[Dict[str, Any]]) -> None:\n",
    "    rows = []\n",
    "    for r in results:\n",
    "        rows.append(r[\"best\"])\n",
    "        rows.append(r[\"last\"])\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(os.path.join(cfg.out_dir, \"summary_across_seeds.csv\"), index=False)\n",
    "\n",
    "    def _isfinite(x: Any) -> bool:\n",
    "        try:\n",
    "            return bool(np.isfinite(float(x)))\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def win_rate(metric: str) -> Dict[str, Any]:\n",
    "        wins_last = 0\n",
    "        wins_best = 0\n",
    "        ties = 0\n",
    "        diffs = []\n",
    "\n",
    "        for r in results:\n",
    "            b = r[\"best\"][metric]\n",
    "            l = r[\"last\"][metric]\n",
    "            if _isfinite(b) and _isfinite(l):\n",
    "                diffs.append(float(l) - float(b))\n",
    "                if float(l) < float(b):\n",
    "                    wins_last += 1\n",
    "                elif float(b) < float(l):\n",
    "                    wins_best += 1\n",
    "                else:\n",
    "                    ties += 1\n",
    "\n",
    "        return {\n",
    "            \"metric\": metric,\n",
    "            \"wins_last\": wins_last,\n",
    "            \"wins_best\": wins_best,\n",
    "            \"ties\": ties,\n",
    "            \"mean(last-best)\": float(np.mean(diffs)) if diffs else float(\"nan\"),\n",
    "            \"std(last-best)\": float(np.std(diffs, ddof=0)) if diffs else float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    metrics = [\"test_mae_cycles\", \"test_rmse_cycles\", \"test_mae_norm\", \"test_rmse_norm\"]\n",
    "    wr = [win_rate(m) for m in metrics]\n",
    "    pd.DataFrame(wr).to_csv(os.path.join(cfg.out_dir, \"win_rate_summary.csv\"), index=False)\n",
    "\n",
    "    lines = []\n",
    "    lines.append(\"=== WIN-RATE SUMMARY (TEST; lower is better) ===\")\n",
    "    for row in wr:\n",
    "        lines.append(\n",
    "            f\"- {row['metric']}: last wins={row['wins_last']}, best wins={row['wins_best']}, ties={row['ties']} | \"\n",
    "            f\"mean(last-best)={row['mean(last-best)']:.6f}, std(last-best)={row['std(last-best)']:.6f}\"\n",
    "        )\n",
    "\n",
    "    agg = df.groupby(\"checkpoint\")[metrics].agg([\"mean\", \"std\"])\n",
    "    lines.append(\"\\n=== MEAN ± STD across seeds (TEST) ===\")\n",
    "    lines.append(str(agg))\n",
    "\n",
    "    with open(os.path.join(cfg.out_dir, \"win_rate_summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nSaved:\")\n",
    "    print(\" -\", os.path.join(cfg.out_dir, \"summary_across_seeds.csv\"))\n",
    "    print(\" -\", os.path.join(cfg.out_dir, \"win_rate_summary.csv\"))\n",
    "    print(\" -\", os.path.join(cfg.out_dir, \"win_rate_summary.txt\"))\n",
    "\n",
    "\n",
    "def run_trial14_seed_sweep(cfg: Config) -> None:\n",
    "    os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "\n",
    "    results = []\n",
    "    for seed in cfg.seeds:\n",
    "        res = run_one_seed(cfg, seed)\n",
    "        results.append(res)\n",
    "\n",
    "    summarize_across_seeds(cfg, results)\n",
    "\n",
    "    print(\"\\nDONE. Check Trial14 folder:\")\n",
    "    print(\" - per seed results: Trial14/seed_<seed>/...\")\n",
    "    print(\" - figures (paper-style): seed_<seed>/<ckpt>/paper_figures/<split>/\")\n",
    "    print(\" - cycle sequence mean CSV: <ckpt>/<split>_cycle_sequence_mean.csv\")\n",
    "    print(\" - PH/α–λ metrics CSV: <ckpt>/<split>_prognostics_metrics_per_file.csv\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 11) Run\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = Config(\n",
    "        data_dir=r\"C:\\Users\\11HOME_AHCI\\Desktop\\PoF\\Winter 2026\\Ref Data4\\100\",\n",
    "        out_dir=r\"./Trial14\",\n",
    "\n",
    "        seeds=(9819123, 111, 222, 333, 444),\n",
    "\n",
    "        seq_len=100,\n",
    "        stride=5,\n",
    "        pred_horizon=0,\n",
    "\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.2,\n",
    "        test_ratio=0.1,\n",
    "\n",
    "        batch_size=512,\n",
    "        epochs=300,\n",
    "        lr=1e-3,\n",
    "        weight_decay=0.0,\n",
    "        patience=30,\n",
    "        grad_clip=1.0,\n",
    "\n",
    "        hidden_size=512,\n",
    "        num_layers=2,\n",
    "        dropout=0.2,\n",
    "\n",
    "        save_figures=True,\n",
    "        max_files_to_plot=None,\n",
    "        num_workers=0,\n",
    "\n",
    "        alpha=0.20,\n",
    "        ph_consecutive_m=5,\n",
    "        rep_method=\"mean\",\n",
    "        lambdas=(0.2, 0.4, 0.6, 0.8),\n",
    "        lambda_to_plot=0.6,\n",
    "\n",
    "        # === Trial13 features (min_vce only) ===\n",
    "        delta_steps=(1, 5, 20, 50),\n",
    "        ema_spans=(10, 50),\n",
    "        roll_std_window=10,\n",
    "        add_window_stats=True,\n",
    "\n",
    "        # === Trial14 changes ===\n",
    "        pooling=\"mean_last_k\",   # try: \"mean_last_k\" or \"attn\" or \"last\"\n",
    "        pool_last_k=10,\n",
    "        attn_dim=128,\n",
    "\n",
    "        time_mask_enable=True,\n",
    "        time_mask_pmax=0.35,\n",
    "        time_mask_gamma=2.0,\n",
    "        time_mask_value=0.0,\n",
    "        time_mask_mode=\"timestep\",\n",
    "    )\n",
    "\n",
    "    run_trial14_seed_sweep(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe837e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ BEST MODEL (Trial14) ================\n",
      "[SELECTED BY VAL]  (recommended for model selection)\n",
      "  Seed             : 333\n",
      "  Checkpoint       : best_by_val_norm\n",
      "  VAL  RMSE (cyc)   : 2096.842\n",
      "  VAL  MAE  (cyc)   : 1414.320\n",
      "  VAL  RMSE (norm)  : 0.121881\n",
      "  VAL  MAE  (norm)  : 0.094012\n",
      "  TEST RMSE (cyc)   : 1544.697\n",
      "  TEST MAE  (cyc)   : 971.831\n",
      "  TEST RMSE (norm)  : 0.146952\n",
      "  TEST MAE  (norm)  : 0.115696\n",
      "\n",
      "  ---- Trial14 settings (from summary row) ----\n",
      "  pooling          : mean_last_k\n",
      "  pool_last_k      : 10\n",
      "  time_mask_enable : 1\n",
      "  time_mask_pmax   : 0.35\n",
      "  time_mask_gamma  : 2.0\n",
      "  time_mask_mode   : timestep\n",
      "\n",
      "[SELECTED BY TEST] (for reporting only; not for tuning)\n",
      "  Seed             : 333\n",
      "  Checkpoint       : best_by_val_norm\n",
      "  TEST RMSE (cyc)   : 1544.697\n",
      "  TEST MAE  (cyc)   : 971.831\n",
      "  TEST RMSE (norm)  : 0.146952\n",
      "  TEST MAE  (norm)  : 0.115696\n",
      "  VAL  RMSE (cyc)   : 2096.842\n",
      "  VAL  MAE  (cyc)   : 1414.320\n",
      "  VAL  RMSE (norm)  : 0.121881\n",
      "  VAL  MAE  (norm)  : 0.094012\n",
      "\n",
      "---------------- WIN-RATE (last_epoch vs best_by_val_norm) ----------------\n",
      "- val_rmse_cycles: last wins=0, best wins=5, ties=0 | mean(last-best)=560.940093\n",
      "- test_rmse_cycles: last wins=1, best wins=4, ties=0 | mean(last-best)=137.428133\n",
      "- val_rmse_norm: last wins=0, best wins=5, ties=0 | mean(last-best)=0.022632\n",
      "- test_rmse_norm: last wins=1, best wins=4, ties=0 | mean(last-best)=0.009022\n",
      "=====================================================\n",
      "\n",
      "Saved -> ./Trial14\\BEST_MODEL_BY_VAL.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================\n",
    "# Trial14 paths\n",
    "# ============================\n",
    "TRIAL14_DIR = \"./Trial14\"\n",
    "SUMMARY_CSV = os.path.join(TRIAL14_DIR, \"summary_across_seeds.csv\")\n",
    "\n",
    "BEST_TAG = \"best_by_val_norm\"\n",
    "LAST_TAG = \"last_epoch\"\n",
    "\n",
    "\n",
    "def _require_cols(df: pd.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in summary CSV: {missing}\")\n",
    "\n",
    "\n",
    "def pick_best_row(df: pd.DataFrame, metric_prefix: str = \"val\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    metric_prefix: \"val\" or \"test\"\n",
    "    Sort rule (lower is better):\n",
    "      1) <prefix>_rmse_cycles ascending\n",
    "      2) <prefix>_mae_cycles  ascending\n",
    "    \"\"\"\n",
    "    rmse_col = f\"{metric_prefix}_rmse_cycles\"\n",
    "    mae_col = f\"{metric_prefix}_mae_cycles\"\n",
    "    _require_cols(df, [\"seed\", \"checkpoint\", rmse_col, mae_col])\n",
    "\n",
    "    df_sorted = df.sort_values(\n",
    "        by=[rmse_col, mae_col],\n",
    "        ascending=[True, True]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return df_sorted.iloc[0]\n",
    "\n",
    "\n",
    "def win_rate(df: pd.DataFrame, metric: str) -> dict:\n",
    "    \"\"\"\n",
    "    Compare BEST_TAG vs LAST_TAG within each seed on the given metric (lower is better).\n",
    "    Returns wins for last, wins for best, ties, and mean(last-best).\n",
    "    \"\"\"\n",
    "    _require_cols(df, [\"seed\", \"checkpoint\", metric])\n",
    "\n",
    "    wins_last = 0\n",
    "    wins_best = 0\n",
    "    ties = 0\n",
    "    diffs = []\n",
    "\n",
    "    for seed, g in df.groupby(\"seed\"):\n",
    "        ckpts = set(g[\"checkpoint\"].astype(str).values)\n",
    "        if not ({BEST_TAG, LAST_TAG} <= ckpts):\n",
    "            continue\n",
    "\n",
    "        b = float(g.loc[g[\"checkpoint\"] == BEST_TAG, metric].iloc[0])\n",
    "        l = float(g.loc[g[\"checkpoint\"] == LAST_TAG, metric].iloc[0])\n",
    "\n",
    "        if np.isfinite(b) and np.isfinite(l):\n",
    "            diffs.append(l - b)  # negative => last better\n",
    "            if l < b:\n",
    "                wins_last += 1\n",
    "            elif b < l:\n",
    "                wins_best += 1\n",
    "            else:\n",
    "                ties += 1\n",
    "\n",
    "    return {\n",
    "        \"metric\": metric,\n",
    "        \"wins_last\": wins_last,\n",
    "        \"wins_best\": wins_best,\n",
    "        \"ties\": ties,\n",
    "        \"mean(last-best)\": float(np.mean(diffs)) if diffs else float(\"nan\"),\n",
    "        \"std(last-best)\": float(np.std(diffs, ddof=0)) if diffs else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(SUMMARY_CSV):\n",
    "        raise FileNotFoundError(f\"Not found: {SUMMARY_CSV}\")\n",
    "\n",
    "    df = pd.read_csv(SUMMARY_CSV)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Trial14 summary columns sanity check\n",
    "    # (matches ms dict in Trial14 run_one_seed.export_ckpt)\n",
    "    # -----------------------------\n",
    "    needed = [\n",
    "        \"seed\", \"checkpoint\",\n",
    "        \"train_rmse_cycles\", \"train_mae_cycles\", \"train_rmse_norm\", \"train_mae_norm\",\n",
    "        \"val_rmse_cycles\", \"val_mae_cycles\", \"val_rmse_norm\", \"val_mae_norm\",\n",
    "        \"test_rmse_cycles\", \"test_mae_cycles\", \"test_rmse_norm\", \"test_mae_norm\",\n",
    "        \"stopped_epoch\", \"best_val_rmse_norm\",\n",
    "        \"alpha\", \"ph_consecutive_m\", \"rep_method\", \"lambdas\", \"lambda_to_plot\",\n",
    "        \"feature_dim\", \"features\",\n",
    "        # Trial14 extras\n",
    "        \"pooling\", \"pool_last_k\",\n",
    "        \"time_mask_enable\", \"time_mask_pmax\", \"time_mask_gamma\", \"time_mask_mode\",\n",
    "    ]\n",
    "    _require_cols(df, needed)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) VAL 기준 best (권장)\n",
    "    # -----------------------------\n",
    "    best_val = pick_best_row(df, metric_prefix=\"val\")\n",
    "    best_val_seed = int(best_val[\"seed\"])\n",
    "    best_val_ckpt = str(best_val[\"checkpoint\"])\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) TEST 기준 best (보고용)\n",
    "    # -----------------------------\n",
    "    best_test = pick_best_row(df, metric_prefix=\"test\")\n",
    "    best_test_seed = int(best_test[\"seed\"])\n",
    "    best_test_ckpt = str(best_test[\"checkpoint\"])\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) win-rate (seed별 last vs best 비교)\n",
    "    # -----------------------------\n",
    "    wr_val_rmse = win_rate(df, \"val_rmse_cycles\")\n",
    "    wr_test_rmse = win_rate(df, \"test_rmse_cycles\")\n",
    "    wr_val_rmse_norm = win_rate(df, \"val_rmse_norm\")\n",
    "    wr_test_rmse_norm = win_rate(df, \"test_rmse_norm\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4) 출력\n",
    "    # -----------------------------\n",
    "    print(\"\\n================ BEST MODEL (Trial14) ================\")\n",
    "    print(\"[SELECTED BY VAL]  (recommended for model selection)\")\n",
    "    print(f\"  Seed             : {best_val_seed}\")\n",
    "    print(f\"  Checkpoint       : {best_val_ckpt}\")\n",
    "    print(f\"  VAL  RMSE (cyc)   : {best_val['val_rmse_cycles']:.3f}\")\n",
    "    print(f\"  VAL  MAE  (cyc)   : {best_val['val_mae_cycles']:.3f}\")\n",
    "    print(f\"  VAL  RMSE (norm)  : {best_val['val_rmse_norm']:.6f}\")\n",
    "    print(f\"  VAL  MAE  (norm)  : {best_val['val_mae_norm']:.6f}\")\n",
    "    print(f\"  TEST RMSE (cyc)   : {best_val['test_rmse_cycles']:.3f}\")\n",
    "    print(f\"  TEST MAE  (cyc)   : {best_val['test_mae_cycles']:.3f}\")\n",
    "    print(f\"  TEST RMSE (norm)  : {best_val['test_rmse_norm']:.6f}\")\n",
    "    print(f\"  TEST MAE  (norm)  : {best_val['test_mae_norm']:.6f}\")\n",
    "\n",
    "    print(\"\\n  ---- Trial14 settings (from summary row) ----\")\n",
    "    print(f\"  pooling          : {best_val['pooling']}\")\n",
    "    print(f\"  pool_last_k      : {best_val['pool_last_k']}\")\n",
    "    print(f\"  time_mask_enable : {best_val['time_mask_enable']}\")\n",
    "    print(f\"  time_mask_pmax   : {best_val['time_mask_pmax']}\")\n",
    "    print(f\"  time_mask_gamma  : {best_val['time_mask_gamma']}\")\n",
    "    print(f\"  time_mask_mode   : {best_val['time_mask_mode']}\")\n",
    "\n",
    "    print(\"\\n[SELECTED BY TEST] (for reporting only; not for tuning)\")\n",
    "    print(f\"  Seed             : {best_test_seed}\")\n",
    "    print(f\"  Checkpoint       : {best_test_ckpt}\")\n",
    "    print(f\"  TEST RMSE (cyc)   : {best_test['test_rmse_cycles']:.3f}\")\n",
    "    print(f\"  TEST MAE  (cyc)   : {best_test['test_mae_cycles']:.3f}\")\n",
    "    print(f\"  TEST RMSE (norm)  : {best_test['test_rmse_norm']:.6f}\")\n",
    "    print(f\"  TEST MAE  (norm)  : {best_test['test_mae_norm']:.6f}\")\n",
    "    print(f\"  VAL  RMSE (cyc)   : {best_test['val_rmse_cycles']:.3f}\")\n",
    "    print(f\"  VAL  MAE  (cyc)   : {best_test['val_mae_cycles']:.3f}\")\n",
    "    print(f\"  VAL  RMSE (norm)  : {best_test['val_rmse_norm']:.6f}\")\n",
    "    print(f\"  VAL  MAE  (norm)  : {best_test['val_mae_norm']:.6f}\")\n",
    "\n",
    "    print(\"\\n---------------- WIN-RATE (last_epoch vs best_by_val_norm) ----------------\")\n",
    "    print(f\"- {wr_val_rmse['metric']}: last wins={wr_val_rmse['wins_last']}, \"\n",
    "          f\"best wins={wr_val_rmse['wins_best']}, ties={wr_val_rmse['ties']} | \"\n",
    "          f\"mean(last-best)={wr_val_rmse['mean(last-best)']:.6f}\")\n",
    "    print(f\"- {wr_test_rmse['metric']}: last wins={wr_test_rmse['wins_last']}, \"\n",
    "          f\"best wins={wr_test_rmse['wins_best']}, ties={wr_test_rmse['ties']} | \"\n",
    "          f\"mean(last-best)={wr_test_rmse['mean(last-best)']:.6f}\")\n",
    "    print(f\"- {wr_val_rmse_norm['metric']}: last wins={wr_val_rmse_norm['wins_last']}, \"\n",
    "          f\"best wins={wr_val_rmse_norm['wins_best']}, ties={wr_val_rmse_norm['ties']} | \"\n",
    "          f\"mean(last-best)={wr_val_rmse_norm['mean(last-best)']:.6f}\")\n",
    "    print(f\"- {wr_test_rmse_norm['metric']}: last wins={wr_test_rmse_norm['wins_last']}, \"\n",
    "          f\"best wins={wr_test_rmse_norm['wins_best']}, ties={wr_test_rmse_norm['ties']} | \"\n",
    "          f\"mean(last-best)={wr_test_rmse_norm['mean(last-best)']:.6f}\")\n",
    "    print(\"=====================================================\\n\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5) 기록 저장 (VAL 기준 best)\n",
    "    # -----------------------------\n",
    "    out_txt = os.path.join(TRIAL14_DIR, \"BEST_MODEL_BY_VAL.txt\")\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"BEST MODEL (Trial14) - Selected by VAL\\n\")\n",
    "        f.write(f\"seed={best_val_seed}\\n\")\n",
    "        f.write(f\"checkpoint={best_val_ckpt}\\n\")\n",
    "        f.write(f\"val_rmse_cycles={best_val['val_rmse_cycles']}\\n\")\n",
    "        f.write(f\"val_mae_cycles={best_val['val_mae_cycles']}\\n\")\n",
    "        f.write(f\"val_rmse_norm={best_val['val_rmse_norm']}\\n\")\n",
    "        f.write(f\"val_mae_norm={best_val['val_mae_norm']}\\n\")\n",
    "        f.write(f\"test_rmse_cycles={best_val['test_rmse_cycles']}\\n\")\n",
    "        f.write(f\"test_mae_cycles={best_val['test_mae_cycles']}\\n\")\n",
    "        f.write(f\"test_rmse_norm={best_val['test_rmse_norm']}\\n\")\n",
    "        f.write(f\"test_mae_norm={best_val['test_mae_norm']}\\n\")\n",
    "\n",
    "        # Trial14 run settings / provenance\n",
    "        for k in [\n",
    "            \"stopped_epoch\", \"best_val_rmse_norm\",\n",
    "            \"alpha\", \"ph_consecutive_m\", \"rep_method\", \"lambdas\", \"lambda_to_plot\",\n",
    "            \"feature_dim\", \"features\",\n",
    "            # Trial14 extras\n",
    "            \"pooling\", \"pool_last_k\",\n",
    "            \"time_mask_enable\", \"time_mask_pmax\", \"time_mask_gamma\", \"time_mask_mode\",\n",
    "        ]:\n",
    "            if k in best_val.index:\n",
    "                f.write(f\"{k}={best_val[k]}\\n\")\n",
    "\n",
    "    print(f\"Saved -> {out_txt}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08334254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ BEST MODEL (Trial14) ================\n",
      "[SELECTED BY VAL (norm)]  (recommended for model selection)\n",
      "  Seed             : 333\n",
      "  Checkpoint       : best_by_val_norm\n",
      "  VAL  RMSE (norm)  : 0.121881\n",
      "  VAL  MAE  (norm)  : 0.094012\n",
      "  VAL  RMSE (cyc)   : 2096.842\n",
      "  VAL  MAE  (cyc)   : 1414.320\n",
      "  TEST RMSE (norm)  : 0.146952\n",
      "  TEST MAE  (norm)  : 0.115696\n",
      "  TEST RMSE (cyc)   : 1544.697\n",
      "  TEST MAE  (cyc)   : 971.831\n",
      "\n",
      "  ---- Trial14 settings (from summary row) ----\n",
      "  pooling          : mean_last_k\n",
      "  pool_last_k      : 10\n",
      "  time_mask_enable : 1\n",
      "  time_mask_pmax   : 0.35\n",
      "  time_mask_gamma  : 2.0\n",
      "  time_mask_mode   : timestep\n",
      "\n",
      "[SELECTED BY TEST (norm)] (for reporting only; not for tuning)\n",
      "  Seed             : 9819123\n",
      "  Checkpoint       : best_by_val_norm\n",
      "  TEST RMSE (norm)  : 0.111484\n",
      "  TEST MAE  (norm)  : 0.079971\n",
      "  TEST RMSE (cyc)   : 1599.707\n",
      "  TEST MAE  (cyc)   : 1033.154\n",
      "  VAL  RMSE (norm)  : 0.157378\n",
      "  VAL  MAE  (norm)  : 0.126985\n",
      "  VAL  RMSE (cyc)   : 3006.334\n",
      "  VAL  MAE  (cyc)   : 2116.039\n",
      "\n",
      "---------------- WIN-RATE (last_epoch vs best_by_val_norm) ----------------\n",
      "- val_rmse_norm: last wins=0, best wins=5, ties=0 | mean(last-best)=0.022632\n",
      "- test_rmse_norm: last wins=1, best wins=4, ties=0 | mean(last-best)=0.009022\n",
      "- val_rmse_cycles: last wins=0, best wins=5, ties=0 | mean(last-best)=560.940093\n",
      "- test_rmse_cycles: last wins=1, best wins=4, ties=0 | mean(last-best)=137.428133\n",
      "=====================================================\n",
      "\n",
      "Saved -> ./Trial14\\BEST_MODEL_BY_VAL_NORM.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================\n",
    "# Trial14 paths\n",
    "# ============================\n",
    "TRIAL14_DIR = \"./Trial14\"\n",
    "SUMMARY_CSV = os.path.join(TRIAL14_DIR, \"summary_across_seeds.csv\")\n",
    "\n",
    "BEST_TAG = \"best_by_val_norm\"\n",
    "LAST_TAG = \"last_epoch\"\n",
    "\n",
    "\n",
    "def _require_cols(df: pd.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in summary CSV: {missing}\")\n",
    "\n",
    "\n",
    "def pick_best_row_by_norm(df: pd.DataFrame, metric_prefix: str = \"val\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    metric_prefix: \"val\" or \"test\"\n",
    "    Sort rule (lower is better):\n",
    "      1) <prefix>_rmse_norm   ascending\n",
    "      2) <prefix>_mae_norm    ascending\n",
    "      3) <prefix>_rmse_cycles ascending  (tie-break)\n",
    "      4) <prefix>_mae_cycles  ascending  (tie-break)\n",
    "    \"\"\"\n",
    "    rmse_norm = f\"{metric_prefix}_rmse_norm\"\n",
    "    mae_norm = f\"{metric_prefix}_mae_norm\"\n",
    "    rmse_cyc = f\"{metric_prefix}_rmse_cycles\"\n",
    "    mae_cyc = f\"{metric_prefix}_mae_cycles\"\n",
    "\n",
    "    _require_cols(df, [\"seed\", \"checkpoint\", rmse_norm, mae_norm, rmse_cyc, mae_cyc])\n",
    "\n",
    "    df_sorted = df.sort_values(\n",
    "        by=[rmse_norm, mae_norm, rmse_cyc, mae_cyc],\n",
    "        ascending=[True, True, True, True]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return df_sorted.iloc[0]\n",
    "\n",
    "\n",
    "def win_rate(df: pd.DataFrame, metric: str) -> dict:\n",
    "    \"\"\"\n",
    "    Compare BEST_TAG vs LAST_TAG within each seed on the given metric (lower is better).\n",
    "    Returns wins for last, wins for best, ties, and mean(last-best).\n",
    "    \"\"\"\n",
    "    _require_cols(df, [\"seed\", \"checkpoint\", metric])\n",
    "\n",
    "    wins_last = 0\n",
    "    wins_best = 0\n",
    "    ties = 0\n",
    "    diffs = []\n",
    "\n",
    "    for seed, g in df.groupby(\"seed\"):\n",
    "        ckpts = set(g[\"checkpoint\"].astype(str).values)\n",
    "        if not ({BEST_TAG, LAST_TAG} <= ckpts):\n",
    "            continue\n",
    "\n",
    "        b = float(g.loc[g[\"checkpoint\"] == BEST_TAG, metric].iloc[0])\n",
    "        l = float(g.loc[g[\"checkpoint\"] == LAST_TAG, metric].iloc[0])\n",
    "\n",
    "        if np.isfinite(b) and np.isfinite(l):\n",
    "            diffs.append(l - b)  # negative => last better\n",
    "            if l < b:\n",
    "                wins_last += 1\n",
    "            elif b < l:\n",
    "                wins_best += 1\n",
    "            else:\n",
    "                ties += 1\n",
    "\n",
    "    return {\n",
    "        \"metric\": metric,\n",
    "        \"wins_last\": wins_last,\n",
    "        \"wins_best\": wins_best,\n",
    "        \"ties\": ties,\n",
    "        \"mean(last-best)\": float(np.mean(diffs)) if diffs else float(\"nan\"),\n",
    "        \"std(last-best)\": float(np.std(diffs, ddof=0)) if diffs else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(SUMMARY_CSV):\n",
    "        raise FileNotFoundError(f\"Not found: {SUMMARY_CSV}\")\n",
    "\n",
    "    df = pd.read_csv(SUMMARY_CSV)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Trial14 summary columns sanity check\n",
    "    # -----------------------------\n",
    "    needed = [\n",
    "        \"seed\", \"checkpoint\",\n",
    "        \"train_rmse_cycles\", \"train_mae_cycles\", \"train_rmse_norm\", \"train_mae_norm\",\n",
    "        \"val_rmse_cycles\", \"val_mae_cycles\", \"val_rmse_norm\", \"val_mae_norm\",\n",
    "        \"test_rmse_cycles\", \"test_mae_cycles\", \"test_rmse_norm\", \"test_mae_norm\",\n",
    "        \"stopped_epoch\", \"best_val_rmse_norm\",\n",
    "        \"alpha\", \"ph_consecutive_m\", \"rep_method\", \"lambdas\", \"lambda_to_plot\",\n",
    "        \"feature_dim\", \"features\",\n",
    "        # Trial14 extras\n",
    "        \"pooling\", \"pool_last_k\",\n",
    "        \"time_mask_enable\", \"time_mask_pmax\", \"time_mask_gamma\", \"time_mask_mode\",\n",
    "    ]\n",
    "    _require_cols(df, needed)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) VAL 기준 best (norm metric 기준) ✅\n",
    "    # -----------------------------\n",
    "    best_val = pick_best_row_by_norm(df, metric_prefix=\"val\")\n",
    "    best_val_seed = int(best_val[\"seed\"])\n",
    "    best_val_ckpt = str(best_val[\"checkpoint\"])\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) TEST 기준 best (norm metric 기준) (보고용)\n",
    "    # -----------------------------\n",
    "    best_test = pick_best_row_by_norm(df, metric_prefix=\"test\")\n",
    "    best_test_seed = int(best_test[\"seed\"])\n",
    "    best_test_ckpt = str(best_test[\"checkpoint\"])\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) win-rate (seed별 last vs best 비교)\n",
    "    # -----------------------------\n",
    "    wr_val_rmse = win_rate(df, \"val_rmse_cycles\")\n",
    "    wr_test_rmse = win_rate(df, \"test_rmse_cycles\")\n",
    "    wr_val_rmse_norm = win_rate(df, \"val_rmse_norm\")\n",
    "    wr_test_rmse_norm = win_rate(df, \"test_rmse_norm\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4) 출력\n",
    "    # -----------------------------\n",
    "    print(\"\\n================ BEST MODEL (Trial14) ================\")\n",
    "    print(\"[SELECTED BY VAL (norm)]  (recommended for model selection)\")\n",
    "    print(f\"  Seed             : {best_val_seed}\")\n",
    "    print(f\"  Checkpoint       : {best_val_ckpt}\")\n",
    "    print(f\"  VAL  RMSE (norm)  : {best_val['val_rmse_norm']:.6f}\")\n",
    "    print(f\"  VAL  MAE  (norm)  : {best_val['val_mae_norm']:.6f}\")\n",
    "    print(f\"  VAL  RMSE (cyc)   : {best_val['val_rmse_cycles']:.3f}\")\n",
    "    print(f\"  VAL  MAE  (cyc)   : {best_val['val_mae_cycles']:.3f}\")\n",
    "    print(f\"  TEST RMSE (norm)  : {best_val['test_rmse_norm']:.6f}\")\n",
    "    print(f\"  TEST MAE  (norm)  : {best_val['test_mae_norm']:.6f}\")\n",
    "    print(f\"  TEST RMSE (cyc)   : {best_val['test_rmse_cycles']:.3f}\")\n",
    "    print(f\"  TEST MAE  (cyc)   : {best_val['test_mae_cycles']:.3f}\")\n",
    "\n",
    "    print(\"\\n  ---- Trial14 settings (from summary row) ----\")\n",
    "    print(f\"  pooling          : {best_val['pooling']}\")\n",
    "    print(f\"  pool_last_k      : {best_val['pool_last_k']}\")\n",
    "    print(f\"  time_mask_enable : {best_val['time_mask_enable']}\")\n",
    "    print(f\"  time_mask_pmax   : {best_val['time_mask_pmax']}\")\n",
    "    print(f\"  time_mask_gamma  : {best_val['time_mask_gamma']}\")\n",
    "    print(f\"  time_mask_mode   : {best_val['time_mask_mode']}\")\n",
    "\n",
    "    print(\"\\n[SELECTED BY TEST (norm)] (for reporting only; not for tuning)\")\n",
    "    print(f\"  Seed             : {best_test_seed}\")\n",
    "    print(f\"  Checkpoint       : {best_test_ckpt}\")\n",
    "    print(f\"  TEST RMSE (norm)  : {best_test['test_rmse_norm']:.6f}\")\n",
    "    print(f\"  TEST MAE  (norm)  : {best_test['test_mae_norm']:.6f}\")\n",
    "    print(f\"  TEST RMSE (cyc)   : {best_test['test_rmse_cycles']:.3f}\")\n",
    "    print(f\"  TEST MAE  (cyc)   : {best_test['test_mae_cycles']:.3f}\")\n",
    "    print(f\"  VAL  RMSE (norm)  : {best_test['val_rmse_norm']:.6f}\")\n",
    "    print(f\"  VAL  MAE  (norm)  : {best_test['val_mae_norm']:.6f}\")\n",
    "    print(f\"  VAL  RMSE (cyc)   : {best_test['val_rmse_cycles']:.3f}\")\n",
    "    print(f\"  VAL  MAE  (cyc)   : {best_test['val_mae_cycles']:.3f}\")\n",
    "\n",
    "    print(\"\\n---------------- WIN-RATE (last_epoch vs best_by_val_norm) ----------------\")\n",
    "    print(f\"- {wr_val_rmse_norm['metric']}: last wins={wr_val_rmse_norm['wins_last']}, \"\n",
    "          f\"best wins={wr_val_rmse_norm['wins_best']}, ties={wr_val_rmse_norm['ties']} | \"\n",
    "          f\"mean(last-best)={wr_val_rmse_norm['mean(last-best)']:.6f}\")\n",
    "    print(f\"- {wr_test_rmse_norm['metric']}: last wins={wr_test_rmse_norm['wins_last']}, \"\n",
    "          f\"best wins={wr_test_rmse_norm['wins_best']}, ties={wr_test_rmse_norm['ties']} | \"\n",
    "          f\"mean(last-best)={wr_test_rmse_norm['mean(last-best)']:.6f}\")\n",
    "    print(f\"- {wr_val_rmse['metric']}: last wins={wr_val_rmse['wins_last']}, \"\n",
    "          f\"best wins={wr_val_rmse['wins_best']}, ties={wr_val_rmse['ties']} | \"\n",
    "          f\"mean(last-best)={wr_val_rmse['mean(last-best)']:.6f}\")\n",
    "    print(f\"- {wr_test_rmse['metric']}: last wins={wr_test_rmse['wins_last']}, \"\n",
    "          f\"best wins={wr_test_rmse['wins_best']}, ties={wr_test_rmse['ties']} | \"\n",
    "          f\"mean(last-best)={wr_test_rmse['mean(last-best)']:.6f}\")\n",
    "    print(\"=====================================================\\n\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5) 기록 저장 (VAL(norm) 기준 best)\n",
    "    # -----------------------------\n",
    "    out_txt = os.path.join(TRIAL14_DIR, \"BEST_MODEL_BY_VAL_NORM.txt\")\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"BEST MODEL (Trial14) - Selected by VAL (norm)\\n\")\n",
    "        f.write(f\"seed={best_val_seed}\\n\")\n",
    "        f.write(f\"checkpoint={best_val_ckpt}\\n\")\n",
    "        f.write(f\"val_rmse_norm={best_val['val_rmse_norm']}\\n\")\n",
    "        f.write(f\"val_mae_norm={best_val['val_mae_norm']}\\n\")\n",
    "        f.write(f\"val_rmse_cycles={best_val['val_rmse_cycles']}\\n\")\n",
    "        f.write(f\"val_mae_cycles={best_val['val_mae_cycles']}\\n\")\n",
    "        f.write(f\"test_rmse_norm={best_val['test_rmse_norm']}\\n\")\n",
    "        f.write(f\"test_mae_norm={best_val['test_mae_norm']}\\n\")\n",
    "        f.write(f\"test_rmse_cycles={best_val['test_rmse_cycles']}\\n\")\n",
    "        f.write(f\"test_mae_cycles={best_val['test_mae_cycles']}\\n\")\n",
    "\n",
    "        for k in [\n",
    "            \"stopped_epoch\", \"best_val_rmse_norm\",\n",
    "            \"alpha\", \"ph_consecutive_m\", \"rep_method\", \"lambdas\", \"lambda_to_plot\",\n",
    "            \"feature_dim\", \"features\",\n",
    "            # Trial14 extras\n",
    "            \"pooling\", \"pool_last_k\",\n",
    "            \"time_mask_enable\", \"time_mask_pmax\", \"time_mask_gamma\", \"time_mask_mode\",\n",
    "        ]:\n",
    "            if k in best_val.index:\n",
    "                f.write(f\"{k}={best_val[k]}\\n\")\n",
    "\n",
    "    print(f\"Saved -> {out_txt}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2574860f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] train: λ=0.20:0.700, λ=0.40:0.643, λ=0.60:0.557, λ=0.80:0.429 | mean_all=0.582\n",
      "[OK] val: λ=0.20:0.650, λ=0.40:0.700, λ=0.60:0.650, λ=0.80:0.250 | mean_all=0.562\n",
      "[OK] test: λ=0.20:0.600, λ=0.40:0.800, λ=0.60:0.800, λ=0.80:0.700 | mean_all=0.725\n",
      "\n",
      "==================== DONE ====================\n",
      "Saved:\n",
      " - ./Trial14\\seed_333\\best_by_val_norm\\alpha_lambda_eval\\alpha_lambda_summary_seed333_best_by_val_norm.csv\n",
      " - ./Trial14\\seed_333\\best_by_val_norm\\alpha_lambda_eval\\alpha_lambda_per_file_seed333_best_by_val_norm.csv\n",
      "==============================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# USER CONFIG (Trial14)\n",
    "# =========================\n",
    "TRIAL14_DIR = r\"./Trial14\"          # ✅ Trial14 루트 폴더\n",
    "SEED = 333                          # 선택된 seed\n",
    "CKPT = \"best_by_val_norm\"           # \"best_by_val_norm\" or \"last_epoch\"\n",
    "SPLITS = [\"train\", \"val\", \"test\"]   # 평가할 split\n",
    "LAM_STRS = [\"0.20\", \"0.40\", \"0.60\", \"0.80\"]\n",
    "\n",
    "# (선택) 후반 λ를 더 중요하게 보고 싶으면 가중치 사용\n",
    "# 예: λ=0.2,0.4,0.6,0.8 가중치 = 1,1,2,3\n",
    "LAMBDA_WEIGHTS = None  # 또는 {\"0.20\":1, \"0.40\":1, \"0.60\":2, \"0.80\":3}\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def _require_cols(df: pd.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "\n",
    "def compute_alpha_lambda_rates(dfm: pd.DataFrame, lam_strs, weights=None) -> dict:\n",
    "    \"\"\"\n",
    "    dfm: <split>_prognostics_metrics_per_file.csv  (per-file summary)\n",
    "    Returns:\n",
    "      - per-lambda success rate (mean of alpha_lambda_ok_{lam})\n",
    "      - overall mean rate (simple mean or weighted mean)\n",
    "    \"\"\"\n",
    "    rates = {}\n",
    "\n",
    "    for ls in lam_strs:\n",
    "        col = f\"alpha_lambda_ok_{ls}\"\n",
    "        if col in dfm.columns:\n",
    "            rates[f\"rate_{ls}\"] = float(dfm[col].mean())  # 0/1 평균 = 성공률\n",
    "        else:\n",
    "            rates[f\"rate_{ls}\"] = np.nan\n",
    "\n",
    "    # overall score\n",
    "    if weights is None:\n",
    "        vals = [rates[f\"rate_{ls}\"] for ls in lam_strs if np.isfinite(rates[f\"rate_{ls}\"])]\n",
    "        rates[\"rate_mean_all\"] = float(np.mean(vals)) if vals else np.nan\n",
    "    else:\n",
    "        num = 0.0\n",
    "        den = 0.0\n",
    "        for ls in lam_strs:\n",
    "            v = rates[f\"rate_{ls}\"]\n",
    "            w = float(weights.get(ls, 0.0))\n",
    "            if np.isfinite(v) and w > 0:\n",
    "                num += w * v\n",
    "                den += w\n",
    "        rates[\"rate_weighted_all\"] = (num / den) if den > 0 else np.nan\n",
    "\n",
    "    return rates\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Trial14 구조: ./Trial14/seed_<seed>/<ckpt>/\n",
    "    seed_dir = os.path.join(TRIAL14_DIR, f\"seed_{SEED}\", CKPT)\n",
    "    if not os.path.isdir(seed_dir):\n",
    "        raise FileNotFoundError(f\"Not found: {seed_dir}\")\n",
    "\n",
    "    out_dir = os.path.join(seed_dir, \"alpha_lambda_eval\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    summary_rows = []\n",
    "    per_file_rows = []  # (선택) 파일별 ok 값도 모아서 저장\n",
    "\n",
    "    for split in SPLITS:\n",
    "        mpath = os.path.join(seed_dir, f\"{split}_prognostics_metrics_per_file.csv\")\n",
    "        if not os.path.exists(mpath):\n",
    "            print(f\"[SKIP] Missing: {mpath}\")\n",
    "            continue\n",
    "\n",
    "        dfm = pd.read_csv(mpath)\n",
    "        _require_cols(dfm, [\"file\"])  # 최소 file은 있어야 함\n",
    "\n",
    "        # split 요약(성공률)\n",
    "        rates = compute_alpha_lambda_rates(dfm, LAM_STRS, weights=LAMBDA_WEIGHTS)\n",
    "\n",
    "        # Trial14 provenance(있으면 같이 저장)\n",
    "        prov_cols = [\n",
    "            \"pooling\", \"pool_last_k\",\n",
    "            \"time_mask_enable\", \"time_mask_pmax\", \"time_mask_gamma\", \"time_mask_mode\",\n",
    "        ]\n",
    "        prov = {}\n",
    "        for c in prov_cols:\n",
    "            if c in dfm.columns:\n",
    "                # per-file csv면 동일값일 가능성이 높지만, 우선 first로 저장\n",
    "                prov[c] = dfm[c].iloc[0]\n",
    "\n",
    "        row = {\n",
    "            \"seed\": SEED,\n",
    "            \"checkpoint\": CKPT,\n",
    "            \"split\": split,\n",
    "            \"n_files\": int(len(dfm)),\n",
    "            **rates,\n",
    "            **prov,\n",
    "        }\n",
    "        summary_rows.append(row)\n",
    "\n",
    "        # (선택) 파일별 pass/fail + t_lambda 저장\n",
    "        keep_cols = [\"file\"]\n",
    "        for ls in LAM_STRS:\n",
    "            c_ok = f\"alpha_lambda_ok_{ls}\"\n",
    "            c_tl = f\"t_lambda_{ls}\"\n",
    "            if c_ok in dfm.columns:\n",
    "                keep_cols.append(c_ok)\n",
    "            if c_tl in dfm.columns:\n",
    "                keep_cols.append(c_tl)\n",
    "\n",
    "        # provenance 컬럼이 dfm에 존재하면 같이 남김(있는 것만)\n",
    "        for c in prov_cols:\n",
    "            if c in dfm.columns and c not in keep_cols:\n",
    "                keep_cols.append(c)\n",
    "\n",
    "        sub = dfm[keep_cols].copy()\n",
    "        sub.insert(0, \"split\", split)\n",
    "        sub.insert(0, \"checkpoint\", CKPT)\n",
    "        sub.insert(0, \"seed\", SEED)\n",
    "        per_file_rows.append(sub)\n",
    "\n",
    "        # 콘솔 출력\n",
    "        msg_parts = []\n",
    "        for ls in LAM_STRS:\n",
    "            v = row.get(f\"rate_{ls}\", np.nan)\n",
    "            if np.isfinite(v):\n",
    "                msg_parts.append(f\"λ={ls}:{v:.3f}\")\n",
    "        msg = \", \".join(msg_parts) if msg_parts else \"no lambda columns found\"\n",
    "        tail = \"\"\n",
    "        if \"rate_weighted_all\" in row and np.isfinite(row[\"rate_weighted_all\"]):\n",
    "            tail = f\" | weighted_all={row['rate_weighted_all']:.3f}\"\n",
    "        elif \"rate_mean_all\" in row and np.isfinite(row[\"rate_mean_all\"]):\n",
    "            tail = f\" | mean_all={row['rate_mean_all']:.3f}\"\n",
    "        print(f\"[OK] {split}: {msg}{tail}\")\n",
    "\n",
    "    # 저장: split별 요약\n",
    "    df_summary = pd.DataFrame(summary_rows)\n",
    "    out_summary = os.path.join(out_dir, f\"alpha_lambda_summary_seed{SEED}_{CKPT}.csv\")\n",
    "    df_summary.to_csv(out_summary, index=False)\n",
    "\n",
    "    # 저장: 파일별 pass/fail (선택)\n",
    "    out_pf = None\n",
    "    if per_file_rows:\n",
    "        df_pf = pd.concat(per_file_rows, axis=0, ignore_index=True)\n",
    "        out_pf = os.path.join(out_dir, f\"alpha_lambda_per_file_seed{SEED}_{CKPT}.csv\")\n",
    "        df_pf.to_csv(out_pf, index=False)\n",
    "\n",
    "    print(\"\\n==================== DONE ====================\")\n",
    "    print(\"Saved:\")\n",
    "    print(\" -\", out_summary)\n",
    "    if out_pf:\n",
    "        print(\" -\", out_pf)\n",
    "    print(\"==============================================\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2941efb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAGGCAYAAACNCg6xAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAgcBJREFUeJztnQd4FOUWhr/0ACGBACH0XkQUBARpIlIFwV5RsFxsWLFgR0BFBBW7V+x6LSjYUECqFJFeVKQTeguBBAjpe5/v38yyu9mQTdhN/d7n+dnMv9N2Zpj55pzznxNgs9lsEEIIIYQQeRKY9yxCCCGEEELCSQghhBAiH8jiJIQQQgjhJRJOQgghhBBeIuEkhBBCCOElEk5CCCGEEF4i4SSEEEII4SUSTkIIIYQQXiLhJIQQQgjhJRJOQpQC6tevj8jISNxxxx3IyMgo6t0RQohSi4STEKWAiRMn4tprr8WkSZPw2Wef5WvZ48ePY+TIkWjZsiUqVKiAKlWqoHXr1njggQewd+9elHUuuugiBAQEOFq5cuVw7rnnmmOelZVVoHX+8ccfeO6553D06FH4g61bt+LOO+9Ew4YNER4ebkR1586d8frrr+PkyZN+2aYQZYUA1aoTonTAspPNmjVDrVq1MG/ePK+WSU9PR4cOHbBhwwYMGTLECCYKqX/++Qc///wzvv32WyMcyjL8/RQiY8eONdPx8fH48ssvsXz5cjz55JN44YUX8r3OCRMm4NFHH8X27duNtdCX/PLLL7jmmmsQFhaGwYMHG0GclpaGRYsWYcqUKbjlllvw/vvv+3SbQpQlgot6B4QQvoHWkOuuuw4vvvgi9u/fj9jY2DyX+eGHH7B69Wr873//w4033ujyXUpKinngCiAqKgo33XST41DcddddaN68Od58802MHj0aQUFBxeIwUYhdf/31qFevHubOnYsaNWo4vhs2bBi2bNlihJUvOHHihLFQClHWkKtOiCJi4cKFuOCCC4zrp0GDBnj77bdN/+WXX45BgwYVaJ1NmjQx7qPvvvvOq/lpSSF047hjuXicLS+erE+0YLhbTbgPdAudc845Zj3VqlVD3759sWLFCpf5vvjiC7Rv3x7ly5dH5cqVceGFF+K3335zmWf69Ono2rWreUhXrFgR/fv3NxYxZygUb731VtSuXdtYWigYLrvsMsTFxTnm4bb79OmDqlWrOo75bbfd5tVx8nRszj//fBw7dgwHDx509K9bt84cD8tFRvHKbRw+fNgxD110tDYR7oPlAnTeVx6Xtm3bmv2Mjo42YmjXrl157tfLL79sLIYffvihi2iyaNy4sXHBEm6P2/3kk09yzMd+7qfzPrNv/fr1RmDzXHXp0sVYzti/Y8eOHOt44oknEBoaiiNHjjj6li5daq4DClGe827dumHx4sV5/i4hihMSTkIUAYxx6dmzpwnkHj9+PDp27Ih7770XU6dONcJhwIABBVrvRx99ZD4nT57s1fy0TBDGRdHV5ytuv/12PPjgg6hTpw7GjRuHxx9/3AiJP//80zHPqFGjcPPNNyMkJMRYbTjN+Wkpsfj888+NUIqIiDDreeaZZ8zDmw9tZ6Fx1VVX4fvvvzfi6Z133sH9999vRM3OnTvN9xQ3vXv3NstwX2gpojh13p/8YgmPSpUqOfpmzZqFbdu2mf3gNih4vv76a/Tr189xfK+88krccMMN5u/XXnvN/EY2iktC1x9dbBTBr776qjmOc+bMMaIyr5goulcp2jp16gR/QBdgcnKysWoOHTrUxNXxGHi63tjHY06RRXhe+RuSkpJMTB3Xwd9z8cUXY9myZX7ZXyH8AmOchBCFy8UXX2yLiIiwJSQkmOmsrCxb69atbbGxsbbg4GDbkSNH8r3Of/75h09mW0xMjC0wMNC2Z8+ePJdJTk62NWvWzCxXr1492y233GL78MMPbQcOHMgxb7du3UxzZ8iQIWZZi7lz55r13X///Tnm5e8kmzdvNvt4xRVX2DIzMz3Oc+zYMVulSpVsQ4cOdfl+//79tqioKEc/jxW3N378+Fx/5/fff2/mWb58eZ7HxNPvbt68ue3QoUOmbdiwwfboo4+a9fXv3z/H8XTnq6++MvMuWLDA0cd9Zd/27dtd5o2Li7MFBQXZXnjhBZf+v/76y1wX7v3OJCYmmnVedtllXv0ubpvzf/zxxzm+Y//IkSMd0/ybfTfccEOOeTt27Ghr27atS9+yZcvM/J999pnjnDZp0sTWp08fx/m1jleDBg1svXr18mqfhSgOyOIkRCHDgGwG6l566aWOt3G+tXOaLie6pZytGN7y3nvvGesNrRfeuuvoCqL7xHId0W1DaxHdPPfddx9SU1PzvR8MQObvoVXBHfZbsVXcx2effRaBgYEe56H1hhYJWmcYkG01xhMxoN0KgOdvoEto/vz5Lm4hZ6zjOW3aNHP88wuD52kRYmNsE62EAwcOzOHm4r44x4hxf+mOJatWrcpzO7Q48rjQkuP8m+nyowXqdEH/tOQQujP9BWO73GFc3cqVKx1uX/LNN98YlyndpWTNmjXYvHmzcfPRbWn9LsZJ9ejRAwsWLCjwCEUhChsJJyEKGT4wGHTdtGlTl/7zzjvPfDq76Th0nGLKU3MeVk73Cd1tdFnRPcKHOx9e3sB4E8bG0PXExvgYjs576623MGbMmHz/Pj5Aa9asaWJzTjcPBVOLFi1ynYcPWkJXjiVarEZ3phVbxAc03XiMhapevbpxB/H38BhZMJaGx4buQMY48YH+8ccfey0MGcNFITdz5kzjCuTIxUOHDhn3ozMJCQkmhoj7QRHFfWUcE0lMTMxzO/zNNPhQJLn/5n///dclnsodKx6NLkp/Yf0Wd/cdz6V1vXH/ORrzkksuceyTdS45ctP9d33wwQfmPHhzfIQoDmhUnRCFjPWwtSwr7lYRPvgt+DBivIwn+OBnIDLh8Hg+eDhyyrICMG5o9+7dJmDaWxjzxGDmK664wsTKcLTd888/79hfT3FQmZmZ8AeWBYIWNE8jBIODT92+GAdEwUlLFsUNY6GYPoBxNRSk3Hda4BjTxDggzsPf+corr5g+xlCdDgamMybNgsH0bdq0MekI3njjDUc/LUWMX6MFj6kduF7+DgZEe2NR4TzcV4pATyP1TrefFCkUrH///Te8wf368+Z8OlvULLhNWkkZ08TjwePJ2DKKWQvrt9NSx+PiibzOgRDFBQknIQoZuuf4ILYCly34QCd79uwxI6oIR4HR0uGJs88+2/H3u+++a5IyMmjaEk60rvDN/6GHHirQPjZq1MjlIcw+Bj674z6iistRmND6kpvVifPwYcpA79wepJyHxMTEuIiW3OD8Dz/8sGm0cHC9FEYcoWZBtxkbA7ApNhkgzuDt//znP8gPPNZMT/Df//4XjzzyCOrWrWvchAzi5nGnC9LCsrZ4I1r4GyhOadlxt0h6A929zNG0ZMkSM+DgdFhuYveAc08j5PKC19s999yDjRs3GrHPEXPOllPrXFLceXMuhSjOyFUnRBFA1xGtI1ZcCmM9aN0hjDmyYKwRHzSemjXcnCOSGD9jWZvIWWedZVIB5OWuW7t2rXEdusOHJ0UNXXbODz/G+tBF5by8+3ByusT48KeAcMeyWDHlAt07tIq5W2KseSga+aDl6CtPcUnWftBNyXgiZ7ivjPWxXHEUNe7WMkuwFSSOizz22GNmvzjyjVgWIvftMMO4O1b+I3fRwhF3XA+Pnft6OO2c1iC3feK6KQQPHDjg0UXKNBGEx5ZuS8YXOUNXZH7hOed+f/XVV0asU8A553jiiwDPCdMXMF2CO87XlBDFHVmchCgC+IDr3r27yYtEl9GPP/5oRBSH3tN6xJgaBtJ6k2CQ8zNOyT33E60ATz/9tLFs0SLiCVqzGMTNQGdaYuguoVWJaQ0oKJxz+XA/KRIoaBhAzngbBqTT8mUJQMLfxTQDdGHR2mK5qZi3it8x7QLzCT311FMmhopuHgoGxioxGzddP3Sz8cHO38Z10S3Gof2MieHvYRJHussYh7Vp0yYTYEw3GWOm6MJjagIKBy5DPv30UyMI6ILkA5xxQCxPw20wVUBB4La4LGN06BpkqRorvoqCinFQjMViUkp3LIsijwH3kUH9tNBw3+gaZQ4kxptRYFIAch38TaxFSAtXbnB5WtJ47imenTOH04VIUWO5dwkF1ksvvWQ+27VrZ0QUj2d+oVWQ55bXB48tt+8MRTKPE+OeeL3Q/czjQ+sqA955HiyLqxDFnqIe1idEWYXD1Fu0aGELCQkxaQi+/fZb2969e20XXnihLSAgIMdQdU8wnUG5cuU8Dv3nkP+8hulv27bN9uyzz9ouuOACk8aAQ96rVatmhtkzrYA7X3zxha1hw4a20NBQkz5h5syZOdIRkIyMDLNdDuPnvFznJZdcYlu5cqXLfB999JHtvPPOs4WFhdkqV65shv7PmjXLZZ558+aZYexMQRAeHm5r1KiRSZuwYsUK8318fLxt2LBhZlsVKlQw83Xo0ME2efJkxzpWrVplhtLXrVvXbIu/9dJLL3Ws43Rwn84++2yP382fP99l6P7u3btNigWmUeB+XHPNNeacug/vJ2PGjLHVqlXLpGVwT00wZcoUW5cuXczvYeNv42/cuHGjzRs2bdpk0jXUr1/fHP+KFSvaOnfubHvzzTdtKSkpLukAbr/9drOvnOfaa6+1HTx4MNd0BEzHkBuTJk0y83A9J0+e9DjP6tWrbVdeeaWtSpUq5jzwuuE258yZ49XvEqI4oFp1QgghhBBeohgnIYQQQggvkXASQgghhPASCSchhBBCCC+RcBJCCCGE8BIJJyGEEEIIL5FwEkIIIYTwEgknIfKASSBzK5FREuC+f/LJJ3nOx2ScTJZYUuBv4m9bsWIFSiol/doqKPPnzze/m0k+/b0NfuaXsnpehHdIOIlix19//YWrr77aFJxlQVxmGO7VqxfefPNNFGeY2ZlZtZn5mlmwWVyXv8NT0VXWj2M2bNZyY10vZnnmzdq9HMU///xjqs+z4C7nY4kMZqcuLVmW9+7da373mjVrinpXRDHDEi95NQr+woBZ4Jkxny8XLC/D7P7ewFJK3E8VMS49qOSKKFawLARLN7BEyNChQxEbG4tdu3aZiuussXXfffehOAs+Fk594IEHjMDZv3+/KV3Svn17U3S1VatWjnlZWoSlRlh6guJw9erVpvTF7NmzTdkLlqiwasaxhMWQIUOMIGNdtilTppgSKSwwyxIcJV04sS4bH0K5FfsVZROW4WFpHgu+VNx9992mbA6/s6hevbrH5fmCcfLkSYSGhvpkf1jKhrUf+cLD/4vewH226geK0oOEkyhWsGo9665RWFSqVMnlO9ZGK848++yzOfpYA4yWJ9ZcY103i0WLFnmsM8Y6ZCzay7pxhLXQ3GupsdYba52xLlhJF06ieMLagqxvR1FfVJx77rmmWbAYNYUT+2666aZcl2PBZ4olvnz4cv9ZbJr1DVlXkEWMPVmS3WHdQdYa5Msgi3qL0oFcdaJYwertLALqLpqsQqLufPHFF0ZElCtXzri9WDCVFip3li5daorNUpTR5dWtWzcsXrw4x3wUNOeff7654VLI0KpzJnCfub2jR4/mOa9l+s9rXroJ6tSp49U6C8LKlSvRqVMnc0wbNGjgIvgsWACYxYFpEaBbkvvDN2v2uxcR7tKlizmfdFU0a9YMTz75pPmOsSc81oSWN8v14k08ljO0wt15552myC6LxbKw7ZEjRxzf01pHCyAL77rTu3dvs0/eMGHCBLN/tAK6w6K8fFhb22VBY7pYaTm1jg/ds7SAnCks0stjyQK5LALMv1n8mKI7MzPTZd4TJ07g4YcfNtvnfvC38nfYbCwrdwr+LgpyupX4/4/zzpgxwxFHxv8X999/v9kOzyWPN4UVr0Eeb1pa2XgNuK/bn1hxTF9//bUpaE23Pv+/sei0pxinMzkvtDJRNHkLC1y/9tpr5gWHhadF6UFnUxQrGNdEtxbf5vIKVKZ1ilXpr732WmPZOXTokImDoomeri9LfM2dO9dUZafA4sOeb6Iff/wxLr74YnMjpSvNcrXxQcqHA+MrMjIyzPy5uQJygw8TPqTpqps4caK5iffo0SPHfFw/5+UDiL+XN36+nVr74/4A5M09MTERP/30E6ZPn56jAr0v4IOfFi4e0xtuuAGTJ082b/kUBbfddpvDGkFXIR+mtHgxPovHjg+JTZs2Od6sGZ/FN3NaCEaPHm0eVFu2bHEIVi7HflrquB66LglFW37gA5/nmuds48aNxrpHcWM9OG+++WZ89tlnmDlzptkfC54fXhs8x97AY0JhwGPy6KOPunzHPl47FA/k22+/NYKOx46CjlZEXpu7d+82350pFEiMp+vQoYMRQnTxvvLKK0bsc5uEAobnad68eSY2h65QHgPuO0UXz5czPBb8HTyeFJoU8lbsGV3kdJvTrUq3+fvvv2+OOV3rFCG0xvz6668YP368+X9LMVWYjBkzxlyjFI8U77m55/x9Xpx58MEHjaWJ/594XEUpoqirDAvhzG+//WYLCgoyrWPHjrbHHnvMNnPmTFtaWprLfHFxcWaeF154waX/r7/+sgUHBzv6s7KybE2aNLH16dPH/O1cFb5Bgwa2Xr16Ofouv/xyW3h4uG3Hjh2OvvXr15vt5Oe/SrNmzcz8bBEREbann37alpmZmWO+JUuWOOZj43Lz5s3zuM4777zTMV9gYKDt6quvtiUkJHi1P1zm448/znO+bt26mXlfeeUVR19qaqqtdevWtpiYGMc5+Pzzz80+LFy40GX59957zyy/ePFiM/3aa6+Z6UOHDuW6zeXLl3u9f+5wGS7btm1bl+vj5ZdfNv0//vijmeaxr127tu26665zWf7VV1+1BQQE2LZt2+b1NnlNcnvOLFu2zGzvs88+c7m+3Bk7dqzZnvP1NXLkyHxdW2TIkCFmmdGjR7v0n3feeS779sMPP5j5nn/+eZf5eO1wP7Zs2eLos66rf/75x+Mxdv//w+PAddx1112OvoyMDHOceR3lBa9zrnf79u1e/25eR1yGx8x9PQ0bNsxxzK3vnP9P+eq89O/f31avXr1cv582bZq5D1nHk+esQoUKXv9WUbyRq04UKzh6jhYnvimvXbsWL7/8snmzpgmelhaLqVOnGssHrQCMfbAa34qbNGli3rIJ35hpMr/xxhtx+PBhx3y04NAKxEBsrodv8Hwbp+uDb9AWtIpw+/mB1iy6Od555x2zPC1F7i4U0qJFC+PKooXGCiB1H1Xn/PbKeT/99FNjPeP6aKnyNXQp0A1jwTd3TjO+jC48wjdz/q7mzZu7HHta8Ih17C2L348//miOsb+gtcrZhUJrAn8HLSCEFsZBgwaZ64eB9hZ0S9G6RXekt9DKx+NAl7IFA4ZpTbvsssscfXRzWvBa4/HhtqhRaA31BXfddZfLNC1227Ztc0zz99OtSxebM3TdcT9otXSG7mtek56gxcp5eD4tXVwH+y24rXbt2rnsQ2FBd6zzMc+Nwjgv/H9J9x/PT27HU5RsJJxEsYNxLxRGdBvRlM74ET7wOLR//fr1Zh6KId7sKJLoWnNu//77ryOQnPNZN1b3+T744ANj1qf7i24+Chyuzx1vY2AsOnbsaMQWH+AUY4zD4m9wh/E4PXv2NA/ccePGmQca/6ZgdIcihfPSBTJt2jQjsAYMGODzeBLGcbiPAGratKn5tHLu8JjSDed+PK35rGNPkdG5c2fjRqW7k/FndFn4WkS5nzPG/NSoUcMlRxCPG88vU0YQuvQogOjGyw+Mj6EQo1giPP4UkhSzPJ8WO3fuNLFIjLuzYpAoTAivtzOFMXhcpzN0EzrHdtFdyfNJ968zFL3W986cTkA6v0wQxgoSxgi59zvvQ2Hhrfj193khdIFSkNGtKUoninESxRZaOyii2PhQZgAxH1KMSeHDl2/AfGvmm647Vs4U6yHN2IvchrtzXvegZl/BhxktMbRuMBbldHCINR/kDHR1Tl3gCYpIWoIYU5RfYXem8Jiec845JujVE9bDlG/3tOjRAvXLL78YKxwFB48Hc+J4Om/+gm/+jHGjiKWI4ievL1os8wOFCC07FIAMcme8Dx/GFL4WtAbScpqQkIARI0YY0UsxyrgiPrR9IRz9cexOZ7HJbXue+gszONzCG2tTYZwXii+OpLvnnntMbCMb4YsOjwvFPIPXPQ10ESUHCSdRIqALgOzbt898MgiWNyK+aVqWDk9wPmfrTm7wzZM3X8tC5QytE2eCFdSdFxRvvHl7M681CshXb8nOeZXownC2OlGcOY/64zGlVYyuzryyK9M6w/nYKLQYRPzUU08ZMcXz4YvszDxnDMK14EOK14l7GgcKpuHDh5vvmJOnf//+jmDu/EBLGh+MvC4oBPkgpPXPgoHyPGZ0qzoHSdPVWtgDLRg0Tmuts9Vpw4YNju/LEoVxXmht4/XHEAM2d3i/olVZqQlKNnLViWIFH6ie3liteBXLukLrDN92aQ53n5/TjGcitDLwQU9rj6f4IbroCNdF9xpvaLQgWNDtR3ebN3jKM8U3zDlz5jiEn/OoO3foOiTO83paJ5flKDEKPV/HUHCkn3MKBsZrcJrCkseS0ErDt3TmtPEk6Ci8CN/s3bGsfpaFzxJoZ5JagSO8nI8nR9Xxd9B95gxHCVKoMUEp43BOlwvodFx11VXmevnqq6+MBZQj9ZyFpmWFcb4u+TcTuBYmFI60srz11ls5XEk8Du7Hp7RTGOeFliS6g90bhT3dq/zbk9telCxkcRLFCg575nBhZgemKZ0Pbg555ps9LR501xGKIZrEeROiOGFQN9+qt2/fbm5ODBjm0GRaPChI+JBgfhouz0BzPvgp0miJssqXUITRnURXDC0KfPhyqDKXW7duXZ77TvcVLSsUB7Rk0BLy4Ycfmoc6s4JbcJg8A3bpbmN8Dn8j0yIwrouiyfmBTncczf1MscD95hB6uv1oNeDwc1+XcaArim4nHlNa8njcGWBPcWIFYNOdSFcVg195DBnHxAc094n9FJr8HUw1QFcdLTu0blAEMmCeCUGZ28k6jwwiZ64onj8KEAYe5ydgm8ePx52CjlYgboPr5wADZyj+mMuLYofb5H4V9OHIByEtaLTmuKeF4HVrJTPldcZrjNneCzv2h1Yw7ictfDyfdP/SRcpgfQ42sKyxZYUzPS+8B1gDVJhWw3LLER5bHm9aH3kvcocvZIzX9PSdKIEU9bA+IZyZPn267bbbbrM1b97cDOUPDQ21NW7c2HbffffZDhw4kONgTZkyxdalSxcz1JeNyw0bNsy2ceNGl/lWr15tu/LKK21VqlSxhYWFmaHE1157rW3OnDku8/3+++9mSDe3yyHOHGLv7ZBxzteuXTtb5cqVzVDkmjVr2q6//nrbunXrXObjMPDBgweb9ZcrV86kQDj77LPN8sePH3eZ96uvvrL17NnTVr16dbNOrpvT1lB7X6cj4H6sWLHCDDfnfvE4vfXWWznm5fD/cePGmfl5PLlfPG6jRo2yJSYmmnl4bC+77DJzHHg8+XnDDTfYNm3a5LIu/pYWLVqY35ef1ATWUHmeszvuuMPsA6+ZQYMG2Q4fPuxxmcmTJ5tlOP+ZMGnSJLOeihUr2k6ePJnje6ax4Hni/lStWtU2dOhQ29q1a3P8voKmI/A0tN3Tuo4dO2Z76KGHzLEPCQkxqTnGjx/vklqAcDn+v8ntGDNthKdtuaea8HbYva/TEXz77be5bsM5HcGZnBfrWHhq/N2nQ+kIShcB/KeoxZsQwn/QLcMUCQyALevQ2sK3flrCrISbovCh1ZXWMFqIvS2WK0RxQTFOQogyA+OyGjZs6HAVCiFEflGMkxCiWOHNKETm4clP1XumeGCMCtMiMBjYfTQft5dXvTImV/UnxWEfhBB5I+EkhChWMCDdGgSQGwxKv+iii7xeJ0fUMZCema4Z+O8OR9pxmPrp8HdUQ3HYByFE3hRpjBPjDJiYkBl8mVuFo6HyGnVA3zhzsTBzMRPtsTCqYjeEKD3wXsD/36eDqREKkoMpN5iRnjmsTsfp8oCVln0QQhRzixPzvXAYJ6uuMy9PXjCQkEOIOQyaQ7KZH4flHFheIb/1xIQQxRP+f2YrTJgPq6jrihWHfRBC5E2xGVXHmIO8LE5Mk88Yhb///tvRx/pXTJ7H/DtCCCGEEP6kRMU4LVmyJIepmpYmJnPLDWYodq5DxpIWzGhcpUoVn5R7EEIIIUTJhjYkJrRlEmAmTi41wolZk1ll3RlOM7MyR6N4KvQ4duxYVakWQgghRJ7s2rXLVDcoNcKpILAkB4PJnYf81q1b18RLORe+9BUsr8ERP0zuZpWoEMKf6JoThY2uOVHarjlam1jqyRtdUKKEE3OYHDhwwKWP06w55MnaRMLCwkzzlAeGy/nj5LJeEV2BEk6iMNA1JwobXXOitF1z1jq9CeEpUZnDO3bsaEbSOTNr1izTL4QQQgjhb4pUOB0/ftxUXmcjdJ/x7507dzrcbIMHD3bMzzQE27Ztw2OPPWYqsbMKOquxP/TQQ0X2G4QQQghRdihS4bRixQqcd955phHGIvHvZ5991pEIzxJRhP5HpiOglYn5n1555RV88MEHyuEkhBBCiEKhSGOcWDLhdGmkPvnkE4/LrF692s97BmRmZhqfan7hMsHBwUhJSTHrKIvQVxwUFFTUuyGEEEL4nBIVHF4YUMgx7QGTahZ0eQaxc0hjWc4TValSJXMcyvIxEEIIUfqQcHLDEk0xMTEmgj+/D34m2GTsFguK5pVEqzRC4ZicnIyDBw+a6cIunSGEEEL4EwknJ+has0QThzwWBAqntLQ0hIeHl0nhRKzUEBRPPJZy2wkhhCgtlM0ney5YMU20NIkzwzqGBYkTE0IIIYorEk4eUFzOmaNjKIQQojQi4SSEEEII4SUSTiIH9evXx8SJE3VkhBBCCDcUHO4nMrNsWLr9MA4eS0FMxXC0bxCNoED/Dc1nfqvWrVv7RPAsX74cFSpU8Ml+CSGEEKUJCSc/MGfjYYyfsxL7k1IcfTWiwjFyQAv0bVmjyNIEcNQgk3PmRbVq1Qpln4QQQoiShlx1PmbG3/vxyPcbXEQT2Z+Ygru/WIUZf+/z9SZxyy234Pfff8frr79ugrLZmHWdn9OnT0fbtm0RFhaGRYsWYevWrbjssstQvXp1k2vq/PPPx+zZs0/rquN6WNrmiiuuMKPlmjRpgp9++snnv0MIIYQo7kg4+dg9N3rav/BURMbqG/XzejOfL6Fg6tixI4YOHWrq+7HVqVPHfPf444/jpZdewr///otzzz3XJOfs168f5syZY0rX9O3bFwMGDHCpCeiJUaNG4dprr8W6devM8oMGDUJCQoJPf4cQQghR3JGrzgsGvLkIh46l5jlfakYmjiTnnreIcmlfYgraPT8LYcF513KrVjEMP9/XJc/5oqKiEBoaaqxBLHNCNmzYYD5Hjx6NXr16OeaNjo42BZItxowZg++//95YkO69997TWrVuuOEG8/eLL76IN954A8uWLTPCSwghhCgrSDh5AUWTu+vtTLCLq8JJDNmuXTuXaVqcnnvuOfzyyy/GMpWRkYGTJ0/maXGitcqCgeORkZGOsipCCCFEWUHCyQto+fGGvCxOFpXLh3htcTpT3EfHPfLII5g1axYmTJiAxo0bm/IoV199tSkTczpCQkJcphn3xPIyQgghRFlCwskLvHGXEcYudX5pLg4kpXiMc2IygtiocCwacbHPUxPQVcdRc3mxePFi43ZjoLdlgYqLi/PpvgghhBClFQWH+xCKoWcvPcv87S6LrGmmJPBHPieOhFu6dKkRQfHx8blagzgiburUqVizZg3Wrl2LG2+8UZYjIYQQwksknHxM35axmHBFc1SPDHfpp6Xp3Zva+C2PE11wQUFBaNGihcnDlFvM0quvvorKlSujU6dOZjRdnz590KZNG7/skxBCCFHakKvOD/RoVgUD29bHih1HCy1zeNOmTbFkyRKXPrrkPFmm5s6d69I3bNgwl2l31x2TZ7pz9OjRM9xjIYQQouQh4eQnKJI6Nqrir9ULIYQQogiQq04IIYQQwksknIQQQgghvETCSQghhBDCSySchBBCCCG8RMJJCCGEEMJLJJyEEEIIIbxEwkkIIYQQwksknIQQQgghvETCSTgyik+cOFFHQwghhDgNyhzuL7IygR2LgeMHgIjqQL1OQGCQ3zYnhBBCCP8j4eQHQrZMR8CC0UDS3lOdkTWBvuOAFgP9sUkhhBBCFAJy1fmaf39G+Wl3u4omkrQPmDwYWP+Tzzf5/vvvo2bNmsjKynLpv+yyy3Dbbbdh69at5u/q1asjIiIC559/PmbPnu3z/RBCCCFKOxJOviQrEwEzHwdgQ0COL232jxmP2914PuSaa67B4cOHMW/ePEdfQkICZsyYgUGDBuH48ePo168f5syZg9WrV6Nv374YMGAAdu7c6dP9EEIIIUo7ctV5w3+7AccP5j1fRioCTh4+zQw2IGkPML4JEByW9/oiYoA7f89ztsqVK+OSSy7Bl19+iR49epi+7777DlWrVkX37t0RGBiIVq1aOeYfM2YMvv/+e/z000+49957894PIYQQQhgknLyBoumYm+vtTDituCoYtCwNHToU77zzDsLCwvC///0P119/vRFNtDg999xz+OWXX7Bv3z5kZGTg5MmTsjgJIYQQ+UTCyRto+fGGjFTvRFG5Kt5bnLyErjebzWbEEWOYFi5ciNdee81898gjj2DWrFmYMGECGjdujHLlyuHqq69GWlqa1+sXQgghhISTd3jhLjNkZcI2saUJBA+wYppcCLCPrnvwL5+nJggPD8eVV15pLE1btmxBs2bN0KZNG/Pd4sWLccstt+CKK64w07RAxcXF+XT7QgghRFlAweE+PZpBsPV5yfyZMzw8e7rvS37L50R3HS1OH330kfnbokmTJpg6dSrWrFmDtWvX4sYbb8wxAk8IIYQQeSPh5GvOGoDkS98FImu49tPSdO1nfs3jdPHFFyM6OhobN2404sji1VdfNQHknTp1Mi69Pn36OKxRQgghhPAexTj5gfTGl8DW+moE7PqzUDOHMxB87969HsupzJ0716Vv2LBhLtNy3QkhhBB5I+HkLyiSGnT12+qFEEIIUfjIVSeEEEII4SUSTkIIIYQQXiLhJIQQQgjhJRJOQgghhBBeIuEkhBBCCFFShNPbb79thssz83WHDh2wbNmy084/ceJEkxWbZUPq1KmDhx56CCkpKYW2v0IUJzKzMrHiwAqsTVtrPjkthK45IUppOoJvvvkGw4cPx3vvvWdEE0URkzMygWNMTM46bV9++SUef/xxkxmbyRw3bdpkSokEBASYJI9ClCVm75iNl5a9hAPJB8z0t3O+RfXy1fF4+8fRs17Pot49UQrRNSdEEVucKHaGDh2KW2+9FS1atDACqnz58kYYeeKPP/5A586dTVZsWql69+6NG264IU8rlRCl8QE2fP5wh2iyOJh80PTzeyF0zQlRiixOaWlpWLlyJZ544gmXzNc9e/bEkiVLPC5DK9MXX3xhhFL79u2xbds2/Prrr7j55psLcc+FKFrojqOlyeahkLTV9/Sip7Hm4BoEBhS5N16UArJsWfh207e5XnMBCMC4ZePQvU53BPm5QoIQZVY4xcfHIzMzE9WrV3fp5/SGDRs8LkNLE5fr0qULbDYbMjIycNddd+HJJ5/MdTupqammWSQlJZnP9PR005zhNNfLArgFLYLL5UlGZgZW71+N+JPxqFquKtrEtPHrDYV16lq1aoXXXnvNJ+ujFfDo0aP4/vvvC7Q8jx+PBY9pUJBupL5k8d7FOSxN7pzIOIFP13/q0+0KkRsUT/uT92PY7GHoVLMTmlRqgsaVGqNSWCUdNOETrOe1+3PbV+RnvSWq5Mr8+fPx4osv4p133jExUVu2bMEDDzyAMWPG4JlnnvG4zNixYzFq1Kgc/b/99ptxCzoTHByM2NhYHD9+3FjECsrve3/H63+9jkMphxx91cKr4YFzHkC3mt3gDygiuc+WMPTFRcR1FnR93JeTJ09iwYIFZj3izN/4t2Vsw5q0Nfgr/S8dTlEsWbxvsWkWkQGRiA2KRfWg6uaTrWpgVQQF6GVKFIxZs2b55dAlJyd7PW+AzTKRFDJ8sFK4fPfdd7j88ssd/UOGDDGWjh9//DHHMl27dsUFF1yA8ePHO/rourvjjjuM2KGrzxuLE0fj0XIVGRnpMi9H5+3atcsxyq8gMLbk4QUP5+inKZtM6DYBPev6NnCX1qHPPvvMpW/r1q3mmDz22GNYtGgRKlSogF69epm4sqpVq5p5eOwpOilAeS7OO+88Y2GaMGECRo8e7bK+OXPm4KKLLvJ6n3gsWTiYx7qgx7Ksw/+am45uwq/bf8X0HdON9TI/PNLmETSPbu63/RNlhw0JGzBh1QSfrCskMAQNoxoaq1TTSk3RpHIT83d0eLRP1i9KJ+np6UY08TkWEhLi8/VTG/DZmJiYmEMbFBuLU2hoKNq2bWseyJZwonuH0/fee2+uitBdHFluoNz0X1hYmGnu8MC7H3y6DjlCj9vwJMK8iT15ecXLHr+z4gDGLx+PHnV7+NRt98Ybb2Dz5s1o2bKlQ/Dwt1Fk/uc//zGjFWn9GTFiBK6//nrMnTsX+/btw6BBg/Dyyy/jiiuuwLFjx7Bw4ULz+x999FHjLuWF9PHHH5v1RUdH5+uYcF6uy9NxFqdn/4n9+GXbL5i2bRq2HN2S4/uI4AhkIhMnM056XJ7XGUfX3XT2TYo3ET6hXY12+HzD52bwgac4J15zMeVj8Eq3V8w1u+nIJtM2HtmIY2nHXOZNz0o3/WzOVAmvgqaVm5rWLLqZ+WwQ1QChQaE6i8KBv54p+VlnkbrqmIqAFqZ27dqZYG8+4E+cOGEsKGTw4MGoVauWcbeRAQMGGIsJLSOWq44uOvb7M47mumnXefW2n5aZhqOpR/OMA7ho8kVe3QwYG/XNpd/kOV9UVJQRorQa0dVInn/+eXOc6Nq04GhFWoCYxoHWKLrQrrzyStSrV898f8455zjmZZ4sWuqs9Qn/wocLrZUUS8v3L8/xcAoODEbXWl0xoNEAXFj7QizcvdCMniPO81qWzRHtR0g0CZ/BFz2mueA1x2vM0zXH71vFtDLNgi+0jMezhNSmBPtnXFIcMm2uOccOpxzGkn1LTHNc9wHBqB9V30VMsVUrV828mAlRFBSpcLruuutw6NAhPPvss9i/fz9at26NGTNmOALGd+7c6WLlePrpp81/Fn7u2bMH1apVM6LphRde8Ot+UjTxTctXnE5c+Yq1a9di3rx5iIiIyPEd3XhM5dCjRw8jlpg7i9NXX301Kleu7Pd9E6fevBfvWWzE0vxd85GaecqlbNG6WmsjlnrX641K4acCbZmn6dWLXnXJ40RoaaJoUh4n4WsKcs3xfh1bIdY0Cn4LXutbj249JaiyRdWR1CMuy2fYMowFi40uawsGnVsiyrTopmgU1QjhwQoLEP6nyGKcigq6n2ih8eTHZFzO9u3b0aBBA5e4HF9ZnJz/0/vS4kQYf0ThSasdueSSS4wFaty4cTnmrVGjhol54qlnbiwGyjO2ieJ16dKl5vczsShjzX744QcUhNyOZVmHx3xd/DpM2zoNM+JmeLxe6kXWw6UNL0X/Bv1RJ7JOnu7hZXuXYdaSWejVsRfa12wvS5PwK/665vh/g/dZFzF1ZBO2JW5DRlbeA0yYeoP/d4x1qvIp6xRFm6xTpSPG6ddff0W/fv38FuOUmzYo0aPqigpvxQtvKH2m9Ml1qLgVezLjqhk+f7jRVccYLYs2bdpgypQpJtCdowU97k9AgEkoykarH112FFB0obqvT5wZu5J2GcsS285jO3N8XzmsMvo26IsBDQegZdWWXt/oeR21q94OB0MPmk/l0BH+xl/XHK/5auWrmda5VmdHf3pmuhFPFFGbj2x2xE65v8xy5On2xO2mzYyb6eivGFLRBKA7u/qYKqF8iOuoaiG8RcLJh/AG8tj5j+Hh3x/ONQ7AX7EnFEi0FnEkG91zw4YNw6RJk0xmdY6sY3A3Y8K+/vprfPDBB1ixYoUJxKeLjuVtuCzdpmeddZZjfTNnzjTlb6pUqWKUuIK888fRlKPGqkSxtPbQ2hzfhwWFmYSBdMV1rNnRjDYSQrgSEhRiRA+bM4dPHsbmo5sdcVNsdOnRBe7MsfRjWHVwlWnO9+M6Fes43HyWoKoVUUtJY0WeSDj5GKYaeP785/HmP28WauzJI488YgLtWbqGI+joJlu8eLEZSUdxxEBvWpT69u1r4sZoimSOJbr2aKLkd6+88opx8RGWwmHeLAbuM5Cc8VL5SUdQVmHsxu+7fsfP237Got2LTIyGM7xht49tj0sbXWqulYjQnDFoQoi8qVKuimkX1LjA0UeX3o6kHadG9SVsNJ/uXgC+1NLyyzZ756nyROWDy9utU5arL7qpSZWg/6fCGQknP8Akl/2a9sOa+DU4lHzImJ79nTm8adOmHkvVTJ061eP8tCwxED83GHjP2CeRN3QRrDyw0liWZsXNMm+47tA1QMtSvwb9TMyFEML3cPRpo0qNTLukgf0lkCSmJjrEFN19FFS0TqVkprgsn5yRbKzD7hZiWqJcgtErNzUWK7nGyyYSTn6C/6HOjz3fX6sXxQCOCvp568/4ZfsvJveSOzHlYtCvYT8T6O3uZhBCFB5RYVHmfux8T2ZM6q5ju1xyTlFU7Tm+J8fy7GObt2ueoy88KNy8EPH/Nq1UlqDitkTpRsJJiHxACyKHRTNB5b8J/+b4nqZ+umNpXTq/+vl6IxWiGL/cMkcUW+/6vV1yqpkkngmugorWKGdorfr78N+mOcOwDOdAdDaO9qM1TJQOdCaFyIPk9GTM2TnHuOL+3Pencc05w7pbLGxKy1L3ut1RLricjqkQJZSKoRVxXsx5plnw/zwtTu6JPGmxck9Wy3gqtgW7Fzj6QgNDjfvQOe8UP1VmpmQi4SSEBxhkunTfUhPkPXfnXI/lTVpWaWmCvPvW72uCVIUQpRPmiGJMExtLZjm/VFklZqxAdFqn3OMc07LSjIXa3UrNXH1WILrl7mMdP44kFMUXCScPlLGcoH6hJB5D7jNvbIxbmr59uikB4Q6DRPs37G8ab3BCiLILc0GdW+1c05zvI4x5tNx8lpWKo/3crdXMRcW2eO9ilzIzDSo1cEniyUaRpUSexQMJJyesPEUsJsxabaLg8Bg6H9PizN7je03cEgUTE+25ExkaiT71+xhXHM33unkJIXKD94caETVM61anm6M/JSMFWxO3uuSdorDiiD9nmMKEVis29yS5zjmn2Oj+Yz44UbhIODnBQsGVKlXCwYP2unQsWZLfh2RWVhbS0tJMyRHnOntlBb5tUTTxGPJY+rP48pmQlJZkUgfQFcdUAu4wGWW32t2MWOpau6sqtAshzgjW0Tu7ytmmOd8vD5085OLqM0WQE+Ny5IBjHT+GD7A5x1fWj7QXQXYWVQxQ1wue/5BwciM21p5jxxJP+YX/EZiAkharsnzhUjRZx7K4wNINC/csNEHeTFLJuAN3mG+LcUssqqthxUIIf8JnREz5GNO61OriUveUpWOMq8/JQuUePpBpyzRWLLbpcdNdrOTueacaV26sgSs+QsLJk5m1Rg1ThoRFBfMLl2FG7gsvvLBEuKn8AX93cbE0UcgymR3FEsufuJvFCd/YrOSUtSvWLpL9FEIICxaBd5SZaXTquFhFkK2afWzMJ+deZoYW9RUHVpjmXLWAaRGcc05ZZWbK8kt+QZBwygU++Avy8OcyGRkZCA8PL7PCqTjAQExTVHfrNOw+vjvH9xwGTKFEV1yLKi104xBCFHsYIM7G9CcWFE07Ene4BKKzHUx29ZowbUJcUpxps3bMcvRXCKmQwzpFccV+4RkJJ1FqSEhJwIztM0xyynXx63J8z0y/F9e92IglFtVVQjohREmH8Zh0w7H1R39H/5GUIy6WKasIMutpOnMi/QRWH1xtmjO1I2q7xE5xlB8t8oEBZS921x0JJ1Gi4UiV+bvmG+vS4j2LcwRU8j95h9gOJm6J+Vf0FiWEKAtUDq+M9jXam+ZcZmbHsewiyAmnXH57T+zNsTwt9Wxzd8119DG5b5NKTVzdfdFNTUxVWULCSZQ4+J+fvntTVHfHLPPG5E7z6ObGssRCnwy8FEKIsg7LzDD/HBsT9zrHRLlbpzjtnviX0+vi1+Ww6NeoUMPV3RfdFHUr1i21Vv3S+auKiMwsG5ZuT8DK+ABU2Z6Ajo1jEBSooDtfwf/MFEt0xbn77wmH4DIxJQUT34jKBFmZCNixCLUSliBgRyTQ8EIgsHgE5gshSga0GLWt3tY0Cybr3H1st4uYssrMuLPvxD7Tft/9u6OP+aVcysxkN1rCCvqyvDZtLWIOxKB9zfZFWgc0wFYSUzyfAUlJSYiKikJiYiIiI31nXpzx9z6M+nk99iWmOPpqRIVj5IAW6Nuyhs+2U9Y4cOKAyeJNwcTgR3ciQiLQq14vI5baxbYrW/739T8BM0YASU5m9siaQN9xQIuBRblnopTD0cO//vor+vXrp0EwZYwT6SdcrFPW38fTj3u1fEy5GDSJPuXqY+wUCy0zVssTs3fMxkvLXjL1/5xfkh9v/7gpqF4U2kDCyUei6e4vVrmVeuTwTzvv3tRG4imf/zH5n4Viicne3ItosiQBc570b9QfF9W+yCSWK3NQNE0ebMbKeLzqrv1M4kn4DQkn4QztL4yTcs45ZZWZcb9/e4IuvUZRbtap6KZYc3ANhs8fnmMdTK1AXr3oVZ+Jp/wIJ7nqfOCeo6XJ06XBPp5eft+rRazcdqeBQ2qX7F1ixNK8nfOQknnKcmdxbtVzTZA3y5+U6ariWZl2S9PprroZjwPN+8ttJ4TwOwEBASYfFFv3ut1dYqKYZ8pRYiY7OzpjqtyLqtOj4O5VoEDyJLzYx+/GLRuH7nW6F7rbTsLpDFm2PcHFPecOTzm//8+ny3FO7UqIjQxH9cgwVDef4ahSIRSBZTQOim8p/xz+x4gluuOYTsAdViOnG46xS0zeVuqh5zz1GHDiEHD8IHDioP3T8fch4PAWV/dczpUASXuAj/oCMc2BCjFARHUgolr23zFAhWpAeBTveIX444QQZYlyweXQsmpL05zv+3S7ucROJWwy+aWYCd2Z01mr+N3+5P1YdXAVzo89H4WJhNMZcvBY7qLJmXkbD5mW4wQEBiCmYhhiIsMdourU39kiKyocFcOCS02SRgYcMsCbgon/WdxhqROO+KBgalWtVcn/3RRDKYmnF0POnxneXVN5snuZveUGi4NSQDkEVbXcRVa5yhJZQogzhvfz2Aqxpl1Y+0JHP/NLbTu6zSGmFu9dbKxVeXEoOedztVgKp6NHj+K7777D1q1b8eijjyI6OhqrVq1C9erVUatWLZQlYiqeWXxNRpYNexNTTDsd5UKCEBsVbkQWBZX1Nz+NwKoYjpjIMISHFM8RVSx1MjNuphFMfENwJzQw1FQSH9BwgIlfCgkKKf5i6OQR78VQZs66eEUOE+El7ba3vGDgpovIyhZURmRZf/Mzxi6yymCBayFEweEovLOqnGUauWj/Rbht5m15LletfDUUe+G0bt069OzZ0wRRxcXFYejQoUY4TZ06FTt37sRnn32GskT7BtFm9Nz+xBSPRkXaSihoJg1uh8PH07A/KQUHHC3V8Xf88dM/WE+mZ2J7/AnTTkel8iHGWkWrVfVsYeVszaLIqhoRVijxVixUuWD3AmNZ4qd7PSXSrno7UyeOAX5FnkQtKytbDFkCKFsUHT/gJpAohg4BHn5PwQkAykefsvxQkLhYgZzECYfzvtUGSNqXS5xTgH103e2zgOTDbkLOg7jjPLasPI5NOnBsr73lBXO3lK/qncjib1b6BCGEh4LrHD3H1DOeXHaMceL3nK/YC6fhw4fjlltuwcsvv4yKFSs6+jks9cYbb0RZgwKEKQc4qo5SxPn0WtJk1MCzcW7tSqddT1pGFuKPpxphdTApxQixA8dSXUVWYgqOpbpmxnbnaHK6aRv2H8t1HmqmahRVlsCKPPW3s4swqlxIvt1kzP3B1P0US7QwHUvLuR8cPcEg7/4N+qNGRA3/B1InJ3gnhpLjgazTH998wdQI5au4iSEn0eAsLCg0gvLx35EpB8youlyuur4vAVG17M2rY3TYg7XMg8g6EQ+4xSHkXF8GcHy/vXl1jKq6CapcRFaFqhJZQpQRggKDTMoBjqpzDxK3RtWNaD+iSPI55TsdAS1NdMs1atTICKe1a9eiYcOG2LFjB5o1a4aUFB/FZ/iJkp7H6URqBg4eSzXCivFVRmBRVB1LMcLKfCalGiF2poQFB56Ks8oWVHaBle0uzO4rFxqEbYnbTEHdX7f/ij3H9+RYV5XwKujXsJ9xxTGr9xnFLfFBzwf4acVQ9ifFUF7WlPwQEGR/gLsLH49iqIp/H/Qe8zjVsosmf+VxMla5hDxEVvbxN1a5DB9b5aq4CqrcxCjPUXF395ZglI5AFBae8jjFlo81oqmo8jjl2+IUFhZmNuDOpk2bUK1a4fsaiwsUR0w5sGTLQfy2cCl6d+3gl8zhFcKC0YCtau6Vq6mFaXWiiDICK9sluD+HezAVWaeRzakZWdiZkGyaJwKCjiE4ci3CK68BwnLGyYQEhuP8ahca61LP+p1RLiQ0941lpjuJIaeHsRFAB9zE0OFcXFQFhK4lPnxdHsC5iKFy0cUnfofiqHl/ZGxbgDULZ6J11z4I9nfmcP52IxyrcgfyFlkpR3MKKk8ii595uj5tdiHM5g08V16JrGpA8GmuTSFEkdGzXk+TcmDZ3mWYtWQWenXsVeSZw/MtnAYOHIjRo0dj8uTJZpqWA8Y2jRgxAldddRXKMhRJHRpE4/C/NvNZVOVWeE4qVwg1rXls7so5I5PuwTSHqDqYLazc47ASTzo90ALSEFxxPUKiViOowmYEBLhac2y2AGSeaIL0xPOQcqwp1tpSsOf3vzE1cDEahCejbthx1Aw+huqBiYi2JSIy8wjKpSUgNO2Ibw8Cg5ndXT0OMeTWF16p+Iih/BIYBFu9LtjzTxJa1etSvFxZPKaMYWJDcy9GHh71IJpzsSS6VXj3CC1jbIc25D0vrwFnQXU6t2pwmPfHQAhxxlAkMR72YOhB81mUoqlAwumVV17B1VdfjZiYGJw8eRLdunXD/v370bFjR7zwwgv+2UvhF4KDAk3wOFur08x3IjUNs7cvxq/bf8GK+AVIy3It/Eiqp4ajXVIoup9IR6PM9agasASVwtwC2Rka49l45RVZgaHILF8NgRVjEFSxeu6xMHzI8UFY0tMYlCV4rjgaj61aUy9yXSV5IbKyrZduhUo9QtHGFr8p73nDolxTOJwukD+knPfHQAhRIsi3cKIPcNasWVi8eLGJbzp+/DjatGljRtqJEkr6SdeHz4mDsB07iE1JW/HzsS34NT0eh9wsS6RGRgb6Hz+BS4+fQKN0p1iWfBhvTtpCEW+LQjyizOchW6T5+5Ctkr3f6btjKAckBwDxQERYsIm1OhXM7hyLBVSPPGlSRYQGl1BLkji9yGLyTraqjfMWWWnHc1zfuQbAp59+1KohNdHemIg0L0Irei+yQnN3vwshSrBwYrqB6667Dp07dzbNIi0tDV9//TUGD+ZInzJKcapUn3bC+4eF08i3/UFB+CWiPKZFVMCW0Oy4DyfDTcXMLPROTjZiqU1KqmeNFFLB84MhIsZYjJKCKuNgZhT2ZkRg78mg7Lgre4C7Peg9FQknTp+e4XhqBo4fysC2Q6d/0DEzu320oF1UuadmKOvZ28uEyAqraG9VGhX8/42xZLnl5/IwYjTn+o4BCWzb8p7X/f+NJzehQ2RFFJ1FtTjd54QoAvI9qi4oKAj79u0zrjpnDh8+bPoyM/MYqlxKR9X5vVK9P96cszkeEIBZFexiaXl4GGxuN+Rgmw1dU9IxICscF4ZVR5hzfJAnd5kP3pxTMzJNULt95KBzWoZTAe6MxUpOO/PrLa/s7bFR9unimr1dI5yKj6U2V5FFC5UvCS53+kzvzqIrLNJ3Isvf9zkhiug+59dRddRZnh4eu3fvNhstk+RWqZ4JCtmfW6X608VquNx4D3gfq+ElDPf+I7IKfo6siPnBWUj1cF9tHdUYl9btjT5NL0clf+dbciMsOAh1osubdjqOpaQbIXXQw6hBS2RRfKVn2nyavd3K2O78N4UW82MVZvZ2Fpleuj0BK+MDUGV7gl9GcopcYPxS5Xr2lhfpKdkpGvLIk8VpxlrlBe8FR3faW14Eh+ce6J6f2MCC3ueEKGV4LZzOO+88I5jYevTogeDgU4vSyrR9+3b07dsXZY48K9UD+OleYO9q+zBqd5Hkq7pkuY0Ocvq0VYjBX1nH8HP8aszcuwhHUnmDdo1dYiFdFtS9tMGlqBNZB8WdiuEhpjWOich1nqwsGxKS7aMHD7qMGnQVWSUte7tr7rAgfLZ5hV9yhwkfEBIOVKpjb3mRkZaHyHJ6weKowTzXlwIk7rK3vAgKzT0v1oLxp7nPBQAzHjfpMeS2E6Udr4XT5Zdfbj7XrFmDPn36ICLi1IMqNDQU9evXL5vpCHb8kUeleo7YSQQWvVrwbbjno8mtIGsu+Wh2Je3CtO3T8Mu/b2NH0o4c31cOq4y+DexFdc+pek6xdEedCYxfolBhO7tm7vOlZ2bh0LGSkb2doonZ6t0fY9xn9r97UxuJp5IK/w97m/Xd1/nPWFMxaY+95QubfRneDxt0zeeyQpRS4TRy5EjzSYHE4PDw8DMrbltq4M3JFxmQcxVDBcuAfDTlqCl58vO2n7H20FqPBRWZVIxiqVOtTghh3qMyTkhQIGpWKmeav7O3M/GoFRQPJOaZvd05Yzvdg+/O33q6d39jiWJCVrntSjm8N0TWsLe8yMxwql3op4z7BbofClGyyHeM05AhQ/yzJyUVihxv6DkKaNwju7BplfzVJfOS1MxU/L7rdyOWFu1ZhAy3ches79M+tr1xxfWq1wsRHJkjSnT2do/bBoz77sc1ezCwVU2Tr0sIc89h/jO2/NZ4jFsELJzgu/uhECWYfD+9Gc/02muvmczhzBjONATOJCR44XMvTdTrZB9Vklel+k73+cX3z6K6Kw+sxC/bfsFvcb/hWHpO91DjSo0xoNEA9GvQD7EVYn2+D6KQs7d7yfDJazFiyjrUjS6PRtUi0CgmAg2rVjCfjapGIKq8rIwiF3ivMnmnqgHVzwYaXAis/fI097lstswC6l6gOoGiVJNv4TRq1Ch88MEHePjhh/H000/jqaeeQlxcHH744Qc8++yzKJM3GG8q1ftYNG09uhXTtk0zgmnfCd7MXIkpF2OK6tIV1yy6mU+3LQo/e3tKeqZDRC3afAhvzPUi+aKJ27Jh66ETpmG9qxulakQoGlJQVatghFXD7M/alcvLxSfycZ9zYvHrduvUVR8A0Q11FEWpJN95nBo1aoQ33ngD/fv3R8WKFU2wuNX3559/4ssvv0RxpnDzOPm2Uv2h5EOYvn26EUz/Jvyb4/vyweVNQUSKJbrkirqej/BfCoIu4+YaF2Bu/3mZWf3CJlWxLXsUIF1+3hIaFIj6Vcu7iKmG2X9HhstKVabJ7T5Xvyvw93eAFR7AjOn9XwFaXVdkuypKF+nFKI9TvoVThQoV8O+//6Ju3bqoUaMGfvnlF1NyZdu2bSZlATdaJoUTycr0eaX65PRkzNk5x4ilP/f9aVxzzgQFBKFTzU5GLHWv2x3lmBhPlHqsUXXwbON0GVXHdAx7jp7E1kPHjeVpm/k8brKuM8g9PzAw3VlMWdaqWpXKKft6WSG3+9yeVcCU212zpJ9zrV1Ahfv4XivKHOnFSDjl21VXu3ZtkzmcwomWpt9++80Ip+XLlyMsrIxXDfdRpXoGdS/dt9SIJYqmkx4SX7as0hKXNroUfer3QdVyVX2w86IkQVFEcXQqj5OdWA95nJiOwUomepGb1zYpJd0IKGcxxc+4+GSkZea0UlFosf25LSHH6D8Gy5tYqmoVskWV3UrFYHpRBu5ztdoAdy4Afn3MHg9F/poM7FoKXPUhUOf8It1tIXxFvu9oV1xxBebMmYMOHTrgvvvuw0033YQPP/zQBIo/9NBDPtuxsgYNf3S/USzRHRd/Mj7HPLUiapkRcWwNoxQ/UNahOGLKgSVbDuK3hUvRu2uHfGcOp+utdZ1Kprm7A3cfSXYRU1sPnsC2+OMeE4XSFci8VZ5yVzGVQqOYCmhYNdtCxSD1ahGoERkuK1VpgzUBr3jXPoJ42kP2yghHdwAf9QG6Pwl0eUgJMkXZE04vvfSS42/mc6pXrx7++OMPNGnSBAMGDPD1/pV69h7fi1+3/4ppW6dha+LWHN9HhkYaqxJdca1jWiMwQEPLxSkokjo0iMbhf23m01d5m7ieelUqmHZxc9fvEpPTsTWeQuq4iaHiJ4XVjsPJpnyNOxwhyLZ4CxMwupawaWCN8nNYqewCq1yo4vNKNOdcDdRuB0z5D7B7OWDLBOaOAbbNB6583z7SWIiyIJzoY7zzzjvxzDPPoEGDBqbvggsuME14T1JaEmbFzTLWpRUHVuT4nskou9XuZsRS19pdEcoyCEIUE5jGoE3dyqa5Z17flZB8ykLlZK06kpzusYTN+n1JprnDmCkrlsrZ9cds6qUts32ppXJ94NbpwO/jgAXMAWUD4hYC73YCBr4FnHVpUe+hEP4XTgzImjJlihFOvuLtt9/G+PHjsX//frRq1Qpvvvkm2rdvn+v8R48eNSkQpk6danJG0eI1ceJEEzBW1GRmZRohtDZtLWIOxKB9zVMj29Iz07Fwz0IjlpikMi0rp7ujTUwbE7fUu15vRIWV0YLJokRnXrePvotAT7gmQkw4kWbiqNxF1Y6EZOMWdIfB7GwLN7u6rCuEBp2yTDnFUdFyVZjFlUU+Mptf/DTQ8CJg6h32siwnjwDfDALa3Qb0edFeLFmI0uyqY8065mzyRTzTN998g+HDh+O9994zMVMUQKyDt3HjRsTExOSYn8k2e/XqZb777rvvUKtWLezYsQOVKrnGZxQFs3fMxkvLXsKBZHuunG/nfIvq5avjumbXmb4ZcTOQmJpzxGH9yPqO5JS1K9Yugj0Xwv9EVwhFdIVotKsf7dLPsjTMiu5uoaL7LyklZ03AE2mZ+GtPomnO0AhFK5XdQnUqjQIFFmsCykpVxNTvAty1CPj5fuDfn+19Kz6y17a7+iN7kk0hSqtwYizT6NGjsXjxYrRt29akJ3Dm/vvv93pdr776KoYOHYpbb73VTFNAMb3BRx99hMcffzzH/OynlYkxVdZwRNbOKw6iafj84bC5ZdWhYHpj9Rs55o8Oj8YlDS7BgIYD0KJKC93URZklNDgQjWMiTHMfLHHYWKlOCSkTT3XouHEHuhupmFRl95GTpv2+6ZDLdxXDgtHQZEvPDkzP/qxXpTzCgmWlKjTKRwPXfg6s/ASY8QTA0cKHNgDvdwd6Pw+0H2pXwEIUc/Kdx8mKbfK4soAAk8/JG2g9Kl++vLEc0YrlXAuP7rgff/wxxzJ0x0VHR5vl+H21atVw4403YsSIEQgKCiqSPE50z/WZ0sdhacqNsMAwXFzvYiOWLqh5gYrqihKT36S4kZqRaQLRXYLT409g28HjOJaa00qVG4yjZ4oGY6FyBKnbrVVVKoTqhcaf19zBDfacTwf+PtXXtC9w2dv2wuZClKY8Ttu3b4cviI+PN3Xvqld3jYXg9IYNGzwuQ1E2d+5cDBo0yBzALVu24J577jEHdOTIkR6XSU1NNc354BAuw3amMKYpL9FExncdjy61utgnMu0xT0L4Aus69sX1XBLguNIG0eGmAacesnwHZKoEu2XKnjGdf2+LTzbxUu6viLRaUYCxzXXbRlQ5exHnhs6tWgXUqVzOWMnKOmd8zVVuBNwyA4FzxyBo+X/tfZtmwPZuJ2QOfAe2Bt18uLeiNJDu5/tcftZbojLTZWVlmfim999/31iY6Crcs2ePCS7PTTiNHTvW1Ndzh4k7abk6UxgI7g0Lly9E0tqco4eE8BWzZs3SwcyGUY/nsVFXVQXSMoFDKcDBlAAcPAkcOMnPABw4CaRl5XQPJZ7MwJpdiaY5EwgbqoazFqQNMeWA6vwMt6F6OaBC6Tf2+eGa64yYhhFos/N9hGUcQ8DxAwj68mpsiemHf2tcBVtgiXpEiRJ8n0tOTvZ63iK7KqtWrWrEz4EDrtYaTsfGxnpchiVeaKJzdsudddZZZkQeXX+hoTmH7T/xxBMmAN3Z4lSnTh307t3bJ646jp5jIHhe9OrYC+2qtzvj7Qnh6U2JNxMOnCgLrjpfQivVgWOpJpaKFqqt8cl2S9WhE9jrlJHdIgsBOJgtwHDE9bvK5UOMZcpYqqqVd1iqaKViMefShG+vuX7A8duQ9dMwBG6fjwDY0OTgL2gUtAeZl/8XiG7ko70WJZl0P9/nLG9UsRZOFDm0GDELuRXjRIsSp++9916Py3Tu3NkUEeZ8gYH2G9GmTZuMoPIkmgjLwHgqBcMD74uDz5QDHD13MPlgjuBwEoAA871zagIh/IGvrumyRp0qoahTpSLcnUPJaRl2MeUoSWOPp2Ifc1C5w1xVK3ceNc2ZkKAA1M2OpXIOTm9UNcLkxCrJ+Oyaq1wbuPl74M+3gdmjgKx0BO5bg8APLgb6TwBa3aDAceHX+1x+1lmkdlBaghgM3q5dO5O7iekITpw44RhlN3jwYJNygO42cvfdd+Ott97CAw88YMq9bN68GS+++GK+RvL5Goqhx9s/bkbVUSQ5iydOkxHtR0g0CVHCKB8ajLNrRpnmDIsm70tKsYspkzXdXoqGJWmYId2d9EybXXQdOgGsd7WwV40IdSmWbKVRqF25vM+ywJcY+DLc6T576oLvWCx4K5B+AvjhbmDLbODS14Bw5bcTRU+RCieWbDl06BCeffZZ425r3bo1ZsyY4QgYZ/07y7JE6GKbOXOmySF17rnnGlFFEcVRdUVJz3o98epFr7rkcSK0NFE08XshROmARZOZM4qta5NqLt8dT83AdoeQyrZSHbJbqVjPzx0Gs8cfT8Cy7a5Fk0ODAlG/avaIP4eosv/N+oKlmprn2YsFTx8BrPnC3vf3FHvpFlMsOPcEyUIUy3QEFDYRERHo0qWLI/P3pEmT0KJFC/N35cquZRiKG75OR+CemmDZ3mWYtWSWiWmSe04UBmUtHUFJhFYqjuyzJ/q0XH/2hJ8Hj50a9esNMRXDXMSUZa2ikKOoK1XXHAXTzywWnB2kHxAEXPQE0HW4igWXMdJLcjqCRx99FOPGjTN///XXX3j44YeNy23evHnm8+OPP0ZZhW47BoAfDD1oPhXTJIQgFDTMGcV2UTPXY5KUkm4ElLOY4mdcfDLSMnNaqSi02P7c5mqlCgsOtBdN9lCSpkJYCR2d1vIqoFY7YOpQYNdSe7Hgec9nFwv+LxClagui8ClQHidalwjr1l166aUmzmjVqlXFol6cEEKUJOh6a12nkmnOsIbf7iP2cjSnavzZBRZdfO7QFbhh/zHT3ImNDEejGI7ycxJVMRGoERmebysV92vp9gSsjA9Ale0J6Ng4xr/xWJXrAbf8CiwYDyx4GbBlATsWAe92Bga+CbQY6L9tC+EL4cTRa1a+g9mzZ5sAbsKM3vkZzieEECJ3KEbqValg2sXNXb9LTE7H1uw4Kit7Oj/j4k8gw0PRZAatsy3ectilv1xIkN1K5Tzaj8KqagTKheYcBTzj730Y9fN67DOpGoLw2eYVqBEVjpEDWqBvyxr+O51BwUD3J4CG3YApQ4Gk3UDKUWDyzUDbW4A+Y4HQM8/LJ4RfhBNjm+iSY2qAZcuWmUK9VlqA2rVlNhVCCH/DNAZt6lY2zZn0zCxTy8+yUDk+408g4UROKxXTKqzfl2SaO4yZci6WfPh4GibO2Zxjvv2JKbj7i1V496Y2/hVPpF4n4G4WC34AWJ9dlou176xiwbHn+Hf7QhREODEdAMucsMbcu+++a0a2kenTp6Nv3746qEIIUUSEBAVmj76LQE+4lrOicNrmwe23IyHZuN/cYTA728LN8afdJpeko46WqF4tYv2fRqFcZeCaT4HVn9tH3qUnA/GbgEkXA71GAx3uUs4nUbyEU926dTFt2rQc/a+99pqv9kkIIYSPia4QiugK0WhXP9qlPy0jCzsT3GOp7G7ApBTviiZTPNF998SUdbiiTW0Tr+XJ1eczAgKANoOBOhcAU24D9v8FZKYBMx4Hts4FLnsHiHBNFSFEkQknBoFzKOA559hNoj/++KMZSceA8eeeey7XDN5CCCGKHyxa3DgmwjRnmKnmsLFSncCUVbvxzfJdea5r8srdpgUHBuDsWlFoV6+yaW3rV0ZMRRZl9jHVmgL/mWPPNs6s42Tzb8C7nYAr3gMa9/D9NkWZJ98FlO68804Tz0S2bduG66+/3hTL/fbbb/HYY4+V+QMqhBClgYCAAFSNCEP7BtG4vLU9JMNbGKC+dtdRfLhoO+7+3yq0f2EOuo2fh+GT1+CrZTux+cAxk9vKJwSHAX1fBAZNASpkW5lOHAS+uBKY+RSQkTO2S4hCtThRNDHDN6FYuvDCC039uMWLFxsRxbIpQgghSg8UTxw9x0BwT3KHUU3VKobh0T7NsGrnEayIO4LNB4+7zLPjcLJpU1ftMdOVyoegbV27Ner8+tE4p1YUwkPOwL3XpCdw9x+nSrSQJW8BcQuBqz4CqjYu+LqFOBPhRPMti+xa6QiYx8kqhxIff/ogQiGEECUPBnwz5QBHz1EkOYsnKxR89GVnm1F117SrY6aPJqdh5Y4jWMEWl4C1uxNNPJXF0eR0zNlw0DSrzMw5te3uvbZ08dWPNnFZ+SIiBrjxW2Dpe8Dskfa4p31rgf9eCPR7GWg9SIHjovCFEwvyPv/88+jZsyd+//13M7LOSoxp1ZgTQghRuqAoYsqBU3mc7MTmksepUvlQ9DirumkkNSMTf+9JNNYoS0wdSU53zM8s6RRabBZMh2DipOpHm0/mnKIL8bSwvmnHe4D6ne3Fgg9vthcL/nFYdrHgiUA512SjQvhVONEVN2jQIPzwww946qmn0Lix3fzJ9ASdOnXK7+qEEEKUECiOmHJgyZaD+G3hUvTu2sHrzOFhwUFoWy/atDuzvRfML7Uy7giWxyUYwcRpZ+ylaE5g8ordZrpKhdBsa5RdTLWsGWWC2z1SoxVw5+/2kXarPrP3/fM9sHsFcNUHQN0LfHBERFkk38Lp3HPPNTXq3Bk/fjyCgvw4/FQIIUSRQ5HUoUE0Dv9rM58FzdtEy5E9uWYErj3f7t6LP57qsDpRTNFClZ55yjHIUX6/rT9gmlWfr1XtStlCqjLa1o02yUEdhFawl2Vp1AP4+X4gJRFI3AV8fAnQbQTQ9RF7VnIh8kGBrpijR48aC9PWrVtN0V+WW1m/fr1x1VkJMYUQQoj8wFF8fc6ONY2kpGea0Xl07Zl4qbgEl9xSrM+3LC7BNIum1SOMVet8iql60agTXQ4BZ18O1GoLTL0D2PmHvd7d/LHZxYInAZXswk0IvwindevWoUePHqhUqRLi4uIwdOhQI5ymTp2KnTt34rPPsk2iQgghxBnAUXYdGlYxjTCFwZZDx+1xUnEJRlAxeaczmw4cN41pD5A92o8iimKqXY/P0XLbBwhaMM4unnYuAd7rDAx4A6C4EsIfwol16m699Va8/PLLqFixoqO/X79+uPHGG/O7OiGEEMIrAgMD0LR6RdNu7FDX9B1MSskONmfQeQL+2ZvkUkLm0LFU/PrXftNIuZDWuDZmAoYfH4+o1H129923Q4AtNwOXjLO794TwpXBavnw5/vvf/+bop4tu/377hSmEEEIUBjGR4eh3Tg3TSHJaBtbQvZc9em/VjiM4nprhUtj40z2x+B6j8ULIhxgQ9Kf9i9Wf49jmRTg58H3ENG2vkyd8J5zCwsKQlJTkMTFmtWqqDSSEEKLoKB8ajE6NqppGaH3auP8YVu5IwPI4e6wUixcnoQLuS78PC7LOxXPBn6JCQCoqHt+O0P9dgteDb8bWRjejbf0qZhTfWTUi/V+8WJRe4TRw4ECMHj0akydPdoyMYGzTiBEjcNVVV/ljH4UQQogCQcHTomakaTd3rG/69h49aQ84j6OYisLAA80wMfhNnBMYh7CADDyQ+THm/7sKj6y7C/GIQkRYMM6rW8kEm3P0HosYVwjTaLyySr7P/CuvvIKrr74aMTExOHnyJLp162ZcdB07dsQLL7zgn70UQgghfETNSuUwkK1VTTN9LOUCrI27BCsWvIh2e78wfRcFrcX0wBF4JP1u/J7aCgs3x5vmEGM1Io01iuViKKaqR/qhiLEoHcIpKioKs2bNMrXp1q5di+PHj6NNmzYmk7gQQghR0qgYHoIuzWsBzd8GtlwJ2/d3IeDEQVQLSMKnoePwZeCleC75GqQhxOH++2tPommf/BFn+mpXLmdElJWgs2lMRRPMLkofBbY1du7c2TQhhBCi1NC4BwJYLPjHe4DNv5muG7Om4eracfj93LGYf7iyCTzfdPAYbE5F+3YfOYndR/bg+9X2IsaR4cFoQxGVXTKGiTrLhSpJdJkUTvfff78ps8JPZ9566y1s2bLFlGQRQgghSiwR1YAbJwNL/wvMesYUCw6N/xu9Fl6HXn1fAi4fjMSTGVi1054CgUKKI/mYkNOCiTrnbzxkGgkODEDLWvYixibLeb1ok2NKlAHhNGXKFPz00085+lmn7qWXXpJwEkIIUfJhMeEL7jpVLDh+I5CebC/dsnUOoga8ju7NY0wjaRlZ+GdvoqNcDD/jj6c5VpeRZTPiiu2DRdtNX/0q5R0FjCmmWH4mzyLGouQJp8OHD5s4J3ciIyMRH28PnBNCCCFKBbHnAHfMB2Y+Aaz8xN63/kdg90rgqklAPXtxexYbPq9uZdP+07WhKWK843CyQ0Txc+sh1yLGcYeTTftupb2IceXyISZGyioZQwsVs6eLEi6c6KabMWMG7r33Xpf+6dOno2HDhr7cNyGEEKLoCS0PDHjdXiz4p/uAlKNA0m7gk/7AhY8CFz6Wo1gwLUf1q1Yw7Zp29lp4CSfS7DX3diRgZdwRrNudiLTMU+69I8npmP3vQdPMZoMCcW7tKLTNrrtHURVdIbSQf7zwSckViqZDhw7h4osvNn1z5swxaQoU3ySEEKLU0mIgUKsNMPVOYMcie72738edKhZcud5pF6fo6dWiumlWEeO/9yRml4yx1947mpzumJ+iyny34wj+i22mr1G1Ck6j96KNu0/uvWIunG677TakpqaanE1jxowxffXr18e7776LwYMH+2MfhRBCiOJBVG1gyE/AoleBeWMBWyawaynwXldgwGtAS+8TQdMNZ2Kc6kcD3RqZIsbb4rOLGGeLKbrynKG7j+3r5bvMdNWIULuIyk7OeXbNKOM2FMUsHcHdd99tGq1O5cqVQ0REhO/3TAghhCiOBAbZXXQNugFTbgeO7gRSE4HvbgO2zLUXCw7L/3OReZ8ax1Q07fr2dR1Filc6WaRooWKguQUD0Gf+c8A0EhYciFZ1KpkYKYqpNnUrI6q8Pf+UKCLhtH37dmRkZKBJkyYutek2b96MkJAQY30SQgghSj112gN3LQKmDQf+/s7et+YLYOcS4OoPgZrnnfEmmLKgb8tY08jJtEys3X3UZfTesZRTRYyZEmHZ9gTTgK1mcCCTcTJOyhJTTNYp914hCqdbbrnFuOsonJxZunQpPvjgA8yfP/8MdkcIIYQoQYRHAVd9ADTuCfz6CJB2HEjYCnzQC+jxLNDxXpqSfLY5JtG8oGEV0wjde0zGSfeeJaaYjNOCSTo3Hjhm2pdLd5q+6pFhjmBzxkudVaMigoPk3vObcFq9erXHjOEXXHBBjpF2QgghRKmHZp3WN9gtUHTd7V0NZKXbk2dumwdc/h5Q0R4Q7mvo3mseG2naTRfYg9MPJKUYIWVZpNbvSzJlYiwOJKXil7/2mUbKhwaZwsVWTikWNGYZGuEj4UTz3rFjx3L0JyYmIjMzM7+rE0IIIUoHVRoBt/0GzHseWPy6vW/rXODdTsDl7wBN+xTKbrDgcP9za5hGTqRmmMSb9qDzBKzacQQn0k49r5PTMvHH1sOmEZbYoxBjsLklplgYWRRQOF144YUYO3YsvvrqKwQF2RNzUTCxr0uXLvldnRBCCFF6CA4Feo0GGnYHvr8LOL4fSI4HvrwW6HAX0HMUEBJeqLtUISwYnRtXNY1kZGZhw/5j2Tml7IHn+xJTHPPTOEUrFdtnS3aYvppR4dkjACsbN1+z2IoIKqNFjPMtnMaNG2fEU7NmzdC1a1fTt3DhQiQlJWHu3Ln+2EchhBCiZNGoO3D3YuDHe4FN0+19S98D4hYBV38EVGtWZLvGeCZmJWcb0sk+oGvP0ZP2kXvZqRA27E9yKWK8NzEFP63daxqpGBaM1nU5es9ukeLf5UMLNFC/xJHvX9miRQusW7fOFPVdu3atSUfA/E2Mb4qOjvbPXgohhBAljQpVgRu+ApZ/AMx8CshMBQ78Dfy3G9B3LND2Fnt8VDGgVqVyqNW6Fi5rXctMJ6WkY/XOo1gZl4Dl2UWMT6afcu8dS83Aws3xphFan86uGekIOKeYioksXMtaYVEgeVizZk28+OKLvt8bIYQQojRBYdR+qL2mHYsFH/oXyDgJTHvQFAvGgDeA8sXP6BAZHoJuTauZRtIzs/DvviQjolbusIsp5piyYPA5S8iwfbw4zvTViS6H8zl6z6RCiEbjahEmmL3MCacFCxac9nu68YQQQgjhRPWzgTvm2S1PKz609/37s71Y8JXvAw3soS/FlRBTN6+Sabd3aWCKGO9KOGlG7tG1RzG16cBxl2X4/a6EPZi6eo+ZjgwPdpSKoUWKiTq9KWJMUbZ0ewJWxgegyvYEdGwcU6TxVfkWThdddFGOPudEWhpZJ4QQQnggpBxw6atA4x7Aj8OAk0eAY3uBTwcAXR8GLnocCCoZaQACAgJQt0p5065qW9v0HU1Ow6qdDDa3tzW7jyIt41QR46SUDMzbeMg0EhIUYOKsKKLaZpeMqRoR5rKdGX/vw6if12cHrwfhs80rUCMqHCMHtEDflvZRg8VeOB05csRlOj093eR2euaZZ0z9OiGEEEKchub97VnFp94BxC1kmkpg4QRg++/2ZJqVS2YFjkrlQ3Fx8+qmkdQMFjFOMtYoK+g84USaY/70TJuJo2KbtHC76WtQtYIRUhRRTJMw+uf1PDou7E9Mwd1frMK7N7UpEvGUb+EUFRWVo69Xr14IDQ3F8OHDsXLlSl/tmxBCCFE6iawJDP4RWDwRmPuCvVjw7uX2YsH9XwXOvQYlnbDgIOOaY7vjQmYxt2F7/AlHCgR+bjt0wmUZfs/27crdua6XQop+LlqierWILXS3nc/GDlavXh0bN2701eqEEEKI0l8smC46q1jwkTggNQmY+h974Hi/8UBYRZQWAgIC0LBahGnXtqtj+g4ftxcxtsrF/LUn0Vii8oJz0H3HmnwdG9nLzxRb4cRUBM5QQe7btw8vvfQSWrdu7ct9E0IIIUo/tdsBdy6017pb9429b+1XwK6ldtddrbYorVSJCEPvs2NNIynpmWZk3mdL4jBtnb0kzOk4eOxU4s5iK5wojqgaKZjca9V99NFHvtw3IYQQomwQHmkfXdeoB/DL8OxiwduAD3sDFz8DdLrfp8WCiyvhIUFo3yDajKTzRjjFVAwv/sJp+3Z7AJdFYGAgqlWrhvDw0pnoSgghhCg0Wl0H1DkfmPIfYM9KICsDmD3SXvPuiv8CkUUzkqywad8g2oyeYyC4J8cdo5pio8LNfIVNvuVrvXr1XFqdOnWQklL4pjIhhBCiVBLdELhtJtBleLZEoNXid3ux4I3Z5VtKOUGBASblAHEP/bam+X1R5HMKLEitum++yfbBArj22mtNqZVatWqZEiwF4e2330b9+vWN1apDhw5YtmyZV8t9/fXXxm14+eWXF2i7QgghRLGE+Zx6jrSPvKuYbWU6mQB8dT3w66NA+kmUdvq2rGFSDtCy5AyniyoVQYGE03vvvWesTGTWrFmmzZgxA5dccgkeffTRfO8ARRjTGIwcORKrVq1Cq1at0KdPHxw8ePC0y8XFxeGRRx5xFBoWQgghSh0NuwF3/wE063+qb9n7wKSLgYP/orTTt2UNLBpxMb64rR0GN8k0n5wuKtFUIOG0f/9+h3CaNm2asTj17t0bjz32GJYvX57vHXj11VcxdOhQ3HrrraaAMIVZ+fLlTxtozuzkgwYNwqhRo9CwYcN8b1MIIYQoMbCW3fX/A/q/AgRnW18Orgfev8heQNhtsFZpIygwAB0aRKNtVZv5LMpyKwUKDq9cuTJ27dplxBMtTc8//7zp5yi7/JZbSUtLMwkzn3jiCZdg8549e2LJkiW5Ljd69GjExMTg9ttvx8KFzLqaO6mpqaZZJCUlOTKes/kaa53+WLcQuuZEcUD3uSKi9RCgZnsE/3AHAkyx4BTgl4eRtXk2Mvu/XiyLBZeUay4/6823cLryyitx4403okmTJjh8+LBx0RGWXWncuHG+1hUfH2/EFpNnOsPpDRs2eFxm0aJF+PDDD7FmzRqvtjF27FhjmXLnt99+M5Ytf0EXphCFia45UdjomisaAms+jLNtX6Nh/Gz79KbpSN3+J1bVuxPxFe0B1aWVWX56tiYnJ/tPOL322msmkJtWp5dffhkRERGmn0kw77nnHviTY8eO4eabb8akSZNQtWpVr5ahNYsxVM4WJ1rL6F6MjIz0i2rliWUZmpCQklGsUZRsdM0JXXNlkcuRsWkGgqbdj4CTCSiXfgSdtoxDVqcHkHXhiBJTLLi43Ocsb5RfhBN3mEHZ7jz00EP5XZURP0FBQThw4IBLP6djY+1ZRJ3ZunWrCQofMGCAoy8ry155OTg42JR8adSokcsyYWFhpnn6Hf4UNv5evxC65kRRo/tcEXP2AKBOO+D7O026ggDYEPTHRATtWGjPOM60BqWMED89W/OzziJNQ8rCwG3btsWcOXNchBCnO3bsmGP+5s2b46+//jJuOqsNHDgQ3bt3N39bQetCCCFEmYAJMW/+Aeg5CgjMtoUwceZ7FwJrT6UOEsWwyG9BoRttyJAhaNeuHdq3b4+JEyfixIkTZpQdGTx4sMkRxVgl5nlq2bKly/KVKlUyn+79QgghRJmApVi6PAg06Ap8x2LB24G0Y8D3d2QXC55gL+kiSodwuu6663Do0CE8++yzJtUBa+FxtJ4VML5z504z0k4IIYQQp4HFgO9iseBH7UWCCYsGm2LBH9qLCYuSL5zIvffea5on5s+ff9plP/nkEz/tlRBCCFHCCKsIXPHeqWLBqUnAkTjgoz5A9yeBzg8CgUFFvZclGp+ZclasWIEHH3zQV6sTQgghREE59xq79an2+fZpFgueMxr47DIgaa+Oa1EJp23btmHMmDEmaJs15v7+++8zWZ0QQgghfEXl+sCt04GuHAmfnW07bqG9WPCGX3ScC0s4Mekli/J26tTJJLycPHmyCeTesWMHZs+2J+MSQgghRDGA+Zx6PAPcMg2IrGXvO3kE+PpGYNrwMlEsuEhinJgi4Ntvv8Xnn39uElBlZGSYXErMFs6ivEIIIYQoxtTvAty1CPjpPmDDNHvfig+BHX8AV38IVD+7qPewdFmcWGLllltuQbVq1fDGG2/g/PPPx88//4yRI0fmWStOCCGEEMUA1rK77gvg0olAcDl7H2vevd8dWPp+qS8WXKjC6ccffzQpAj7++GPceeed+PPPP03NOOZV6tGjhxFSX3/9db6L/AohhBCiEAkIANrdCtwxH6ienf8wMxWY/ijw1Q3AicM6Hb4QTo8//rgRR84wszfFEsugXHzxxRg2bBgaNGjgzeqEEEIIUZTENAf+MwfocNepvk3T7YHjW+cV5Z6VDuFEl1z58uU9fscyJ+PGjTNFf5966ilf758QQggh/EFIOHDJOODGyUD5qva+4/uBz68AZj0LZKTpuPs6HcFXX31lyqMQCiu68YQQQghRgmjaB7h7MdCwe3aHDVj8OvBRb+Dw1iLeuVImnCiUDhw44Lu9EUIIIUThUzEWuGkq0GsMEBhi79u7GvjvhcCaLxU47ivhZFMEvhBCCFE6YF3YzvcD/5kFRDey96UdB364G5jyHyAlsaj3sFig6rlCCCGEOEXN84A7FwCtbzrV9/d3wHtdgV3Ly/yROiPhFMBhjUIIIYQoXYRFAJe/DVz9ERAWae87usNeLHjBeCCr7KYfkqtOCCGEEJ5peZU943idDvZpWyYw93ng04FA4u4yedTOSDhNnz4dtWpl174RQgghROmjcj3gll+BbiOAgGzZsGMR8G5n4N+fUdbIt3BKTk52/N2lSxeEhYU5pplZXAghhBCljKBgoPuTwC2/AJG17X0pR4FvbgJ+fhBIO6UNSjv5Fk6sV3f55Zfj008/RUJCgqN/7ty5SoAphBBClGbqdQLuXgS0uOxU38qPgfcvAvb/hbJAvoXT5s2bUalSJdx2222IjY1Fy5YtERkZiRtuuAGvvPKKf/ZSCCGEEMWDcpWBaz4FBrwBhGRXFYnfCEy6GPjzvVKf8ym4INnCv/nmG1x33XVo3769sUCxCPDMmTORlqb07EIIIUSpJyAAaDsEqNsRmHKb3dqUmQbMGAFsnQNc9g4QUQ2lkXwLpwkTJuD7779H3759HX2DBg3C2rVr0bt3bwwZMsTX+yiEEEKI4ki1pvZiwbNHAX++be/b/BvwXmfg8neBxj2Asu6qY206uujcadasGTIyMny1X0IIIYQoCQSHAX1fBAZ9B1TItjIdPwB8cSXw29OlrlhwvoXTVVddZeKZJk+ejJ07d2L//v1YuHChCRjv2rWrf/ZSCCGEEMWbJr2Au/8AGjlZmf54E/iwJxC/BWVWOL311ls4++yzjXhq0KCByePUvXt3EyA+adIk/+ylEEIIIYo/ETF2y1OfF08VC9631l4sePUXpSJwPN8xThUqVMB3332Hw4cPY8uWLSaPEwVUVFSUf/ZQCCGEECWrWHDHYUD9LsB3twOHNwPpJ4AfhwFb5gCXvgaUq4QyI5wsqlSpYpoQQgghRA5qtALu/B2YPgJY/bm975+pwO4VwFWTgLoXoMyVXBFCCCGEyJXQCsBlbwHXfAKEZ3umEncCH18CzB9XIosFSzgJIYQQwr+cfQVw12J73idiywLmvwh8cilwdFeJOvoSTkIIIYTwP5XqAEOmARc9eapY8M4/7Dmf/vmhxJwBCSchhBBCFF6x4ItGALdOB6Lq2PtSEoFvhwA/3QeknSj2Z0LCSQghhBCFS90LgLsW2V14Fqs+A/7bzZ6+oBgj4SSEEEKIwqdcJeDqj4HL3gZCKtj7mLrgg57AkreBrKxieVYknIQQQghRdMWCz7sJuHOBPX0BYbHgmU8CX14DHD9oRt4F7FiEWglLzGdRj8QrcB4nIYQQQgifULUxcPssYM5oYMlb9r4ts4E32wJBIQhOPox27NvxLhBZE+g7DmgxEEWBLE5CCCGEKB7Fgvu8ANw0FagQY+9LTQKSD7vOl7QPmDwYWP9TkeymhJMQQgghig+NewB3LrQLKY9k17ub8XiRuO0knIQQQghRvDi8GchIPc0MNiBpD7DjDxQ2Ek5CCCGEKF4cP+Db+XyIhJMQQgghihcR1X07nw+RcBJCCCFE8aJeJ/voOQTkMkMAEFnLPl8hI+EkhBBCiOJFYJA95YDBXTxlT/d9yT5fISPhJIQQQojiR4uBwLWfAZE1XPtpiWJ/EeVxUgJMIYQQQhRPWgwEmvdHxrYFWLNwJlp37YPghhcWiaXJQhYnIYQQQhRfAoNgq9cFe6I7ms+iFE1md4p060IIIYQQJQgJJyGEEEIIL5FwEkIIIYQoScLp7bffRv369REeHo4OHTpg2bJluc47adIkdO3aFZUrVzatZ8+ep51fCCGEEKLUCKdvvvkGw4cPx8iRI7Fq1Sq0atUKffr0wcGDBz3OP3/+fNxwww2YN28elixZgjp16qB3797Ys2dPoe+7EEIIIcoWRS6cXn31VQwdOhS33norWrRogffeew/ly5fHRx995HH+//3vf7jnnnvQunVrNG/eHB988AGysrIwZ86cQt93IYQQQpQtijSPU1paGlauXIknnnjC0RcYGGjcb7QmeUNycjLS09MRHR3t8fvU1FTTLJKSkswnl2HzNdY6/bFuIXTNieKA7nOitF1z+VlvkQqn+Ph4ZGZmonp11yJ9nN6wYYNX6xgxYgRq1qxpxJYnxo4di1GjRuXo/+2334xly1/MmjXLb+sWQtecKA7oPidKyzVHI0yZyBz+0ksv4euvvzZxTwws9wStWYyhcrY4WXFRkZGRflGtPLG9evVCSEiIz9cvhK45UdToPidK2zVneaOKvXCqWrUqgoKCcODAAZd+TsfGxp522QkTJhjhNHv2bJx77rm5zhcWFmaaOzzw/hQ2/l6/ELrmRFGj+5woLddcftZZpMHhoaGhaNu2rUtgtxXo3bFjx1yXe/nllzFmzBjMmDED7dq1K6S9FUIIIURZp8hddXSjDRkyxAig9u3bY+LEiThx4oQZZUcGDx6MWrVqmVglMm7cODz77LP48ssvTe6n/fv3m/6IiAjThBBCCCFKrXC67rrrcOjQISOGKIKYZoCWJCtgfOfOnWakncW7775rRuNdffXVLuthHqjnnnuu0PdfCCGEEGWHIhdO5N577zXNEwz8diYuLq6Q9koIIYQQopglwBRCCCGEKClIOAkhhBBCeImEkxBCCCGEl0g4CSGEEEJ4iYSTEEIIIYSXSDgJIYQQQniJhJMQQgghhJdIOAkhhBBCeImEkxBCCCGEl0g4CSGEEEJ4iYSTEEIIIYSXSDgJIYQQQniJhJMQQgghhJdIOAkhhBBCeImEkxBCCCGEl0g4CSGEEEJ4iYSTEEIIIYSXSDgJIYQQQniJhJMQQgghhJdIOAkhhBBCeImEkxBCCCGEl0g4CSGEEEJ4iYSTEEIIIYSXSDgJIYQQQniJhJMQQgghhJdIOAkhhBBCeImEkxBCCCGEl0g4CSGEEEJ4iYSTEEIIIYSXSDgJIYQQQniJhJMQQgghhJdIOAkhhBBCeImEkxBCCCGEl0g4CSGEEEJ4iYSTEEIIIYSXSDgJIYQQQniJhJMQQgghhJdIOAkhhBBCeImEkxBCCCGEl0g4CSGEEEJ4iYSTEEIIIYSXSDgJIYQQQniJhJMQQgghhJdIOAkhhBBCeImEkxBCCCFESRJOb7/9NurXr4/w8HB06NABy5YtO+383377LZo3b27mP+ecc/Drr78W2r4KIYQQouxS5MLpm2++wfDhwzFy5EisWrUKrVq1Qp8+fXDw4EGP8//xxx+44YYbcPvtt2P16tW4/PLLTfv7778Lfd+FEEIIUbYocuH06quvYujQobj11lvRokULvPfeeyhfvjw++ugjj/O//vrr6Nu3Lx599FGcddZZGDNmDNq0aYO33nqr0PddCCGEEGWLIhVOaWlpWLlyJXr27HlqhwIDzfSSJUs8LsN+5/kJLVS5zS+EEEII4SuCUYTEx8cjMzMT1atXd+nn9IYNGzwus3//fo/zs98TqampplkkJiaaz4SEBKSnp8PXcJ3Jyck4fPgwQkJCfL5+IXTNiaJG9zlR2q65Y8eOmU+bzVa8hVNhMHbsWIwaNSpHf4MGDYpkf4QQQghRPKGAioqKKr7CqWrVqggKCsKBAwdc+jkdGxvrcRn252f+J554wgSfW2RlZRlrU5UqVRAQEABfk5SUhDp16mDXrl2IjIz0+fqF0DUnihrd50Rpu+ZoaaJoqlmzZp7zFqlwCg0NRdu2bTFnzhwzMs4SNpy+9957PS7TsWNH8/2DDz7o6Js1a5bp90RYWJhpzlSqVAn+hidWwkkUJrrmRGGja06UpmsuL0tTsXHV0Ro0ZMgQtGvXDu3bt8fEiRNx4sQJM8qODB48GLVq1TIuN/LAAw+gW7dueOWVV9C/f398/fXXWLFiBd5///0i/iVCCCGEKO0UuXC67rrrcOjQITz77LMmwLt169aYMWOGIwB8586dZqSdRadOnfDll1/i6aefxpNPPokmTZrghx9+QMuWLYvwVwghhBCiLFDkwonQLZeba27+/Pk5+q655hrTiiN0CzKZp7t7UAhdc6K0oPucKMvXXIDNm7F3QgghhBCi6DOHCyGEEEKUFCSchBBCCCG8RMJJCCGEEMJLJJzy4O2330b9+vURHh6ODh06YNmyZbnOO2nSJHTt2hWVK1c2jTX13OdnSBlHENaoUQPlypUz82zevNnb8yXKCPm57pxheg4mdrXyolnouhO+vuaOHj2KYcOGmXsZA3abNm2KX3/91SfXsSgbvJ3P64Ppipo1a2aenUyG+dBDDyElJeWM1lkgGBwuPPP111/bQkNDbR999JHtn3/+sQ0dOtRWqVIl24EDBzzOf+ONN9refvtt2+rVq23//vuv7ZZbbrFFRUXZdu/e7ZjnpZdeMn0//PCDbe3atbaBAwfaGjRoYDt58qROgyjQdWexfft2W61atWxdu3a1XXbZZS7f6boTvrzmUlNTbe3atbP169fPtmjRInPtzZ8/37ZmzZoCr1OULb7O5/Xxv//9zxYWFmY+eb3NnDnTVqNGDdtDDz1U4HUWFAmn09C+fXvbsGHDHNOZmZm2mjVr2saOHevVwc3IyLBVrFjR9umnn5rprKwsW2xsrG38+PGOeY4ePWouhq+++qrgZ1HYyvp1x2utU6dOtg8++MA2ZMgQF+Gk6074+pp79913bQ0bNrSlpaX59DoWZYf2+bw+OO/FF1/s0jd8+HBb586dC7zOgiJXXS6kpaVh5cqVxpVmwUScnF6yZIlX1jxWcmZF5+joaDO9fft2k+TTeZ1M8U5zorfrFKWbgl53o0ePRkxMDG6//fYc3+m6E76+5n766SdT5oquOiYrZgLiF198EZmZmWd0HYuyQVoBrg8mv+Yylutt27ZtxjXcr1+/Aq+zRCfALI7Ex8ebm4CVwdyC0xs2bPBqHSNGjDAFA60TSdFkrcN9ndZ3omxTkOtu0aJF+PDDD7FmzRqP3+u6E76+5vjQmjt3LgYNGmQeXlu2bME999xjXhSZpNAX909ReokvwPVx4403muW6dOliYjYzMjJw1113mQoiBV1nQZHFyU+89NJLJlD3+++/N0FqQvgDVvO++eabzcCEqlWr6iCLQoHF2GnhZI1QFmpn6aynnnoK7733ns6A8AusIkKr5jvvvINVq1Zh6tSp+OWXXzBmzBgUNrI45QIfQkFBQThw4IBLP6djY2NPe1AnTJhghNPs2bNx7rnnOvqt5bgOjkRxXidr9AmR3+tu69atiIuLw4ABA1weauY/d3AwNm7cqOtO+Pxex/tXSEiIWc7irLPOMtZNukzO5P4pSj9VC3B9PPPMM+Yl8T//+Y+ZPuecc3DixAnccccdRrQX5jUni1MuhIaGmjepOXPmuDyQOE3ffm68/PLLRgGzUHG7du1cvmvQoIE5gc7rTEpKwtKlS0+7TlF2yO9117x5c/z111/GTWe1gQMHonv37uZvDtnVdSd8ec2Rzp07G/ecJdLJpk2bjKDi+gp6/xRlg9ACXB+MGWbMkjOWcKfrrlCvOZ+GmpcyOLSRI94++eQT2/r162133HGHGdq4f/9+8/3NN99se/zxx12GfHMo5HfffWfbt2+fox07dsxlHq7jxx9/tK1bt86MflI6AnEm15077qPqdN0JX9/rdu7caUYM33vvvbaNGzfapk2bZouJibE9//zzXq9TlG2+zuc1N3LkSHPNcQT6tm3bbL/99putUaNGtmuvvdbrdfoKCac8ePPNN21169Y1gohDHf/880/Hd926dTMPKYt69eqxYHKOxhPuPDT8mWeesVWvXt2c4B49epgbjxAFve68EU667oQv73Xkjz/+sHXo0MHcx5ia4IUXXjBpMbxdpxBv5uOaS09Ptz333HNGLIWHh9vq1Klju+eee2xHjhwp9GsugP/41oYlhBBCCFE6UYyTEEIIIYSXSDgJIYQQQniJhJMQQgghhJdIOAkhhBBCeImEkxBCCCGEl0g4CSGEEEJ4iYSTEEIIIYSXSDgJIYQQQniJhJMQQgghhJdIOAkh8s1zzz2H8PBwXHvttcjIyPB6uQ8//BC9e/d2TN9yyy24/PLLHdMXXXQRHnzwQZfCnldddRUiIyMREBCAo0ePFuhs7d+/H7169UKFChVQqVIlFMXxat26tU/XyULiXKdzoV0hhP+RcBJC5JtHHnkE06dPx08//YRvv/3Wq2VSUlLwzDPPYOTIkbnOM3XqVIwZM8Yx/emnn2LhwoX4448/sG/fPkRFRRXobL322mtm+TVr1mDTpk3wJxR4P/zwQ47j5Vy13Rf07dsXISEh+N///ufT9QohTo+EkxAi30RERKB79+64/vrr8fnnn3u1zHfffWcsR507d851nujoaFSsWNExvXXrVpx11llo2bIlYmNjjSgpCFxP27Zt0aRJE8TExHicJz09Hf48XlWqVPH5emmxe+ONN3y+XiFE7kg4CSEKzAUXXIBZs2bh0KFDec779ddfY8CAAaedx9lVx79feeUVLFiwwAgmTpPU1FRjwalVq5ZxvXXo0AHz58/PdZ3169fHlClT8Nlnn5n1UGwQ/v3uu+9i4MCBZj0vvPACMjMzcfvtt6NBgwYoV64cmjVrhtdffz3HOj/66COcffbZCAsLQ40aNXDvvfc6tkWuuOIKs35r2t1VR/fa6NGjUbt2bbMOfkfXm0VcXJxZnhY4CtTy5cujVatWWLJkict+8HiuWLHCCEMhROEg4SSEKDCffPKJiXGiKMqLRYsWoV27dl6vm6Jh6NCh6Nixo3GzcZpQpFBAcJvr1q3DNddcY9xWmzdv9rie5cuXm+8Zj8X1OAshChqKnL/++gu33XabETQUM3Q/rl+/Hs8++yyefPJJTJ482bEMxdawYcNwxx13mOXormzcuLFjW+Tjjz8227Km3eE+UBROmDDB/IY+ffoYAef+G5566ikjEulibNq0KW644QaXmLK6deuievXqxp0phCgkbEIIUQD++OMPW0BAgG3AgAG2Dh06nHbeI0eO2Hi7WbBggUv/kCFDbJdddpljulu3brYHHnjAMc2/2WexY8cOW1BQkG3Pnj0u6+nRo4ftiSeeyHX73Aa35Qz358EHH8zzdw4bNsx21VVXOaZr1qxpe+qpp3Kdn+v9/vvvXfpGjhxpa9Wqlcs6XnjhBZd5zj//fNs999xj/t6+fbtZzwcffOD4/p9//jF9//77r8ty5513nu25557L83cIIXxDcGEJNCFE6WLixIm49NJLMWrUKLRp0wZbtmxxWF7cOXnypPnkSLwzgRYeutNofXGG7ruCxBB5soC9/fbbxhW3c+dOs99paWkON9vBgwexd+9e9OjRo8C/ISkpyazDPdaL02vXrnXpO/fccx1/0yVo7UPz5s0d/XQpcvShEKJwkHASQuSbXbt2GdcZ45vOO+88E+/D0V25jZijqGHMzpEjR87oaB8/fhxBQUFYuXKl+XQPwM4vjG1yhu4/usboRqOLkIHq48ePx9KlSx0ipTDhqDkLKzDePf1AQkICqlWrVqj7JURZRjFOQoh889ZbbxlriBWwfdNNN512WHxoaChatGhh4obOBIo0WpxodaF1y7lx1N2ZsnjxYnTq1An33HOP2RbX6xx4TSHFgO/TpRag2OE+5gZHFtasWdNsy33bPEb5gSkeuH/cVyFE4SDhJITIF3QLTZo0CcOHD3f0DRo0yLjqli1blutyDIBmgPiZQBcdtzV48GBj8dq+fbvZ5tixY/HLL7/gTGG6Ao5Smzlzpsn3xLxT7gHeDCinRYppABjMvWrVKrz55puO7y1hxaSbuVnYHn30UYwbNw7ffPMNNm7ciMcff9wEgD/wwAP52t8///zTjMqjdUwIUThIOAkh8gWH9XN4PEepWdSpU8dYn7744otcl+Mw/19//RWJiYlndMQ5Yo3C6eGHHzbpAph5nOKGI8zOlDvvvBNXXnklrrvuOpPm4PDhw8b65MyQIUNMfNc777xjXJSM83IeDUdRRRcmj0lulqD777/fCE/+hnPOOcekIuDoPAq3/PDVV18ZIcnzIYQoHAIYIV5I2xJClHGYOoCB5E888URR70qJJz4+3ghHWsiYd0oIUTjI4iSEKDQYaF2QIG6REybJpNVLokmIwkUWJyGEEEIIL5HFSQghhBDCSySchBBCCCG8RMJJCCGEEMJLJJyEEEIIIbxEwkkIIYQQwksknIQQQgghvETCSQghhBDCSySchBBCCCG8RMJJCCGEEALe8X//OBR7c0T/YQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] ./Trial14\\seed_333\\best_by_val_norm\\alpha_lambda_eval\\alpha_lambda_curve_seed333_best_by_val_norm.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# USER CONFIG (Trial14\n",
    "# =========================\n",
    "TRIAL14_DIR = r\"./Trial14\"\n",
    "SEED = 333\n",
    "CKPT = \"best_by_val_norm\"   # or \"last_epoch\"\n",
    "\n",
    "LAM_STRS = [\"0.20\", \"0.40\", \"0.60\", \"0.80\"]\n",
    "LAM = [float(x) for x in LAM_STRS]\n",
    "SPLITS_ORDER = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# =========================\n",
    "# Paths\n",
    "# =========================\n",
    "AL_DIR = os.path.join(\n",
    "    TRIAL14_DIR, f\"seed_{SEED}\", CKPT, \"alpha_lambda_eval\"\n",
    ")\n",
    "SUMMARY_CSV = os.path.join(\n",
    "    AL_DIR, f\"alpha_lambda_summary_seed{SEED}_{CKPT}.csv\"\n",
    ")\n",
    "OUT_PNG = os.path.join(\n",
    "    AL_DIR, f\"alpha_lambda_curve_seed{SEED}_{CKPT}.png\"\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Load\n",
    "# =========================\n",
    "if not os.path.exists(SUMMARY_CSV):\n",
    "    raise FileNotFoundError(f\"Not found: {SUMMARY_CSV}\")\n",
    "\n",
    "df = pd.read_csv(SUMMARY_CSV)\n",
    "\n",
    "# =========================\n",
    "# Plot\n",
    "# =========================\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "for split in SPLITS_ORDER:\n",
    "    sub = df[df[\"split\"] == split]\n",
    "    if sub.empty:\n",
    "        print(f\"[SKIP] no data for split={split}\")\n",
    "        continue\n",
    "\n",
    "    row = sub.iloc[0]\n",
    "    rates = []\n",
    "    for ls in LAM_STRS:\n",
    "        v = row.get(f\"rate_{ls}\", np.nan)\n",
    "        rates.append(float(v) if np.isfinite(v) else np.nan)\n",
    "\n",
    "    plt.plot(\n",
    "        LAM,\n",
    "        rates,\n",
    "        marker=\"o\",\n",
    "        linewidth=2,\n",
    "        label=split\n",
    "    )\n",
    "\n",
    "plt.xticks(LAM, [f\"{x:.2f}\" for x in LAM])\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.xlabel(\"λ (life fraction)\")\n",
    "plt.ylabel(\"α–λ success rate\")\n",
    "plt.title(f\"α–λ Success Rate Curve\\nSeed {SEED} | {CKPT} | Trial14\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# =========================\n",
    "# Save\n",
    "# =========================\n",
    "os.makedirs(os.path.dirname(OUT_PNG), exist_ok=True)\n",
    "plt.savefig(OUT_PNG, dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"[SAVE] {OUT_PNG}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f6b4962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] DONE -> ./Trial14\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\\train\n",
      "[val] DONE -> ./Trial14\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\\val\n",
      "[test] DONE -> ./Trial14\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\\test\n",
      "\n",
      "ALL DONE.\n",
      "Saved under: ./Trial14\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================\n",
    "# USER CONFIG (Trial14)\n",
    "# ============================================================\n",
    "TRIAL_DIR = r\"./Trial14\"              # ✅ Trial14 폴더\n",
    "SEED = 333                            # seed 선택\n",
    "CKPT = \"best_by_val_norm\"             # \"best_by_val_norm\" or \"last_epoch\"\n",
    "\n",
    "SPLITS = [\"train\", \"val\", \"test\"]     # ✅ 여러 split 한 번에\n",
    "\n",
    "ALPHA = 0.20\n",
    "LAMBDA_TO_PLOT = 0.60                 # α–λ 그림에 표시할 λ\n",
    "\n",
    "MAX_FILES = None                      # None=모두, 아니면 예: 10\n",
    "\n",
    "# 저장 폴더 루트\n",
    "OUT_ROOT = os.path.join(TRIAL_DIR, f\"seed_{SEED}\", CKPT, \"paper_figures_bookstyle\")\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "def safe_name(s: str) -> str:\n",
    "    return s.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\")\n",
    "\n",
    "\n",
    "def load_cycle_seq_and_metrics(seed_dir: str, split: str):\n",
    "    \"\"\"\n",
    "    Trial14가 만들어 둔 파일들:\n",
    "      - <split>_cycle_sequence_mean.csv\n",
    "      - <split>_prognostics_metrics_per_file.csv\n",
    "    \"\"\"\n",
    "    seq_csv = os.path.join(seed_dir, f\"{split}_cycle_sequence_mean.csv\")\n",
    "    met_csv = os.path.join(seed_dir, f\"{split}_prognostics_metrics_per_file.csv\")\n",
    "\n",
    "    if not os.path.exists(seq_csv):\n",
    "        raise FileNotFoundError(f\"Missing: {seq_csv}\")\n",
    "    if not os.path.exists(met_csv):\n",
    "        raise FileNotFoundError(f\"Missing: {met_csv}\")\n",
    "\n",
    "    df_seq = pd.read_csv(seq_csv)\n",
    "    df_met = pd.read_csv(met_csv)\n",
    "    return df_seq, df_met\n",
    "\n",
    "\n",
    "def get_eval_segment(df_one_file: pd.DataFrame, t_s: int, t_e: int) -> pd.DataFrame:\n",
    "    df = df_one_file.sort_values(\"cycle\").copy()\n",
    "    df = df[(df[\"cycle\"] >= t_s) & (df[\"cycle\"] <= t_e)].copy()\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Plotters\n",
    "# ============================================================\n",
    "def plot_ph_alpha_absolute_band(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    alpha: float,\n",
    "    out_path: str,\n",
    "    ph_start: Optional[float] = None,\n",
    "    title_prefix: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    BOOK Fig.2.9(a) 스타일: PH용 α-zone은 '절대 폭(평행 밴드)'\n",
    "      alphaZone = alpha * EOL_true\n",
    "      zone = RUL_true ± alphaZone\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    x = df_eval[\"cycle\"].to_numpy()\n",
    "    y_true = df_eval[\"RUL_true\"].to_numpy()\n",
    "    y_pred = df_eval[\"RUL_pred\"].to_numpy()\n",
    "\n",
    "    last_cycle = int(df_eval[\"cycle\"].max())\n",
    "    eol_true = last_cycle + 1\n",
    "\n",
    "    alpha_zone = alpha * float(eol_true)  # ✅ book-style 핵심 (평행 밴드)\n",
    "    upper = y_true + alpha_zone\n",
    "    lower = y_true - alpha_zone\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y_true, \"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, \"r\", label=\"Prediction (cycles)\")\n",
    "    plt.plot(x, upper, \"b--\", label=f\"+{alpha:.2f} α-zone (±α·EOL)\")\n",
    "    plt.plot(x, lower, \"b--\", label=f\"-{alpha:.2f} α-zone (±α·EOL)\")\n",
    "\n",
    "    if ph_start is not None and np.isfinite(ph_start):\n",
    "        plt.axvline(int(ph_start), color=\"g\", linestyle=\"-.\", label=\"PH start\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title_prefix} | BOOK-STYLE α+PH (absolute band)\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_alpha_lambda_relative_band(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    alpha: float,\n",
    "    lambda_to_plot: float,\n",
    "    t_lambda: Optional[int],\n",
    "    out_path: str,\n",
    "    title_prefix: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    BOOK Fig.2.9(b) 스타일: α–λ는 '상대 폭(수렴 밴드)'\n",
    "      zone = RUL_true*(1±alpha), 그리고 t >= t_lambda 구간만 표시\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    x = df_eval[\"cycle\"].to_numpy()\n",
    "    y_true = df_eval[\"RUL_true\"].to_numpy()\n",
    "    y_pred = df_eval[\"RUL_pred\"].to_numpy()\n",
    "\n",
    "    upper = y_true * (1.0 + alpha)\n",
    "    lower = y_true * (1.0 - alpha)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y_true, \"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, \"r\", label=\"Prediction (cycles)\")\n",
    "\n",
    "    if t_lambda is not None and np.isfinite(t_lambda):\n",
    "        t_lambda = int(t_lambda)\n",
    "        plt.axvline(t_lambda, color=\"g\", linestyle=\":\", label=f\"t_λ (λ={lambda_to_plot:.2f})\")\n",
    "\n",
    "        mask = x >= t_lambda\n",
    "        if np.any(mask):\n",
    "            plt.plot(x[mask], upper[mask], \"b--\", label=f\"+{alpha:.2f} α–λ zone\")\n",
    "            plt.plot(x[mask], lower[mask], \"b--\", label=f\"-{alpha:.2f} α–λ zone\")\n",
    "        else:\n",
    "            plt.plot(x, upper, \"b--\", label=f\"+{alpha:.2f} α zone\")\n",
    "            plt.plot(x, lower, \"b--\", label=f\"-{alpha:.2f} α zone\")\n",
    "    else:\n",
    "        plt.plot(x, upper, \"b--\", label=f\"+{alpha:.2f} α zone\")\n",
    "        plt.plot(x, lower, \"b--\", label=f\"-{alpha:.2f} α zone\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title_prefix} | BOOK-STYLE α–λ (relative band)\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Main (multi-split)\n",
    "# ============================================================\n",
    "def run_for_one_split(seed_dir: str, split: str):\n",
    "    df_seq, df_met = load_cycle_seq_and_metrics(seed_dir, split)\n",
    "\n",
    "    files = df_seq[\"file\"].unique().tolist()\n",
    "    if MAX_FILES is not None:\n",
    "        files = files[:MAX_FILES]\n",
    "\n",
    "    out_dir = os.path.join(OUT_ROOT, split)  # ✅ split별 폴더\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    lam_key = f\"t_lambda_{LAMBDA_TO_PLOT:.2f}\"\n",
    "    title_prefix = f\"SEED {SEED} | {CKPT.upper()} | {split}\"\n",
    "\n",
    "    for f in files:\n",
    "        sub = df_seq[df_seq[\"file\"] == f].copy()\n",
    "        mrow = df_met[df_met[\"file\"] == f]\n",
    "        if mrow.empty:\n",
    "            continue\n",
    "        mrow = mrow.iloc[0].to_dict()\n",
    "\n",
    "        t_s = int(mrow[\"t_s\"])\n",
    "        t_e = int(mrow[\"t_e\"])\n",
    "        ph_start = mrow.get(\"t_PH_start\", np.nan)\n",
    "        t_lambda = mrow.get(lam_key, np.nan)\n",
    "\n",
    "        df_eval = get_eval_segment(sub, t_s, t_e)\n",
    "        if df_eval.empty:\n",
    "            continue\n",
    "\n",
    "        sname = safe_name(f)\n",
    "\n",
    "        out_a = os.path.join(out_dir, f\"FIG_A_BOOKSTYLE_alpha_PH__{sname}.png\")\n",
    "        plot_ph_alpha_absolute_band(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            alpha=ALPHA,\n",
    "            out_path=out_a,\n",
    "            ph_start=ph_start if np.isfinite(ph_start) else None,\n",
    "            title_prefix=title_prefix,\n",
    "        )\n",
    "\n",
    "        out_b = os.path.join(out_dir, f\"FIG_B_BOOKSTYLE_alpha_lambda__lam{LAMBDA_TO_PLOT:.2f}__{sname}.png\")\n",
    "        plot_alpha_lambda_relative_band(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            alpha=ALPHA,\n",
    "            lambda_to_plot=LAMBDA_TO_PLOT,\n",
    "            t_lambda=int(t_lambda) if np.isfinite(t_lambda) else None,\n",
    "            out_path=out_b,\n",
    "            title_prefix=title_prefix,\n",
    "        )\n",
    "\n",
    "    print(f\"[{split}] DONE -> {out_dir}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    seed_dir = os.path.join(TRIAL_DIR, f\"seed_{SEED}\", CKPT)\n",
    "\n",
    "    for split in SPLITS:\n",
    "        run_for_one_split(seed_dir, split)\n",
    "\n",
    "    print(\"\\nALL DONE.\")\n",
    "    print(\"Saved under:\", OUT_ROOT)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac033fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] DONE -> ./Trial14\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\\train\n",
      "[val] DONE -> ./Trial14\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\\val\n",
      "[test] DONE -> ./Trial14\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\\test\n",
      "\n",
      "ALL DONE.\n",
      "Saved under: ./Trial14\\seed_333\\best_by_val_norm\\paper_figures_bookstyle\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================\n",
    "# USER CONFIG (Trial14)\n",
    "# ============================================================\n",
    "TRIAL_DIR = r\"./Trial14\"             # ✅ Trial14 루트 폴더\n",
    "SEED = 333                           # seed 선택\n",
    "CKPT = \"best_by_val_norm\"            # \"best_by_val_norm\" or \"last_epoch\"\n",
    "\n",
    "SPLITS = [\"train\", \"val\", \"test\"]    # ✅ 여러 split 한 번에\n",
    "\n",
    "ALPHA = 0.20\n",
    "LAMBDA_TO_PLOT = 0.60                # α–λ 그림에 표시할 λ\n",
    "MAX_FILES = None                     # None=모두, 아니면 예: 10\n",
    "\n",
    "# ============================================================\n",
    "SEED_DIR = os.path.join(TRIAL_DIR, f\"seed_{SEED}\", CKPT)\n",
    "\n",
    "# 저장 폴더 루트 (원하면 이름 바꿔도 됨)\n",
    "OUT_ROOT = os.path.join(SEED_DIR, \"paper_figures_bookstyle\")\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "def safe_name(s: str) -> str:\n",
    "    return s.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\")\n",
    "\n",
    "\n",
    "def load_cycle_seq_and_metrics(seed_dir: str, split: str):\n",
    "    seq_csv = os.path.join(seed_dir, f\"{split}_cycle_sequence_mean.csv\")\n",
    "    met_csv = os.path.join(seed_dir, f\"{split}_prognostics_metrics_per_file.csv\")\n",
    "\n",
    "    if not os.path.exists(seq_csv):\n",
    "        raise FileNotFoundError(f\"Missing: {seq_csv}\")\n",
    "    if not os.path.exists(met_csv):\n",
    "        raise FileNotFoundError(f\"Missing: {met_csv}\")\n",
    "\n",
    "    df_seq = pd.read_csv(seq_csv)\n",
    "    df_met = pd.read_csv(met_csv)\n",
    "    return df_seq, df_met\n",
    "\n",
    "\n",
    "def get_eval_segment(df_one_file: pd.DataFrame, t_s: int, t_e: int) -> pd.DataFrame:\n",
    "    df = df_one_file.sort_values(\"cycle\").copy()\n",
    "    df = df[(df[\"cycle\"] >= t_s) & (df[\"cycle\"] <= t_e)].copy()\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Plotters\n",
    "# ============================================================\n",
    "def plot_ph_alpha_absolute_band(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    alpha: float,\n",
    "    out_path: str,\n",
    "    ph_start: Optional[float] = None,\n",
    "    title_prefix: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    BOOK Fig.2.9(a) 스타일: PH용 α-zone은 '절대 폭(평행 밴드)'\n",
    "      alphaZone = alpha * EOL_true\n",
    "      zone = RUL_true ± alphaZone\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    x = df_eval[\"cycle\"].to_numpy()\n",
    "    y_true = df_eval[\"RUL_true\"].to_numpy()\n",
    "    y_pred = df_eval[\"RUL_pred\"].to_numpy()\n",
    "\n",
    "    last_cycle = int(df_eval[\"cycle\"].max())\n",
    "    eol_true = last_cycle + 1\n",
    "\n",
    "    alpha_zone = alpha * float(eol_true)  # ✅ book-style 핵심 (평행 밴드)\n",
    "    upper = y_true + alpha_zone\n",
    "    lower = y_true - alpha_zone\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y_true, \"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, \"r\", label=\"Prediction (cycles)\")\n",
    "    plt.plot(x, upper, \"b--\", label=f\"+{alpha:.2f} α-zone (±α·EOL)\")\n",
    "    plt.plot(x, lower, \"b--\", label=f\"-{alpha:.2f} α-zone (±α·EOL)\")\n",
    "\n",
    "    if ph_start is not None and np.isfinite(ph_start):\n",
    "        plt.axvline(int(ph_start), color=\"g\", linestyle=\"-.\", label=\"PH start\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title_prefix} | BOOK-STYLE α+PH (absolute band)\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_alpha_lambda_relative_band(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    alpha: float,\n",
    "    lambda_to_plot: float,\n",
    "    t_lambda: Optional[int],\n",
    "    out_path: str,\n",
    "    title_prefix: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    BOOK Fig.2.9(b) 스타일: α–λ는 '상대 폭(수렴 밴드)'\n",
    "      zone = RUL_true*(1±alpha), 그리고 t >= t_lambda 구간만 표시\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    x = df_eval[\"cycle\"].to_numpy()\n",
    "    y_true = df_eval[\"RUL_true\"].to_numpy()\n",
    "    y_pred = df_eval[\"RUL_pred\"].to_numpy()\n",
    "\n",
    "    upper = y_true * (1.0 + alpha)\n",
    "    lower = y_true * (1.0 - alpha)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y_true, \"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, \"r\", label=\"Prediction (cycles)\")\n",
    "\n",
    "    if t_lambda is not None and np.isfinite(t_lambda):\n",
    "        t_lambda = int(t_lambda)\n",
    "        plt.axvline(t_lambda, color=\"g\", linestyle=\":\", label=f\"t_λ (λ={lambda_to_plot:.2f})\")\n",
    "\n",
    "        mask = x >= t_lambda\n",
    "        if np.any(mask):\n",
    "            plt.plot(x[mask], upper[mask], \"b--\", label=f\"+{alpha:.2f} α–λ zone\")\n",
    "            plt.plot(x[mask], lower[mask], \"b--\", label=f\"-{alpha:.2f} α–λ zone\")\n",
    "        else:\n",
    "            plt.plot(x, upper, \"b--\", label=f\"+{alpha:.2f} α zone\")\n",
    "            plt.plot(x, lower, \"b--\", label=f\"-{alpha:.2f} α zone\")\n",
    "    else:\n",
    "        plt.plot(x, upper, \"b--\", label=f\"+{alpha:.2f} α zone\")\n",
    "        plt.plot(x, lower, \"b--\", label=f\"-{alpha:.2f} α zone\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title_prefix} | BOOK-STYLE α–λ (relative band)\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Main (multi-split)\n",
    "# ============================================================\n",
    "def run_for_one_split(seed_dir: str, split: str):\n",
    "    df_seq, df_met = load_cycle_seq_and_metrics(seed_dir, split)\n",
    "\n",
    "    files = df_seq[\"file\"].unique().tolist()\n",
    "    if MAX_FILES is not None:\n",
    "        files = files[:MAX_FILES]\n",
    "\n",
    "    out_dir = os.path.join(OUT_ROOT, split)  # ✅ split별 폴더\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    lam_key = f\"t_lambda_{LAMBDA_TO_PLOT:.2f}\"\n",
    "    title_prefix = f\"SEED {SEED} | {CKPT.upper()} | {split} | Trial12\"\n",
    "\n",
    "    for f in files:\n",
    "        sub = df_seq[df_seq[\"file\"] == f].copy()\n",
    "        mrow = df_met[df_met[\"file\"] == f]\n",
    "        if mrow.empty:\n",
    "            continue\n",
    "        mrow = mrow.iloc[0].to_dict()\n",
    "\n",
    "        t_s = int(mrow[\"t_s\"])\n",
    "        t_e = int(mrow[\"t_e\"])\n",
    "        ph_start = mrow.get(\"t_PH_start\", np.nan)\n",
    "        t_lambda = mrow.get(lam_key, np.nan)\n",
    "\n",
    "        df_eval = get_eval_segment(sub, t_s, t_e)\n",
    "        if df_eval.empty:\n",
    "            continue\n",
    "\n",
    "        sname = safe_name(f)\n",
    "\n",
    "        out_a = os.path.join(out_dir, f\"FIG_A_BOOKSTYLE_alpha_PH__{sname}.png\")\n",
    "        plot_ph_alpha_absolute_band(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            alpha=ALPHA,\n",
    "            out_path=out_a,\n",
    "            ph_start=ph_start if np.isfinite(ph_start) else None,\n",
    "            title_prefix=title_prefix,\n",
    "        )\n",
    "\n",
    "        out_b = os.path.join(out_dir, f\"FIG_B_BOOKSTYLE_alpha_lambda__lam{LAMBDA_TO_PLOT:.2f}__{sname}.png\")\n",
    "        plot_alpha_lambda_relative_band(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            alpha=ALPHA,\n",
    "            lambda_to_plot=LAMBDA_TO_PLOT,\n",
    "            t_lambda=int(t_lambda) if np.isfinite(t_lambda) else None,\n",
    "            out_path=out_b,\n",
    "            title_prefix=title_prefix,\n",
    "        )\n",
    "\n",
    "    print(f\"[{split}] DONE -> {out_dir}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not os.path.isdir(SEED_DIR):\n",
    "        raise FileNotFoundError(f\"Not found: {SEED_DIR}\")\n",
    "\n",
    "    # split별 실행\n",
    "    for split in SPLITS:\n",
    "        run_for_one_split(SEED_DIR, split)\n",
    "\n",
    "    print(\"\\nALL DONE.\")\n",
    "    print(\"Saved under:\", OUT_ROOT)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88c6a437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "OUT_DIR: ./Trial14\\seed_333\\best_by_val_norm\\xai_phm_pack\\test\n",
      "[Saved] 00_phm_kpi_summary.csv\n",
      "windows prePH: 452 postPH: 8169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_31012\\2014071569.py:395: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_dim: 11\n",
      "features: ['min_vce', 'delta_1', 'delta_5', 'delta_20', 'delta_50', 'ema_10', 'ema_50', 'rollstd_10', 'win_mean', 'win_std', 'win_slope']\n",
      "[Saved] ./Trial14\\seed_333\\best_by_val_norm\\xai_phm_pack\\test\\01_feat_perm_prePH.csv\n",
      "[Saved] ./Trial14\\seed_333\\best_by_val_norm\\xai_phm_pack\\test\\01_feat_perm_prePH.png\n",
      "[Saved] ./Trial14\\seed_333\\best_by_val_norm\\xai_phm_pack\\test\\02_feat_perm_postPH.csv\n",
      "[Saved] ./Trial14\\seed_333\\best_by_val_norm\\xai_phm_pack\\test\\02_feat_perm_postPH.png\n",
      "[Saved] 03_feat_perm_delta_post_minus_pre.csv\n",
      "\n",
      "DONE. Check: ./Trial14\\seed_333\\best_by_val_norm\\xai_phm_pack\\test\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Trial14 PHM-XAI Pack (Permutation-based, PH-aware)\n",
    "# - PHM 관점에서 \"개선이 어디서/왜 발생했는지\" 진단\n",
    "# - PH-start 기준으로 pre-PH vs post-PH 그룹을 나눠\n",
    "#   feature permutation importance(ΔMAE/ΔRMSE)를 비교\n",
    "#\n",
    "# Outputs:\n",
    "#   ./Trial14/seed_<seed>/<ckpt>/xai_phm_pack/<split>/\n",
    "#     00_phm_kpi_summary.csv\n",
    "#     01_feat_perm_prePH.csv (+png)\n",
    "#     02_feat_perm_postPH.csv (+png)\n",
    "#     03_feat_perm_delta_post_minus_pre.csv\n",
    "#     04_used_windows_prePH.csv\n",
    "#     05_used_windows_postPH.csv\n",
    "#\n",
    "# Notes:\n",
    "# - \"late-step 의존\"은 Code#2에서 별도로 진단\n",
    "# - 여기서는 PH/Convergence 기반 \"PHM 구간\" 원인을 보는 XAI\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# USER CONFIG\n",
    "# ----------------------------\n",
    "DATA_DIR   = r\"C:\\Users\\11HOME_AHCI\\Desktop\\PoF\\Winter 2026\\Ref Data4\\100\"\n",
    "TRIAL_DIR  = r\"./Trial14\"\n",
    "SEED       = 333\n",
    "CKPT_TAG   = \"best_by_val_norm\"   # \"best_by_val_norm\" or \"last_epoch\"\n",
    "SPLIT      = \"test\"               # \"train\"/\"val\"/\"test\"\n",
    "\n",
    "SEQ_LEN      = 100\n",
    "STRIDE       = 5\n",
    "PRED_HORIZON = 0\n",
    "\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS  = 2\n",
    "DROPOUT     = 0.2\n",
    "\n",
    "# Trial14 pooling setting (반드시 학습 때와 동일)\n",
    "POOLING = \"mean_last_k\"     # \"last\" | \"mean_last_k\"\n",
    "POOL_LAST_K = 10            # 학습 때 사용한 값\n",
    "\n",
    "# Permutation config\n",
    "MAX_WINDOWS_PER_GROUP = 3000       # 너무 크면 느려짐\n",
    "BATCH_EVAL = 512\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "OUT_DIR = os.path.join(TRIAL_DIR, f\"seed_{SEED}\", CKPT_TAG, \"xai_phm_pack\", SPLIT)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "def load_scaler_from_csv(seed_dir: str) -> Tuple[StandardScaler, List[str]]:\n",
    "    path = os.path.join(seed_dir, \"scaler_x_mean_std.csv\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Missing scaler file: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    if not set([\"mean\", \"std\"]).issubset(df.columns):\n",
    "        raise ValueError(\"scaler_x_mean_std.csv must contain columns: feature, mean, std\")\n",
    "\n",
    "    feat_names = df[\"feature\"].astype(str).tolist()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.mean_ = df[\"mean\"].values.astype(np.float64)\n",
    "    std = df[\"std\"].values.astype(np.float64)\n",
    "    scaler.var_ = (std ** 2)\n",
    "    scaler.scale_ = std\n",
    "    scaler.n_features_in_ = len(df)\n",
    "    return scaler, feat_names\n",
    "\n",
    "\n",
    "def read_one_csv(csv_path: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    vce = df.iloc[:, 0].astype(np.float32).to_numpy()\n",
    "    rul = df.iloc[:, 1].astype(np.float32).to_numpy()\n",
    "    return vce, rul\n",
    "\n",
    "\n",
    "# ---- Trial13/14 feature builder (min_vce-only) ----\n",
    "def delta_k(v: np.ndarray, k: int) -> np.ndarray:\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    if k <= 0:\n",
    "        return out\n",
    "    out[k:] = v[k:] - v[:-k]\n",
    "    return out\n",
    "\n",
    "def ema(v: np.ndarray, span: int) -> np.ndarray:\n",
    "    if span <= 1:\n",
    "        return v.astype(np.float32).copy()\n",
    "    a = 2.0 / (float(span) + 1.0)\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    out[0] = v[0]\n",
    "    for i in range(1, len(v)):\n",
    "        out[i] = a * v[i] + (1.0 - a) * out[i - 1]\n",
    "    return out\n",
    "\n",
    "def rolling_std(v: np.ndarray, w: int) -> np.ndarray:\n",
    "    w = int(w)\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    for i in range(len(v)):\n",
    "        j0 = max(0, i - w + 1)\n",
    "        out[i] = float(np.std(v[j0:i + 1], ddof=0))\n",
    "    return out\n",
    "\n",
    "\n",
    "# IMPORTANT:\n",
    "# - Trial14가 Trial13 features를 그대로 쓰는 전제(너가 그렇게 진행했었음)\n",
    "# - feature list는 scaler 파일의 \"feature\" 컬럼 순서를 그대로 따른다.\n",
    "def build_all_features_from_min_vce(vce: np.ndarray, feat_names: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    scaler에 저장된 feature_names 순서대로 (T,F) 구성.\n",
    "    여기서는 Trial13 feature set:\n",
    "      min_vce, delta_*, ema_*, rollstd_*, win_mean, win_std, win_slope\n",
    "    \"\"\"\n",
    "    # base precompute\n",
    "    cache: Dict[str, np.ndarray] = {}\n",
    "    cache[\"min_vce\"] = vce.astype(np.float32)\n",
    "\n",
    "    # deltas\n",
    "    for name in feat_names:\n",
    "        if name.startswith(\"delta_\"):\n",
    "            k = int(name.split(\"_\")[1])\n",
    "            cache[name] = delta_k(vce, k)\n",
    "\n",
    "    # ema\n",
    "    for name in feat_names:\n",
    "        if name.startswith(\"ema_\"):\n",
    "            s = int(name.split(\"_\")[1])\n",
    "            cache[name] = ema(vce, s)\n",
    "\n",
    "    # rollstd\n",
    "    for name in feat_names:\n",
    "        if name.startswith(\"rollstd_\"):\n",
    "            w = int(name.split(\"_\")[1])\n",
    "            cache[name] = rolling_std(vce, w)\n",
    "\n",
    "    # window stats placeholder: handled per-window in Dataset\n",
    "    # -> we store none here\n",
    "\n",
    "    # stack base features that are time-varying\n",
    "    time_feats = []\n",
    "    for name in feat_names:\n",
    "        if name in [\"win_mean\", \"win_std\", \"win_slope\"]:\n",
    "            # placeholder; will be appended in Dataset __getitem__\n",
    "            continue\n",
    "        if name not in cache:\n",
    "            raise ValueError(f\"Unknown feature in scaler list: {name}\")\n",
    "        time_feats.append(cache[name])\n",
    "    Xbase = np.stack(time_feats, axis=1).astype(np.float32)  # (T, F_base)\n",
    "    return Xbase\n",
    "\n",
    "\n",
    "class WindowedDatasetTrial14(Dataset):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "      x(L,F), y_norm(1), file, start_idx, cycle_target, y_cycles, rul0\n",
    "    \"\"\"\n",
    "    def __init__(self, file_list: List[Path], feat_names: List[str],\n",
    "                 seq_len: int, stride: int, pred_horizon: int, scaler_x: StandardScaler):\n",
    "        self.file_list = file_list\n",
    "        self.feat_names = feat_names\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.scaler_x = scaler_x\n",
    "\n",
    "        self.series = []\n",
    "        for fp in self.file_list:\n",
    "            vce, rul = read_one_csv(fp)\n",
    "            rul0 = float(rul[0])\n",
    "            Xbase = build_all_features_from_min_vce(vce, feat_names)\n",
    "            # store raw vce for window stats\n",
    "            self.series.append((fp.name, Xbase, vce.astype(np.float32), rul.astype(np.float32), rul0))\n",
    "\n",
    "        self.index = []\n",
    "        for fi, (_name, Xbase, _vce, _rul, _rul0) in enumerate(self.series):\n",
    "            T = Xbase.shape[0]\n",
    "            last_start = T - (seq_len + pred_horizon)\n",
    "            if last_start < 0:\n",
    "                continue\n",
    "            for s in range(0, last_start + 1, stride):\n",
    "                self.index.append((fi, s))\n",
    "\n",
    "        if len(self.index) == 0:\n",
    "            raise ValueError(\"No windows created. Check seq_len/stride/pred_horizon vs file lengths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fi, s = self.index[idx]\n",
    "        name, Xbase, vce_raw, rul, rul0 = self.series[fi]\n",
    "\n",
    "        # Xbase currently excludes window stats columns, if present in feat_names\n",
    "        xw = Xbase[s:s + self.seq_len, :]  # (L, F_base)\n",
    "\n",
    "        # append window stats if they exist in feat_names\n",
    "        need_stats = any(f in [\"win_mean\", \"win_std\", \"win_slope\"] for f in self.feat_names)\n",
    "        if need_stats:\n",
    "            seg = vce_raw[s:s + self.seq_len]\n",
    "            wmean = float(np.mean(seg))\n",
    "            wstd = float(np.std(seg, ddof=0))\n",
    "            t = np.arange(self.seq_len, dtype=np.float32)\n",
    "            denom = float(np.var(t) + 1e-12)\n",
    "            slope = float(np.cov(t, seg, ddof=0)[0, 1] / denom) if denom > 0 else 0.0\n",
    "            stats = np.array([wmean, wstd, slope], dtype=np.float32).reshape(1, 3)\n",
    "            stats_rep = np.repeat(stats, repeats=self.seq_len, axis=0)\n",
    "\n",
    "            # ensure order matches feat_names: win_mean, win_std, win_slope at the END (Trial13 convention)\n",
    "            x = np.concatenate([xw, stats_rep], axis=1).astype(np.float32)\n",
    "        else:\n",
    "            x = xw.astype(np.float32)\n",
    "\n",
    "        y_idx = s + self.seq_len - 1 + self.pred_horizon\n",
    "        y_cycles = float(rul[y_idx])\n",
    "        y_norm = np.array([y_cycles / rul0], dtype=np.float32)\n",
    "        cycle_target = int(s + (self.seq_len - 1) + self.pred_horizon)\n",
    "\n",
    "        x = self.scaler_x.transform(x).astype(np.float32)\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x),\n",
    "            torch.from_numpy(y_norm),\n",
    "            name,\n",
    "            torch.tensor(s, dtype=torch.long),\n",
    "            torch.tensor(cycle_target, dtype=torch.long),\n",
    "            torch.tensor(y_cycles, dtype=torch.float32),\n",
    "            torch.tensor(rul0, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout,\n",
    "                 pooling=\"last\", pool_last_k=10):\n",
    "        super().__init__()\n",
    "        self.pooling = pooling\n",
    "        self.pool_last_k = pool_last_k\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # (B,T,H)\n",
    "        if self.pooling == \"last\":\n",
    "            h = out[:, -1, :]\n",
    "        elif self.pooling == \"mean_last_k\":\n",
    "            k = min(self.pool_last_k, out.size(1))\n",
    "            h = out[:, -k:, :].mean(dim=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling: {self.pooling}\")\n",
    "        return self.head(h)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_batch_mae_rmse_cycles(model, xb, y_cycles, rul0):\n",
    "    pred_norm = model(xb)\n",
    "    pred_cycles = pred_norm * rul0\n",
    "    err = pred_cycles - y_cycles\n",
    "    mae = torch.mean(torch.abs(err)).item()\n",
    "    rmse = torch.sqrt(torch.mean(err ** 2)).item()\n",
    "    return mae, rmse\n",
    "\n",
    "\n",
    "def plot_bar_importance(df_imp: pd.DataFrame, title: str, out_png: str):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(df_imp[\"feature\"].values, df_imp[\"delta_rmse\"].values)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"ΔRMSE (cycles) when permuted\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, axis=\"y\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load artifacts\n",
    "# ============================================================\n",
    "seed_dir = os.path.join(TRIAL_DIR, f\"seed_{SEED}\")\n",
    "ckpt_path = os.path.join(seed_dir, f\"{CKPT_TAG}.pt\")\n",
    "sub_dir = os.path.join(seed_dir, CKPT_TAG)\n",
    "\n",
    "win_csv = os.path.join(sub_dir, f\"{SPLIT}_predictions_windows.csv\")\n",
    "met_csv = os.path.join(sub_dir, f\"{SPLIT}_prognostics_metrics_per_file.csv\")\n",
    "\n",
    "for p in [ckpt_path, win_csv, met_csv]:\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Missing: {p}\")\n",
    "\n",
    "dfw = pd.read_csv(win_csv)\n",
    "dfm = pd.read_csv(met_csv)\n",
    "\n",
    "# KPI summary (PHM 관점 1차 확인)\n",
    "kpi_cols = [\"PH\", \"CRA\", \"Convergence_cycles\"] + \\\n",
    "           [c for c in dfm.columns if c.startswith(\"alpha_lambda_ok_\")]\n",
    "\n",
    "kpi = {}\n",
    "for c in kpi_cols:\n",
    "    if c in dfm.columns:\n",
    "        v = pd.to_numeric(dfm[c], errors=\"coerce\")\n",
    "        kpi[c + \"_mean\"] = float(v.mean())\n",
    "        kpi[c + \"_median\"] = float(v.median())\n",
    "        kpi[c + \"_nan_rate\"] = float(np.mean(~np.isfinite(v.values)))\n",
    "pd.DataFrame([kpi]).to_csv(os.path.join(OUT_DIR, \"00_phm_kpi_summary.csv\"), index=False)\n",
    "\n",
    "print(\"[Saved] 00_phm_kpi_summary.csv\")\n",
    "\n",
    "# map file -> t_PH_start\n",
    "dfm2 = dfm.copy()\n",
    "dfm2[\"t_PH_start\"] = pd.to_numeric(dfm2.get(\"t_PH_start\", np.nan), errors=\"coerce\")\n",
    "tph_map = dict(zip(dfm2[\"file\"].astype(str), dfm2[\"t_PH_start\"].values))\n",
    "\n",
    "dfw[\"t_PH_start\"] = dfw[\"file\"].map(tph_map)\n",
    "dfw[\"cycle\"] = pd.to_numeric(dfw[\"cycle\"], errors=\"coerce\")\n",
    "\n",
    "# groups\n",
    "df_pre = dfw[dfw[\"t_PH_start\"].notna() & (dfw[\"cycle\"] < dfw[\"t_PH_start\"])].copy()\n",
    "df_post = dfw[dfw[\"t_PH_start\"].notna() & (dfw[\"cycle\"] >= dfw[\"t_PH_start\"])].copy()\n",
    "\n",
    "print(\"windows prePH:\", len(df_pre), \"postPH:\", len(df_post))\n",
    "\n",
    "# sample cap\n",
    "rng = np.random.RandomState(RANDOM_SEED)\n",
    "def sample_df(df, n):\n",
    "    if len(df) > n:\n",
    "        return df.sample(n=n, random_state=RANDOM_SEED)\n",
    "    return df\n",
    "\n",
    "df_pre_s = sample_df(df_pre, MAX_WINDOWS_PER_GROUP)\n",
    "df_post_s = sample_df(df_post, MAX_WINDOWS_PER_GROUP)\n",
    "\n",
    "df_pre_s.to_csv(os.path.join(OUT_DIR, \"04_used_windows_prePH.csv\"), index=False)\n",
    "df_post_s.to_csv(os.path.join(OUT_DIR, \"05_used_windows_postPH.csv\"), index=False)\n",
    "\n",
    "# dataset lookup\n",
    "scaler_x, feat_names = load_scaler_from_csv(seed_dir)\n",
    "\n",
    "split_list_path = os.path.join(seed_dir, f\"{SPLIT}_files.csv\")\n",
    "names = pd.read_csv(split_list_path, header=None)[0].astype(str).tolist()\n",
    "file_list = [Path(DATA_DIR) / n for n in names]\n",
    "\n",
    "ds = WindowedDatasetTrial14(\n",
    "    file_list=file_list,\n",
    "    feat_names=feat_names,\n",
    "    seq_len=SEQ_LEN,\n",
    "    stride=STRIDE,\n",
    "    pred_horizon=PRED_HORIZON,\n",
    "    scaler_x=scaler_x\n",
    ")\n",
    "\n",
    "lookup: Dict[Tuple[str, int], int] = {}\n",
    "for idx in range(len(ds)):\n",
    "    _x, _y, name, s, cyc, yc, r0 = ds[idx]\n",
    "    lookup[(name, int(s.item()))] = idx\n",
    "\n",
    "# model\n",
    "model = LSTMRegressor(\n",
    "    input_size=len(feat_names),\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pooling=POOLING,\n",
    "    pool_last_k=POOL_LAST_K,\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "print(\"feature_dim:\", len(feat_names))\n",
    "print(\"features:\", feat_names)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Feature permutation importance (group-specific)\n",
    "# ============================================================\n",
    "def group_feature_permutation(df_group: pd.DataFrame, tag: str) -> pd.DataFrame:\n",
    "    df_group = df_group.copy()\n",
    "    # keep only windows existing in dataset\n",
    "    ok = []\n",
    "    for _, r in df_group.iterrows():\n",
    "        ok.append((str(r[\"file\"]), int(r[\"start_idx\"])) in lookup)\n",
    "    df_group = df_group[np.array(ok, dtype=bool)]\n",
    "    if df_group.empty:\n",
    "        raise ValueError(f\"{tag}: no valid windows after lookup match\")\n",
    "\n",
    "    # build tensor once\n",
    "    xs = []\n",
    "    ys = []\n",
    "    r0s = []\n",
    "    for _, r in df_group.iterrows():\n",
    "        ds_idx = lookup[(str(r[\"file\"]), int(r[\"start_idx\"]))]\n",
    "        x, _y, _name, _s, _cyc, y_cycles, rul0 = ds[ds_idx]\n",
    "        xs.append(x.numpy())\n",
    "        ys.append(float(y_cycles.item()))\n",
    "        r0s.append(float(rul0.item()))\n",
    "\n",
    "    X = torch.tensor(np.stack(xs, axis=0), dtype=torch.float32).to(device)  # (N,T,F)\n",
    "    y_cycles = torch.tensor(np.array(ys, dtype=np.float32).reshape(-1, 1)).to(device)\n",
    "    rul0 = torch.tensor(np.array(r0s, dtype=np.float32).reshape(-1, 1)).to(device)\n",
    "\n",
    "    # baseline performance\n",
    "    base_mae, base_rmse = eval_batch_mae_rmse_cycles(model, X, y_cycles, rul0)\n",
    "\n",
    "    rows = []\n",
    "    N, T, F = X.shape\n",
    "    for j in range(F):\n",
    "        Xp = X.clone()\n",
    "        # permute feature j across samples (keep time structure)\n",
    "        perm = torch.randperm(N, device=device)\n",
    "        Xp[:, :, j] = Xp[perm, :, j]\n",
    "\n",
    "        mae_p, rmse_p = eval_batch_mae_rmse_cycles(model, Xp, y_cycles, rul0)\n",
    "\n",
    "        rows.append({\n",
    "            \"feature\": feat_names[j],\n",
    "            \"base_mae\": base_mae,\n",
    "            \"base_rmse\": base_rmse,\n",
    "            \"perm_mae\": mae_p,\n",
    "            \"perm_rmse\": rmse_p,\n",
    "            \"delta_mae\": mae_p - base_mae,\n",
    "            \"delta_rmse\": rmse_p - base_rmse,\n",
    "            \"n_windows\": int(N),\n",
    "        })\n",
    "\n",
    "    df_imp = pd.DataFrame(rows).sort_values(\"delta_rmse\", ascending=False)\n",
    "    out_csv = os.path.join(OUT_DIR, f\"{tag}.csv\")\n",
    "    df_imp.to_csv(out_csv, index=False)\n",
    "\n",
    "    out_png = os.path.join(OUT_DIR, f\"{tag}.png\")\n",
    "    plot_bar_importance(df_imp, title=f\"{tag} (ΔRMSE)\", out_png=out_png)\n",
    "\n",
    "    print(\"[Saved]\", out_csv)\n",
    "    print(\"[Saved]\", out_png)\n",
    "    return df_imp\n",
    "\n",
    "\n",
    "imp_pre = group_feature_permutation(df_pre_s[[\"file\", \"start_idx\"]], \"01_feat_perm_prePH\")\n",
    "imp_post = group_feature_permutation(df_post_s[[\"file\", \"start_idx\"]], \"02_feat_perm_postPH\")\n",
    "\n",
    "# delta post - pre\n",
    "merge = imp_post[[\"feature\", \"delta_rmse\"]].merge(\n",
    "    imp_pre[[\"feature\", \"delta_rmse\"]],\n",
    "    on=\"feature\",\n",
    "    suffixes=(\"_post\", \"_pre\"),\n",
    ")\n",
    "merge[\"delta_post_minus_pre\"] = merge[\"delta_rmse_post\"] - merge[\"delta_rmse_pre\"]\n",
    "merge = merge.sort_values(\"delta_post_minus_pre\", ascending=False)\n",
    "merge.to_csv(os.path.join(OUT_DIR, \"03_feat_perm_delta_post_minus_pre.csv\"), index=False)\n",
    "print(\"[Saved] 03_feat_perm_delta_post_minus_pre.csv\")\n",
    "\n",
    "print(\"\\nDONE. Check:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1858c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "OUT_DIR: ./Trial14\\seed_333\\best_by_val_norm\\xai_late_step_pack\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_31012\\2663371481.py:266: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N windows used: 5000\n",
      "feature_dim: 11\n",
      "[Saved] 01_time_permutation_importance.csv/png\n",
      "[Saved] 02_tail_occlusion_curve.csv/png\n",
      "\n",
      "DONE. Check: ./Trial14\\seed_333\\best_by_val_norm\\xai_late_step_pack\\test\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Trial14 Late-step Dependence XAI Pack\n",
    "# - time permutation importance (t=0..L-1)\n",
    "# - tail occlusion curve: last-k를 점점 마스킹하며 성능 저하 측정\n",
    "#\n",
    "# Outputs:\n",
    "#   ./Trial14/seed_<seed>/<ckpt>/xai_late_step_pack/<split>/\n",
    "#     01_time_permutation_importance.csv (+png)\n",
    "#     02_tail_occlusion_curve.csv (+png)\n",
    "#\n",
    "# Interpretation:\n",
    "# - time_perm이 98~99에만 몰리면 late-step shortcut 여전\n",
    "# - occlusion curve가 \"k 조금만 마스킹해도 성능 폭락\"이면 late-step 의존 심함\n",
    "# - Trial14가 성공이면:\n",
    "#     (a) time_perm 분포가 last-k로 퍼지고\n",
    "#     (b) occlusion curve가 더 완만해짐\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# USER CONFIG\n",
    "# ----------------------------\n",
    "DATA_DIR   = r\"C:\\Users\\11HOME_AHCI\\Desktop\\PoF\\Winter 2026\\Ref Data4\\100\"\n",
    "TRIAL_DIR  = r\"./Trial14\"\n",
    "SEED       = 333\n",
    "CKPT_TAG   = \"best_by_val_norm\"\n",
    "SPLIT      = \"test\"\n",
    "\n",
    "SEQ_LEN      = 100\n",
    "STRIDE       = 5\n",
    "PRED_HORIZON = 0\n",
    "\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS  = 2\n",
    "DROPOUT     = 0.2\n",
    "\n",
    "# Trial14 pooling\n",
    "POOLING = \"mean_last_k\"\n",
    "POOL_LAST_K = 10\n",
    "\n",
    "# sampling\n",
    "MAX_WINDOWS = 5000\n",
    "BATCH = 512\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "# tail occlusion curve\n",
    "OCC_STEPS = [0, 1, 2, 5, 10, 15, 20, 30, 50, 80, 100]  # 마지막 k-step을 0으로 마스킹\n",
    "\n",
    "OUT_DIR = os.path.join(TRIAL_DIR, f\"seed_{SEED}\", CKPT_TAG, \"xai_late_step_pack\", SPLIT)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helpers (scaler + dataset + model)  -- Code#1과 동일 구조\n",
    "# ============================================================\n",
    "def load_scaler_from_csv(seed_dir: str) -> Tuple[StandardScaler, List[str]]:\n",
    "    path = os.path.join(seed_dir, \"scaler_x_mean_std.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "    feat_names = df[\"feature\"].astype(str).tolist()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.mean_ = df[\"mean\"].values.astype(np.float64)\n",
    "    std = df[\"std\"].values.astype(np.float64)\n",
    "    scaler.var_ = (std ** 2)\n",
    "    scaler.scale_ = std\n",
    "    scaler.n_features_in_ = len(df)\n",
    "    return scaler, feat_names\n",
    "\n",
    "def read_one_csv(csv_path: Path):\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    vce = df.iloc[:, 0].astype(np.float32).to_numpy()\n",
    "    rul = df.iloc[:, 1].astype(np.float32).to_numpy()\n",
    "    return vce, rul\n",
    "\n",
    "def delta_k(v, k):\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    out[k:] = v[k:] - v[:-k]\n",
    "    return out\n",
    "\n",
    "def ema(v, span):\n",
    "    if span <= 1:\n",
    "        return v.astype(np.float32).copy()\n",
    "    a = 2.0 / (float(span) + 1.0)\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    out[0] = v[0]\n",
    "    for i in range(1, len(v)):\n",
    "        out[i] = a * v[i] + (1.0 - a) * out[i - 1]\n",
    "    return out\n",
    "\n",
    "def rolling_std(v, w):\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    for i in range(len(v)):\n",
    "        j0 = max(0, i - w + 1)\n",
    "        out[i] = float(np.std(v[j0:i + 1], ddof=0))\n",
    "    return out\n",
    "\n",
    "def build_all_features_from_min_vce(vce, feat_names):\n",
    "    cache = {\"min_vce\": vce.astype(np.float32)}\n",
    "    for name in feat_names:\n",
    "        if name.startswith(\"delta_\"):\n",
    "            k = int(name.split(\"_\")[1]); cache[name] = delta_k(vce, k)\n",
    "        if name.startswith(\"ema_\"):\n",
    "            s = int(name.split(\"_\")[1]); cache[name] = ema(vce, s)\n",
    "        if name.startswith(\"rollstd_\"):\n",
    "            w = int(name.split(\"_\")[1]); cache[name] = rolling_std(vce, w)\n",
    "\n",
    "    time_feats = []\n",
    "    for name in feat_names:\n",
    "        if name in [\"win_mean\",\"win_std\",\"win_slope\"]:\n",
    "            continue\n",
    "        time_feats.append(cache[name])\n",
    "    return np.stack(time_feats, axis=1).astype(np.float32)\n",
    "\n",
    "class WindowedDataset(Dataset):\n",
    "    def __init__(self, file_list, feat_names, seq_len, stride, pred_horizon, scaler_x):\n",
    "        self.file_list = file_list\n",
    "        self.feat_names = feat_names\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.scaler_x = scaler_x\n",
    "\n",
    "        self.series = []\n",
    "        for fp in self.file_list:\n",
    "            vce, rul = read_one_csv(fp)\n",
    "            rul0 = float(rul[0])\n",
    "            Xbase = build_all_features_from_min_vce(vce, feat_names)\n",
    "            self.series.append((fp.name, Xbase, vce.astype(np.float32), rul.astype(np.float32), rul0))\n",
    "\n",
    "        self.index = []\n",
    "        for fi, (_n, Xb, _v, _r, _r0) in enumerate(self.series):\n",
    "            T = Xb.shape[0]\n",
    "            last_start = T - (seq_len + pred_horizon)\n",
    "            for s in range(0, last_start + 1, stride):\n",
    "                self.index.append((fi, s))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fi, s = self.index[idx]\n",
    "        name, Xbase, vce_raw, rul, rul0 = self.series[fi]\n",
    "        xw = Xbase[s:s+self.seq_len, :]\n",
    "\n",
    "        # window stats if present\n",
    "        need_stats = any(f in [\"win_mean\",\"win_std\",\"win_slope\"] for f in self.feat_names)\n",
    "        if need_stats:\n",
    "            seg = vce_raw[s:s+self.seq_len]\n",
    "            wmean = float(np.mean(seg))\n",
    "            wstd = float(np.std(seg, ddof=0))\n",
    "            t = np.arange(self.seq_len, dtype=np.float32)\n",
    "            denom = float(np.var(t) + 1e-12)\n",
    "            slope = float(np.cov(t, seg, ddof=0)[0, 1] / denom) if denom > 0 else 0.0\n",
    "            stats = np.array([wmean, wstd, slope], dtype=np.float32).reshape(1,3)\n",
    "            x = np.concatenate([xw, np.repeat(stats, self.seq_len, axis=0)], axis=1).astype(np.float32)\n",
    "        else:\n",
    "            x = xw.astype(np.float32)\n",
    "\n",
    "        y_idx = s + self.seq_len - 1 + self.pred_horizon\n",
    "        y_cycles = float(rul[y_idx])\n",
    "        y_norm = np.array([y_cycles / rul0], dtype=np.float32)\n",
    "\n",
    "        x = self.scaler_x.transform(x).astype(np.float32)\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x),\n",
    "            torch.from_numpy(y_norm),\n",
    "            name,\n",
    "            torch.tensor(s, dtype=torch.long),\n",
    "            torch.tensor(y_cycles, dtype=torch.float32),\n",
    "            torch.tensor(rul0, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, pooling=\"last\", pool_last_k=10):\n",
    "        super().__init__()\n",
    "        self.pooling = pooling\n",
    "        self.pool_last_k = pool_last_k\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            dropout=dropout if num_layers > 1 else 0.0,\n",
    "                            batch_first=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size//2, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        if self.pooling == \"last\":\n",
    "            h = out[:, -1, :]\n",
    "        elif self.pooling == \"mean_last_k\":\n",
    "            k = min(self.pool_last_k, out.size(1))\n",
    "            h = out[:, -k:, :].mean(dim=1)\n",
    "        else:\n",
    "            raise ValueError(self.pooling)\n",
    "        return self.head(h)\n",
    "\n",
    "@torch.no_grad()\n",
    "def batch_rmse_cycles(model, X, y_cycles, rul0):\n",
    "    pred_norm = model(X)\n",
    "    pred_cycles = pred_norm * rul0\n",
    "    err = pred_cycles - y_cycles\n",
    "    return torch.sqrt(torch.mean(err**2)).item()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load\n",
    "# ============================================================\n",
    "seed_dir = os.path.join(TRIAL_DIR, f\"seed_{SEED}\")\n",
    "ckpt_path = os.path.join(seed_dir, f\"{CKPT_TAG}.pt\")\n",
    "if not os.path.exists(ckpt_path):\n",
    "    raise FileNotFoundError(ckpt_path)\n",
    "\n",
    "scaler_x, feat_names = load_scaler_from_csv(seed_dir)\n",
    "\n",
    "split_list_path = os.path.join(seed_dir, f\"{SPLIT}_files.csv\")\n",
    "names = pd.read_csv(split_list_path, header=None)[0].astype(str).tolist()\n",
    "file_list = [Path(DATA_DIR) / n for n in names]\n",
    "\n",
    "ds = WindowedDataset(file_list, feat_names, SEQ_LEN, STRIDE, PRED_HORIZON, scaler_x)\n",
    "\n",
    "# sample windows\n",
    "rng = np.random.RandomState(RANDOM_SEED)\n",
    "idxs = np.arange(len(ds))\n",
    "if len(idxs) > MAX_WINDOWS:\n",
    "    idxs = rng.choice(idxs, size=MAX_WINDOWS, replace=False)\n",
    "idxs = idxs.tolist()\n",
    "\n",
    "# build tensors\n",
    "xs, ys, r0s = [], [], []\n",
    "for i in idxs:\n",
    "    x, _yn, _nm, _s, ycyc, r0 = ds[i]\n",
    "    xs.append(x.numpy())\n",
    "    ys.append(float(ycyc.item()))\n",
    "    r0s.append(float(r0.item()))\n",
    "\n",
    "X = torch.tensor(np.stack(xs, axis=0), dtype=torch.float32).to(device)      # (N,T,F)\n",
    "y_cycles = torch.tensor(np.array(ys, dtype=np.float32).reshape(-1,1)).to(device)\n",
    "rul0 = torch.tensor(np.array(r0s, dtype=np.float32).reshape(-1,1)).to(device)\n",
    "\n",
    "model = LSTMRegressor(\n",
    "    input_size=len(feat_names),\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pooling=POOLING,\n",
    "    pool_last_k=POOL_LAST_K,\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "print(\"N windows used:\", X.size(0))\n",
    "print(\"feature_dim:\", X.size(2))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# (1) Time permutation importance\n",
    "# ============================================================\n",
    "base = batch_rmse_cycles(model, X, y_cycles, rul0)\n",
    "\n",
    "rows = []\n",
    "N, T, F = X.shape\n",
    "for t in range(T):\n",
    "    Xp = X.clone()\n",
    "    perm = torch.randperm(N, device=device)\n",
    "    # permute entire feature vector at time t across samples\n",
    "    Xp[:, t, :] = Xp[perm, t, :]\n",
    "    rmse_p = batch_rmse_cycles(model, Xp, y_cycles, rul0)\n",
    "    rows.append({\"t_in_window\": t, \"base_rmse\": base, \"perm_rmse\": rmse_p, \"delta_rmse\": rmse_p - base})\n",
    "\n",
    "df_time = pd.DataFrame(rows)\n",
    "df_time.to_csv(os.path.join(OUT_DIR, \"01_time_permutation_importance.csv\"), index=False)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(df_time[\"t_in_window\"], df_time[\"delta_rmse\"])\n",
    "plt.xlabel(\"t in window\")\n",
    "plt.ylabel(\"ΔRMSE (cycles) when permuted\")\n",
    "plt.title(\"Time Permutation Importance (Late-step dependence check)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"01_time_permutation_importance.png\"), dpi=200)\n",
    "plt.close()\n",
    "\n",
    "print(\"[Saved] 01_time_permutation_importance.csv/png\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# (2) Tail occlusion curve (mask last k steps progressively)\n",
    "# ============================================================\n",
    "rows2 = []\n",
    "for k in OCC_STEPS:\n",
    "    k = int(k)\n",
    "    Xo = X.clone()\n",
    "    if k > 0:\n",
    "        kk = min(k, T)\n",
    "        Xo[:, -kk:, :] = 0.0\n",
    "    rmse_k = batch_rmse_cycles(model, Xo, y_cycles, rul0)\n",
    "    rows2.append({\"mask_last_k\": k, \"rmse_cycles\": rmse_k, \"delta_rmse_vs_no_mask\": rmse_k - base})\n",
    "\n",
    "df_occ = pd.DataFrame(rows2)\n",
    "df_occ.to_csv(os.path.join(OUT_DIR, \"02_tail_occlusion_curve.csv\"), index=False)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(df_occ[\"mask_last_k\"], df_occ[\"rmse_cycles\"], marker=\"o\")\n",
    "plt.xlabel(\"Mask last k timesteps\")\n",
    "plt.ylabel(\"RMSE (cycles)\")\n",
    "plt.title(\"Tail Occlusion Curve (Late-step dependence check)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"02_tail_occlusion_curve.png\"), dpi=200)\n",
    "plt.close()\n",
    "\n",
    "print(\"[Saved] 02_tail_occlusion_curve.csv/png\")\n",
    "\n",
    "print(\"\\nDONE. Check:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fca70c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "OUT_DIR: ./Trial14\\seed_333\\best_by_val_norm\\xai_diag_pack_trial14\\test\n",
      "WIN_CSV: ./Trial14\\seed_333\\best_by_val_norm\\test_predictions_windows.csv\n",
      "CKPT_PATH: ./Trial14\\seed_333\\best_by_val_norm.pt\n",
      "feature_dim: 11\n",
      "features: ['min_vce', 'delta_1', 'delta_5', 'delta_20', 'delta_50', 'ema_10', 'ema_50', 'rollstd_10', 'win_mean', 'win_std', 'win_slope']\n",
      "Dataset windows: 8621 Lookup size: 8621\n",
      "\n",
      "[CRA robust] summary: {'CRA_robust_mean_of_files': 0.7183305380790415, 'CRA_robust_median_of_files': 0.7816175686427795, 'CRA_robust_std_of_files': 0.16426266123471975, 'n_files': 10, 'denom_floor_cyc': 25.0, 'exclude_rul_below': 25.0}\n",
      "\n",
      "[Feature shift] saved: 01_feature_stats_high.csv, 01_feature_stats_low.csv, 01_feature_shift_report.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_31012\\282503606.py:666: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(CKPT_PATH, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Tail ablation] saved: 02_tail_ablation_metrics.csv (+ plot if enabled)\n",
      "\n",
      "[DONE] Trial14 XAI+Diagnosis pack saved to:\n",
      " -> ./Trial14\\seed_333\\best_by_val_norm\\xai_diag_pack_trial14\\test\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Trial14 XAI + Diagnosis Pack (FULL)\n",
    "# ------------------------------------------------------------\n",
    "# What this script does (one run = one seed/ckpt/split):\n",
    "#\n",
    "# (1) CRA \"explosion\" fix diagnostics\n",
    "#     - Recompute CRA/RA with robust denominator floor (cycles)\n",
    "#     - Optional: exclude near-EOL region from CRA\n",
    "#     - Saves: 00_CRA_robust_summary.csv, 00_CRA_robust_per_file.csv\n",
    "#\n",
    "# (2) High-error vs Low-error feature distribution check\n",
    "#     - Uses <split>_predictions_windows.csv to pick High/Low error windows\n",
    "#     - Rebuilds per-window raw features (UNSCALED) from min_vce only\n",
    "#     - Compares feature stats (mean/std/quantiles) overall + tail-only\n",
    "#     - Saves: 01_error_split_summary.csv, 01_feature_stats_high.csv, 01_feature_stats_low.csv\n",
    "#              01_feature_shift_report.csv  (+ optional plots)\n",
    "#\n",
    "# (3) Tail ablation suite (why tail matters)\n",
    "#     - Evaluates RMSE under 4 ablations:\n",
    "#         A) mask_tail: set last K steps to 0\n",
    "#         B) keep_tail: keep last K, zero-out earlier\n",
    "#         C) shuffle_tail: shuffle last K steps in time\n",
    "#         D) shuffle_all: shuffle all steps in time\n",
    "#     - Saves: 02_tail_ablation_metrics.csv, 02_tail_ablation_per_file.csv\n",
    "#\n",
    "# Requirements:\n",
    "#   pip install torch numpy pandas scikit-learn matplotlib\n",
    "#\n",
    "# Folder assumptions (matches your Trial9/13/14 style):\n",
    "#   ./Trial14/seed_<seed>/<ckpt>/\n",
    "#       <split>_predictions_windows.csv\n",
    "#       <split>_cycle_sequence_mean.csv   (optional; script can build from windows)\n",
    "#       scaler_x_mean_std.csv\n",
    "#       best_by_val_norm.pt (at seed root) or <ckpt>.pt depending on your training code\n",
    "#\n",
    "# NOTE:\n",
    "# - This pack is intentionally model-agnostic: it only needs (a) checkpoint, (b) scaler CSV,\n",
    "#   and (c) split file list to rebuild features.\n",
    "# - If your Trial14 uses a different feature set than Trial13, the script still works\n",
    "#   as long as scaler_x_mean_std.csv contains the correct feature names & order.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# USER CONFIG\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    TRIAL_DIR: str = r\"./Trial14\"\n",
    "    DATA_DIR: str = r\"C:\\Users\\11HOME_AHCI\\Desktop\\PoF\\Winter 2026\\Ref Data4\\100\"\n",
    "\n",
    "    SEED: int = 333\n",
    "    CKPT_TAG: str = \"best_by_val_norm\"   # \"best_by_val_norm\" or \"last_epoch\"\n",
    "    SPLIT: str = \"test\"                  # \"train\"/\"val\"/\"test\"\n",
    "\n",
    "    # windowing (must match training)\n",
    "    SEQ_LEN: int = 100\n",
    "    STRIDE: int = 5\n",
    "    PRED_HORIZON: int = 0\n",
    "\n",
    "    # model (must match training)\n",
    "    HIDDEN_SIZE: int = 512\n",
    "    NUM_LAYERS: int = 2\n",
    "    DROPOUT: float = 0.2\n",
    "\n",
    "    # --------------------------\n",
    "    # (1) CRA robust settings\n",
    "    # --------------------------\n",
    "    # denom floor in cycles for rel_err = |e| / max(|RUL_true|, floor)\n",
    "    CRA_DENOM_FLOOR_CYC: float = 25.0\n",
    "\n",
    "    # optionally exclude near-EOL region from CRA (where RUL_true is very small)\n",
    "    CRA_EXCLUDE_RUL_BELOW: Optional[float] = 25.0  # None to disable\n",
    "\n",
    "    # --------------------------\n",
    "    # (2) High/Low error split settings\n",
    "    # --------------------------\n",
    "    ERROR_MODE: str = \"quantile\"         # \"quantile\" or \"fixed\"\n",
    "    ERROR_Q_HIGH: float = 0.90           # top 10% = high error\n",
    "    ERROR_Q_LOW: float = 0.10            # bottom 10% = low error\n",
    "    ERROR_FIXED_THR: float = 300.0       # if ERROR_MODE=\"fixed\", abs_err >= thr\n",
    "\n",
    "    MAX_WINDOWS_PER_GROUP: int = 3000    # cap for speed\n",
    "    TAIL_K_FOR_STATS: int = 10           # compute tail-only stats on last K steps\n",
    "\n",
    "    # --------------------------\n",
    "    # (3) Tail ablation settings\n",
    "    # --------------------------\n",
    "    TAIL_K: int = 10                     # last K steps = \"tail\"\n",
    "    ABLATION_MAX_WINDOWS: int = 12000    # cap windows for evaluation speed\n",
    "    BATCH_SIZE: int = 512\n",
    "\n",
    "    # output\n",
    "    OUT_SUBDIR: str = \"xai_diag_pack_trial14\"\n",
    "    SAVE_PLOTS: bool = True\n",
    "\n",
    "\n",
    "cfg = Cfg()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Paths\n",
    "# ============================================================\n",
    "seed_dir = os.path.join(cfg.TRIAL_DIR, f\"seed_{cfg.SEED}\")\n",
    "sub_dir = os.path.join(seed_dir, cfg.CKPT_TAG)\n",
    "\n",
    "OUT_DIR = os.path.join(sub_dir, cfg.OUT_SUBDIR, cfg.SPLIT)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "WIN_CSV = os.path.join(sub_dir, f\"{cfg.SPLIT}_predictions_windows.csv\")\n",
    "SEQ_CSV = os.path.join(sub_dir, f\"{cfg.SPLIT}_cycle_sequence_mean.csv\")  # optional\n",
    "SPLIT_LIST = os.path.join(seed_dir, f\"{cfg.SPLIT}_files.csv\")\n",
    "SCALER_CSV = os.path.join(seed_dir, \"scaler_x_mean_std.csv\")\n",
    "\n",
    "# checkpoint naming: try both patterns\n",
    "CKPT_CANDIDATES = [\n",
    "    os.path.join(seed_dir, f\"{cfg.CKPT_TAG}.pt\"),          # common pattern used by your XAI packs\n",
    "    os.path.join(seed_dir, f\"{cfg.CKPT_TAG}.pth\"),\n",
    "    os.path.join(seed_dir, f\"{cfg.CKPT_TAG}.ckpt\"),\n",
    "    os.path.join(seed_dir, f\"{cfg.CKPT_TAG}.pt\"),          # (same)\n",
    "    os.path.join(seed_dir, f\"{cfg.CKPT_TAG}.pt\"),\n",
    "]\n",
    "# also try \"best_by_val_norm.pt\" / \"last_epoch.pt\" at seed root (Trial9/13 style)\n",
    "CKPT_CANDIDATES += [\n",
    "    os.path.join(seed_dir, f\"{cfg.CKPT_TAG}.pt\"),          # e.g. best_by_val_norm.pt\n",
    "    os.path.join(seed_dir, f\"{cfg.CKPT_TAG}.pt\"),\n",
    "    os.path.join(seed_dir, f\"{cfg.CKPT_TAG}.pt\"),\n",
    "]\n",
    "# and if user stored it like: seed_dir/<ckpt>.pt (already)\n",
    "# We'll resolve by existence check.\n",
    "\n",
    "\n",
    "def _require(path: str, msg: str) -> None:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"{msg}\\nMissing: {path}\")\n",
    "\n",
    "\n",
    "_require(WIN_CSV, \"Need windows prediction CSV.\")\n",
    "_require(SPLIT_LIST, \"Need split file list CSV.\")\n",
    "_require(SCALER_CSV, \"Need scaler_x_mean_std.csv.\")\n",
    "\n",
    "# resolve checkpoint\n",
    "def resolve_ckpt_path(seed_dir_: str, ckpt_tag: str) -> str:\n",
    "    candidates = [\n",
    "        os.path.join(seed_dir_, f\"{ckpt_tag}.pt\"),\n",
    "        os.path.join(seed_dir_, f\"{ckpt_tag}.pth\"),\n",
    "        os.path.join(seed_dir_, f\"{ckpt_tag}.ckpt\"),\n",
    "        os.path.join(seed_dir_, f\"{ckpt_tag}.bin\"),\n",
    "        os.path.join(seed_dir_, f\"{ckpt_tag}.pt\"),\n",
    "    ]\n",
    "    # common names from your training scripts:\n",
    "    if ckpt_tag == \"best_by_val_norm\":\n",
    "        candidates += [os.path.join(seed_dir_, \"best_by_val_norm.pt\")]\n",
    "    if ckpt_tag == \"last_epoch\":\n",
    "        candidates += [os.path.join(seed_dir_, \"last_epoch.pt\")]\n",
    "\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    raise FileNotFoundError(f\"Could not find checkpoint for CKPT_TAG={ckpt_tag} under {seed_dir_}\")\n",
    "\n",
    "CKPT_PATH = resolve_ckpt_path(seed_dir, cfg.CKPT_TAG)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "print(\"WIN_CSV:\", WIN_CSV)\n",
    "print(\"CKPT_PATH:\", CKPT_PATH)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# I/O helpers\n",
    "# ============================================================\n",
    "def read_one_csv(csv_path: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(f\"{csv_path.name}: expected >=2 cols\")\n",
    "    vce = df.iloc[:, 0].astype(np.float32).to_numpy()\n",
    "    rul = df.iloc[:, 1].astype(np.float32).to_numpy()\n",
    "    if len(vce) != len(rul):\n",
    "        raise ValueError(f\"{csv_path.name}: length mismatch\")\n",
    "    return vce, rul\n",
    "\n",
    "\n",
    "def windows_to_cycle_sequence_mean(dfw: pd.DataFrame) -> pd.DataFrame:\n",
    "    # dfw columns must include: file, cycle, RUL_true, RUL_pred, rul0\n",
    "    g = dfw.groupby([\"file\", \"cycle\"], as_index=False).agg(\n",
    "        rul0=(\"rul0\", \"first\"),\n",
    "        RUL_true=(\"RUL_true\", \"mean\"),\n",
    "        RUL_pred=(\"RUL_pred\", \"mean\"),\n",
    "        n_windows=(\"RUL_pred\", \"count\"),\n",
    "    )\n",
    "    return g\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Scaler loader (keeps feature order!)\n",
    "# ============================================================\n",
    "def load_scaler_and_features(scaler_csv: str) -> Tuple[StandardScaler, List[str]]:\n",
    "    df = pd.read_csv(scaler_csv)\n",
    "    if not {\"feature\", \"mean\", \"std\"}.issubset(df.columns):\n",
    "        raise ValueError(f\"scaler_x_mean_std.csv must have columns feature/mean/std. Got: {df.columns.tolist()}\")\n",
    "    feat_names = df[\"feature\"].astype(str).tolist()\n",
    "    mean = df[\"mean\"].values.astype(np.float64)\n",
    "    std = df[\"std\"].values.astype(np.float64)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.mean_ = mean\n",
    "    scaler.var_ = (std ** 2)\n",
    "    scaler.scale_ = std\n",
    "    scaler.n_features_in_ = len(feat_names)\n",
    "    return scaler, feat_names\n",
    "\n",
    "\n",
    "scaler_x, FEATURE_NAMES = load_scaler_and_features(SCALER_CSV)\n",
    "F_DIM = len(FEATURE_NAMES)\n",
    "print(\"feature_dim:\", F_DIM)\n",
    "print(\"features:\", FEATURE_NAMES)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Feature builders (min_vce-only family)\n",
    "# - Works for Trial13 features, and any Trial14 variant as long\n",
    "#   as the scaler feature list matches what we generate.\n",
    "# - If your Trial14 feature list differs, update build_feature_matrix()\n",
    "#   to match the feature names in scaler CSV.\n",
    "# ============================================================\n",
    "def delta_k(v: np.ndarray, k: int) -> np.ndarray:\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    if k <= 0:\n",
    "        return out\n",
    "    out[k:] = v[k:] - v[:-k]\n",
    "    return out\n",
    "\n",
    "\n",
    "def ema(v: np.ndarray, span: int) -> np.ndarray:\n",
    "    if span <= 1:\n",
    "        return v.astype(np.float32).copy()\n",
    "    a = 2.0 / (float(span) + 1.0)\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    out[0] = v[0]\n",
    "    for i in range(1, len(v)):\n",
    "        out[i] = a * v[i] + (1.0 - a) * out[i - 1]\n",
    "    return out\n",
    "\n",
    "\n",
    "def rolling_std(v: np.ndarray, w: int) -> np.ndarray:\n",
    "    w = int(w)\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    for i in range(len(v)):\n",
    "        j0 = max(0, i - w + 1)\n",
    "        out[i] = float(np.std(v[j0:i + 1], ddof=0))\n",
    "    return out\n",
    "\n",
    "\n",
    "def window_stats(seg: np.ndarray) -> Tuple[float, float, float]:\n",
    "    # mean, std, slope\n",
    "    wmean = float(np.mean(seg))\n",
    "    wstd = float(np.std(seg, ddof=0))\n",
    "    t = np.arange(len(seg), dtype=np.float32)\n",
    "    denom = float(np.var(t) + 1e-12)\n",
    "    slope = float(np.cov(t, seg, ddof=0)[0, 1] / denom) if denom > 0 else 0.0\n",
    "    return wmean, wstd, slope\n",
    "\n",
    "\n",
    "def build_feature_matrix_full(vce: np.ndarray, feat_names: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build X(T,F) in EXACT feature order of feat_names.\n",
    "\n",
    "    Supported names (based on your Trial13):\n",
    "      - \"min_vce\"\n",
    "      - \"delta_<k>\"\n",
    "      - \"ema_<span>\"\n",
    "      - \"rollstd_<w>\"\n",
    "      - \"win_mean\", \"win_std\", \"win_slope\"  (NOTE: these are window-dependent; not built here)\n",
    "    \"\"\"\n",
    "    T = len(vce)\n",
    "    # precompute some commonly requested features by parsing feat_names\n",
    "    base_map: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    # required\n",
    "    base_map[\"min_vce\"] = vce.astype(np.float32)\n",
    "\n",
    "    # parse deltas / ema / rollstd\n",
    "    for name in feat_names:\n",
    "        if name.startswith(\"delta_\"):\n",
    "            k = int(name.split(\"_\")[1])\n",
    "            base_map[name] = delta_k(vce, k)\n",
    "        elif name.startswith(\"ema_\"):\n",
    "            s = int(name.split(\"_\")[1])\n",
    "            base_map[name] = ema(vce, s)\n",
    "        elif name.startswith(\"rollstd_\"):\n",
    "            w = int(name.split(\"_\")[1])\n",
    "            base_map[name] = rolling_std(vce, w)\n",
    "\n",
    "    # window stats handled later (they need a specific window segment)\n",
    "    # We'll fill placeholders (zeros) here; dataset will overwrite per-window.\n",
    "    for name in [\"win_mean\", \"win_std\", \"win_slope\"]:\n",
    "        if name in feat_names:\n",
    "            base_map[name] = np.zeros((T,), dtype=np.float32)\n",
    "\n",
    "    # assemble in order\n",
    "    X = np.stack([base_map[n] for n in feat_names], axis=1).astype(np.float32)\n",
    "    return X\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Dataset to fetch windows (scaled + raw)\n",
    "# ============================================================\n",
    "class WindowedDatasetWithRaw(Dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x_scaled: (L,F)\n",
    "      y_norm: (1,)\n",
    "      y_cycles: float\n",
    "      rul0: float\n",
    "      file: str\n",
    "      start_idx: int\n",
    "      cycle_target: int\n",
    "      x_raw: (L,F)  (UNSCALED)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_list: List[Path],\n",
    "        seq_len: int,\n",
    "        stride: int,\n",
    "        pred_horizon: int,\n",
    "        scaler_x: StandardScaler,\n",
    "        feat_names: List[str],\n",
    "    ):\n",
    "        self.file_list = file_list\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.scaler_x = scaler_x\n",
    "        self.feat_names = feat_names\n",
    "\n",
    "        self.series: List[Tuple[str, np.ndarray, np.ndarray, float]] = []\n",
    "        for fp in self.file_list:\n",
    "            vce, rul = read_one_csv(fp)\n",
    "            rul0 = float(rul[0])\n",
    "            if rul0 <= 0:\n",
    "                raise ValueError(f\"{fp.name}: RUL0 must be >0\")\n",
    "            Xfull = build_feature_matrix_full(vce, feat_names=self.feat_names)  # (T,F) with win_stats placeholders\n",
    "            self.series.append((fp.name, Xfull, rul.astype(np.float32), rul0))\n",
    "\n",
    "        self.index: List[Tuple[int, int]] = []\n",
    "        for fi, (_name, Xfull, _rul, _rul0) in enumerate(self.series):\n",
    "            T = Xfull.shape[0]\n",
    "            last_start = T - (seq_len + pred_horizon)\n",
    "            if last_start < 0:\n",
    "                continue\n",
    "            for s in range(0, last_start + 1, stride):\n",
    "                self.index.append((fi, s))\n",
    "\n",
    "        if len(self.index) == 0:\n",
    "            raise ValueError(\"No windows created. Check seq_len/pred_horizon vs file lengths.\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fi, s = self.index[idx]\n",
    "        name, Xfull, rul, rul0 = self.series[fi]\n",
    "\n",
    "        x_raw = Xfull[s:s + self.seq_len, :].copy()  # (L,F)\n",
    "\n",
    "        # overwrite win_* stats if present\n",
    "        if (\"win_mean\" in self.feat_names) or (\"win_std\" in self.feat_names) or (\"win_slope\" in self.feat_names):\n",
    "            # need raw vce segment: we can recover it from min_vce column\n",
    "            vseg = x_raw[:, self.feat_names.index(\"min_vce\")]\n",
    "            wm, ws, wsl = window_stats(vseg)\n",
    "            if \"win_mean\" in self.feat_names:\n",
    "                x_raw[:, self.feat_names.index(\"win_mean\")] = wm\n",
    "            if \"win_std\" in self.feat_names:\n",
    "                x_raw[:, self.feat_names.index(\"win_std\")] = ws\n",
    "            if \"win_slope\" in self.feat_names:\n",
    "                x_raw[:, self.feat_names.index(\"win_slope\")] = wsl\n",
    "\n",
    "        y_idx = s + self.seq_len - 1 + self.pred_horizon\n",
    "        y_cycles = float(rul[y_idx])\n",
    "        y_norm = np.array([y_cycles / rul0], dtype=np.float32)\n",
    "\n",
    "        x_scaled = self.scaler_x.transform(x_raw).astype(np.float32)\n",
    "\n",
    "        cycle_target = int(s + (self.seq_len - 1) + self.pred_horizon)\n",
    "        return (\n",
    "            torch.from_numpy(x_scaled),\n",
    "            torch.from_numpy(y_norm),\n",
    "            torch.tensor(y_cycles, dtype=torch.float32),\n",
    "            torch.tensor(rul0, dtype=torch.float32),\n",
    "            name,\n",
    "            torch.tensor(s, dtype=torch.long),\n",
    "            torch.tensor(cycle_target, dtype=torch.long),\n",
    "            torch.from_numpy(x_raw.astype(np.float32)),\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Model (same as your Trial13 family: last hidden state)\n",
    "# If Trial14 changed readout, update forward() accordingly.\n",
    "# ============================================================\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.head(last)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load data list & build dataset\n",
    "# ============================================================\n",
    "names = pd.read_csv(SPLIT_LIST, header=None)[0].astype(str).tolist()\n",
    "file_list = [Path(cfg.DATA_DIR) / n for n in names]\n",
    "\n",
    "ds = WindowedDatasetWithRaw(\n",
    "    file_list=file_list,\n",
    "    seq_len=cfg.SEQ_LEN,\n",
    "    stride=cfg.STRIDE,\n",
    "    pred_horizon=cfg.PRED_HORIZON,\n",
    "    scaler_x=scaler_x,\n",
    "    feat_names=FEATURE_NAMES,\n",
    ")\n",
    "\n",
    "# build lookup from (file,start_idx) -> ds_idx for fast selection\n",
    "lookup: Dict[Tuple[str, int], int] = {}\n",
    "for i in range(len(ds)):\n",
    "    _x, _yn, _yc, _r0, fname, s, cyc, _xr = ds[i]\n",
    "    lookup[(fname, int(s.item()))] = i\n",
    "\n",
    "print(\"Dataset windows:\", len(ds), \"Lookup size:\", len(lookup))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load windows CSV and ensure abs_err\n",
    "# ============================================================\n",
    "dfw = pd.read_csv(WIN_CSV)\n",
    "for col in [\"file\", \"start_idx\", \"cycle\", \"rul0\", \"RUL_true\", \"RUL_pred\"]:\n",
    "    if col not in dfw.columns:\n",
    "        raise ValueError(f\"{WIN_CSV} missing column: {col}\")\n",
    "\n",
    "dfw[\"abs_err\"] = np.abs(pd.to_numeric(dfw[\"RUL_pred\"], errors=\"coerce\") - pd.to_numeric(dfw[\"RUL_true\"], errors=\"coerce\"))\n",
    "dfw = dfw.dropna(subset=[\"file\", \"start_idx\", \"cycle\", \"rul0\", \"RUL_true\", \"RUL_pred\", \"abs_err\"]).copy()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# (1) CRA robust recompute\n",
    "# ============================================================\n",
    "def compute_cra_robust_from_cycle_seq(\n",
    "    df_seq: pd.DataFrame,\n",
    "    denom_floor_cyc: float,\n",
    "    exclude_rul_below: Optional[float] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    df_seq = df_seq.sort_values([\"file\", \"cycle\"]).reset_index(drop=True).copy()\n",
    "    df_seq[\"RUL_true\"] = pd.to_numeric(df_seq[\"RUL_true\"], errors=\"coerce\")\n",
    "    df_seq[\"RUL_pred\"] = pd.to_numeric(df_seq[\"RUL_pred\"], errors=\"coerce\")\n",
    "    df_seq = df_seq.dropna(subset=[\"RUL_true\", \"RUL_pred\", \"file\"]).copy()\n",
    "\n",
    "    # robust rel err\n",
    "    denom = np.maximum(np.abs(df_seq[\"RUL_true\"].values), float(denom_floor_cyc))\n",
    "    rel_err = np.abs(df_seq[\"RUL_true\"].values - df_seq[\"RUL_pred\"].values) / denom\n",
    "    RA = 1.0 - rel_err\n",
    "\n",
    "    df_seq[\"rel_err_robust\"] = rel_err\n",
    "    df_seq[\"RA_robust\"] = RA\n",
    "\n",
    "    if exclude_rul_below is not None:\n",
    "        mask = df_seq[\"RUL_true\"].values >= float(exclude_rul_below)\n",
    "        df_use = df_seq[mask].copy()\n",
    "    else:\n",
    "        df_use = df_seq\n",
    "\n",
    "    # CRA per file\n",
    "    per_file = (\n",
    "        df_use.groupby(\"file\", as_index=False)\n",
    "        .agg(\n",
    "            CRA_robust_mean=(\"RA_robust\", \"mean\"),\n",
    "            CRA_robust_median=(\"RA_robust\", \"median\"),\n",
    "            n_points=(\"RA_robust\", \"count\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # global stats\n",
    "    cra_vals = per_file[\"CRA_robust_mean\"].values.astype(np.float64)\n",
    "    out = {\n",
    "        \"CRA_robust_mean_of_files\": float(np.mean(cra_vals)) if len(cra_vals) else np.nan,\n",
    "        \"CRA_robust_median_of_files\": float(np.median(cra_vals)) if len(cra_vals) else np.nan,\n",
    "        \"CRA_robust_std_of_files\": float(np.std(cra_vals, ddof=0)) if len(cra_vals) else np.nan,\n",
    "        \"n_files\": int(len(per_file)),\n",
    "        \"denom_floor_cyc\": float(denom_floor_cyc),\n",
    "        \"exclude_rul_below\": float(exclude_rul_below) if exclude_rul_below is not None else np.nan,\n",
    "    }\n",
    "    return {\"summary\": out, \"per_file\": per_file, \"seq_with_ra\": df_seq}\n",
    "\n",
    "\n",
    "# load or build cycle seq\n",
    "if os.path.exists(SEQ_CSV):\n",
    "    dfseq = pd.read_csv(SEQ_CSV)\n",
    "else:\n",
    "    dfseq = windows_to_cycle_sequence_mean(dfw)\n",
    "    dfseq.to_csv(os.path.join(OUT_DIR, f\"00_built_{cfg.SPLIT}_cycle_sequence_mean.csv\"), index=False)\n",
    "\n",
    "cra_res = compute_cra_robust_from_cycle_seq(\n",
    "    df_seq=dfseq,\n",
    "    denom_floor_cyc=cfg.CRA_DENOM_FLOOR_CYC,\n",
    "    exclude_rul_below=cfg.CRA_EXCLUDE_RUL_BELOW,\n",
    ")\n",
    "\n",
    "pd.DataFrame([cra_res[\"summary\"]]).to_csv(os.path.join(OUT_DIR, \"00_CRA_robust_summary.csv\"), index=False)\n",
    "cra_res[\"per_file\"].to_csv(os.path.join(OUT_DIR, \"00_CRA_robust_per_file.csv\"), index=False)\n",
    "\n",
    "print(\"\\n[CRA robust] summary:\", cra_res[\"summary\"])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# (2) High-error vs Low-error feature distribution\n",
    "# ============================================================\n",
    "def pick_error_groups(dfw_: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, float]]:\n",
    "    dfw_ = dfw_.copy()\n",
    "    if cfg.ERROR_MODE == \"fixed\":\n",
    "        thr = float(cfg.ERROR_FIXED_THR)\n",
    "        df_high = dfw_[dfw_[\"abs_err\"] >= thr].copy()\n",
    "        df_low = dfw_[dfw_[\"abs_err\"] < thr].copy()\n",
    "        meta = {\"mode\": \"fixed\", \"thr\": thr}\n",
    "        return df_high, df_low, meta\n",
    "\n",
    "    # quantile mode\n",
    "    qh = float(dfw_[\"abs_err\"].quantile(cfg.ERROR_Q_HIGH))\n",
    "    ql = float(dfw_[\"abs_err\"].quantile(cfg.ERROR_Q_LOW))\n",
    "    df_high = dfw_[dfw_[\"abs_err\"] >= qh].copy()\n",
    "    df_low = dfw_[dfw_[\"abs_err\"] <= ql].copy()\n",
    "    meta = {\"mode\": \"quantile\", \"q_high\": cfg.ERROR_Q_HIGH, \"q_low\": cfg.ERROR_Q_LOW, \"thr_high\": qh, \"thr_low\": ql}\n",
    "    return df_high, df_low, meta\n",
    "\n",
    "\n",
    "def sample_windows_exist(df_sel: pd.DataFrame, max_n: int) -> pd.DataFrame:\n",
    "    # keep only rows that exist in lookup\n",
    "    ok = []\n",
    "    for _, r in df_sel.iterrows():\n",
    "        key = (str(r[\"file\"]), int(r[\"start_idx\"]))\n",
    "        ok.append(key in lookup)\n",
    "    df_sel = df_sel[np.array(ok, dtype=bool)].copy()\n",
    "    if len(df_sel) > max_n:\n",
    "        df_sel = df_sel.sample(n=max_n, random_state=0).copy()\n",
    "    return df_sel\n",
    "\n",
    "\n",
    "def compute_feature_stats_for_windows(df_sel: pd.DataFrame, tail_k: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns per-feature stats over all timesteps in selected windows:\n",
    "      - mean, std, q10/q50/q90 (overall)\n",
    "      - tail_mean, tail_std (last tail_k steps only)\n",
    "    Uses UN-SCALED raw features.\n",
    "    \"\"\"\n",
    "    mats_all = []\n",
    "    mats_tail = []\n",
    "    for _, r in df_sel.iterrows():\n",
    "        ds_idx = lookup[(str(r[\"file\"]), int(r[\"start_idx\"]))]\n",
    "        *_ignore, x_raw = ds[ds_idx]\n",
    "        xr = x_raw.numpy()  # (L,F)\n",
    "        mats_all.append(xr)\n",
    "        mats_tail.append(xr[-tail_k:, :] if tail_k > 0 else xr)\n",
    "\n",
    "    Xall = np.concatenate(mats_all, axis=0)  # (N*L, F)\n",
    "    Xtail = np.concatenate(mats_tail, axis=0)  # (N*tail_k, F)\n",
    "\n",
    "    rows = []\n",
    "    for j, fn in enumerate(FEATURE_NAMES):\n",
    "        col = Xall[:, j].astype(np.float64)\n",
    "        col_tail = Xtail[:, j].astype(np.float64)\n",
    "        rows.append({\n",
    "            \"feature\": fn,\n",
    "            \"mean\": float(np.mean(col)),\n",
    "            \"std\": float(np.std(col, ddof=0)),\n",
    "            \"q10\": float(np.quantile(col, 0.10)),\n",
    "            \"q50\": float(np.quantile(col, 0.50)),\n",
    "            \"q90\": float(np.quantile(col, 0.90)),\n",
    "            \"tail_mean\": float(np.mean(col_tail)),\n",
    "            \"tail_std\": float(np.std(col_tail, ddof=0)),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "df_high, df_low, meta_err = pick_error_groups(dfw)\n",
    "df_high = sample_windows_exist(df_high, cfg.MAX_WINDOWS_PER_GROUP)\n",
    "df_low = sample_windows_exist(df_low, cfg.MAX_WINDOWS_PER_GROUP)\n",
    "\n",
    "summary_row = {\n",
    "    \"error_mode\": meta_err.get(\"mode\", \"\"),\n",
    "    **meta_err,\n",
    "    \"n_windows_total\": int(len(dfw)),\n",
    "    \"n_high_used\": int(len(df_high)),\n",
    "    \"n_low_used\": int(len(df_low)),\n",
    "}\n",
    "pd.DataFrame([summary_row]).to_csv(os.path.join(OUT_DIR, \"01_error_split_summary.csv\"), index=False)\n",
    "\n",
    "if len(df_high) == 0 or len(df_low) == 0:\n",
    "    print(\"\\n[Warn] High/Low groups empty after lookup filtering. Skipping feature distribution check.\")\n",
    "else:\n",
    "    st_high = compute_feature_stats_for_windows(df_high, tail_k=cfg.TAIL_K_FOR_STATS)\n",
    "    st_low = compute_feature_stats_for_windows(df_low, tail_k=cfg.TAIL_K_FOR_STATS)\n",
    "\n",
    "    st_high.to_csv(os.path.join(OUT_DIR, \"01_feature_stats_high.csv\"), index=False)\n",
    "    st_low.to_csv(os.path.join(OUT_DIR, \"01_feature_stats_low.csv\"), index=False)\n",
    "\n",
    "    # shift report (high - low)\n",
    "    merged = st_high.merge(st_low, on=\"feature\", suffixes=(\"_high\", \"_low\"))\n",
    "    for c in [\"mean\", \"std\", \"q10\", \"q50\", \"q90\", \"tail_mean\", \"tail_std\"]:\n",
    "        merged[f\"delta_{c}\"] = merged[f\"{c}_high\"] - merged[f\"{c}_low\"]\n",
    "    merged.to_csv(os.path.join(OUT_DIR, \"01_feature_shift_report.csv\"), index=False)\n",
    "\n",
    "    if cfg.SAVE_PLOTS:\n",
    "        # plot top-|delta| features for mean shift\n",
    "        top = merged.copy()\n",
    "        top[\"abs_delta_mean\"] = np.abs(top[\"delta_mean\"].values)\n",
    "        top = top.sort_values(\"abs_delta_mean\", ascending=False).head(min(15, len(top)))\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.bar(top[\"feature\"].values, top[\"delta_mean\"].values)\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.title(\"High-error minus Low-error: feature mean shift (raw)\")\n",
    "        plt.grid(True, axis=\"y\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, \"01_feature_mean_shift_top.png\"), dpi=200, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "    print(\"\\n[Feature shift] saved:\",\n",
    "          \"01_feature_stats_high.csv, 01_feature_stats_low.csv, 01_feature_shift_report.csv\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# (3) Tail ablation suite\n",
    "# ============================================================\n",
    "model = LSTMRegressor(F_DIM, cfg.HIDDEN_SIZE, cfg.NUM_LAYERS, cfg.DROPOUT).to(device)\n",
    "model.load_state_dict(torch.load(CKPT_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Build a loader over ds indices sampled from dfw so we align with predictions\n",
    "def build_eval_indices(dfw_: pd.DataFrame, max_n: int) -> List[int]:\n",
    "    # keep only lookup hits\n",
    "    keys = []\n",
    "    for _, r in dfw_.iterrows():\n",
    "        k = (str(r[\"file\"]), int(r[\"start_idx\"]))\n",
    "        if k in lookup:\n",
    "            keys.append(lookup[k])\n",
    "    if len(keys) == 0:\n",
    "        return []\n",
    "    if len(keys) > max_n:\n",
    "        rng = np.random.RandomState(0)\n",
    "        keys = rng.choice(keys, size=max_n, replace=False).tolist()\n",
    "    return keys\n",
    "\n",
    "\n",
    "class SubsetIndexDataset(Dataset):\n",
    "    def __init__(self, base_ds: Dataset, indices: List[int]):\n",
    "        self.base_ds = base_ds\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.base_ds[self.indices[i]]\n",
    "\n",
    "\n",
    "eval_indices = build_eval_indices(dfw, cfg.ABLATION_MAX_WINDOWS)\n",
    "if len(eval_indices) == 0:\n",
    "    print(\"\\n[Warn] No eval indices found for tail ablation (lookup mismatch). Skipping.\")\n",
    "else:\n",
    "    eval_ds = SubsetIndexDataset(ds, eval_indices)\n",
    "    eval_loader = DataLoader(eval_ds, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    def apply_ablation(x: torch.Tensor, mode: str, tail_k: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B,L,F) scaled input\n",
    "        \"\"\"\n",
    "        if tail_k <= 0:\n",
    "            return x\n",
    "\n",
    "        B, L, F = x.shape\n",
    "        k = min(tail_k, L)\n",
    "        out = x.clone()\n",
    "\n",
    "        if mode == \"none\":\n",
    "            return out\n",
    "\n",
    "        if mode == \"mask_tail\":\n",
    "            out[:, L-k:, :] = 0.0\n",
    "            return out\n",
    "\n",
    "        if mode == \"keep_tail\":\n",
    "            out[:, :L-k, :] = 0.0\n",
    "            return out\n",
    "\n",
    "        if mode == \"shuffle_tail\":\n",
    "            # shuffle time order within tail per sample\n",
    "            for b in range(B):\n",
    "                perm = torch.randperm(k, device=out.device)\n",
    "                out[b, L-k:, :] = out[b, L-k:, :][perm, :]\n",
    "            return out\n",
    "\n",
    "        if mode == \"shuffle_all\":\n",
    "            for b in range(B):\n",
    "                perm = torch.randperm(L, device=out.device)\n",
    "                out[b, :, :] = out[b, :, :][perm, :]\n",
    "            return out\n",
    "\n",
    "        raise ValueError(f\"Unknown ablation mode: {mode}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_with_ablation(mode: str) -> Dict[str, float]:\n",
    "        mae_cyc = []\n",
    "        mse_cyc = []\n",
    "        mae_norm = []\n",
    "        mse_norm = []\n",
    "\n",
    "        for x_scaled, y_norm, y_cycles, rul0, *_rest in eval_loader:\n",
    "            x_scaled = x_scaled.to(device)\n",
    "            y_norm = y_norm.to(device).view(-1, 1)\n",
    "            y_cycles = y_cycles.to(device).view(-1, 1)\n",
    "            rul0 = rul0.to(device).view(-1, 1)\n",
    "\n",
    "            xa = apply_ablation(x_scaled, mode=mode, tail_k=cfg.TAIL_K)\n",
    "            pred_norm = model(xa)\n",
    "            pred_cycles = pred_norm * rul0\n",
    "\n",
    "            err_norm = pred_norm - y_norm\n",
    "            err_cyc = pred_cycles - y_cycles\n",
    "\n",
    "            mae_norm.append(torch.mean(torch.abs(err_norm)).item())\n",
    "            mse_norm.append(torch.mean(err_norm ** 2).item())\n",
    "            mae_cyc.append(torch.mean(torch.abs(err_cyc)).item())\n",
    "            mse_cyc.append(torch.mean(err_cyc ** 2).item())\n",
    "\n",
    "        return {\n",
    "            \"mode\": mode,\n",
    "            \"n_windows\": int(len(eval_ds)),\n",
    "            \"mae_norm\": float(np.mean(mae_norm)),\n",
    "            \"rmse_norm\": float(np.sqrt(np.mean(mse_norm))),\n",
    "            \"mae_cycles\": float(np.mean(mae_cyc)),\n",
    "            \"rmse_cycles\": float(np.sqrt(np.mean(mse_cyc))),\n",
    "        }\n",
    "\n",
    "    modes = [\"none\", \"mask_tail\", \"keep_tail\", \"shuffle_tail\", \"shuffle_all\"]\n",
    "    rows = [eval_with_ablation(m) for m in modes]\n",
    "    df_ab = pd.DataFrame(rows)\n",
    "\n",
    "    # deltas vs baseline\n",
    "    base_rmse = float(df_ab.loc[df_ab[\"mode\"] == \"none\", \"rmse_cycles\"].iloc[0])\n",
    "    df_ab[\"delta_rmse_cycles_vs_base\"] = df_ab[\"rmse_cycles\"] - base_rmse\n",
    "\n",
    "    df_ab.to_csv(os.path.join(OUT_DIR, \"02_tail_ablation_metrics.csv\"), index=False)\n",
    "\n",
    "    if cfg.SAVE_PLOTS:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.bar(df_ab[\"mode\"].values, df_ab[\"delta_rmse_cycles_vs_base\"].values)\n",
    "        plt.xticks(rotation=30, ha=\"right\")\n",
    "        plt.ylabel(\"Δ RMSE (cycles) vs base\")\n",
    "        plt.title(f\"Tail ablation impact (TAIL_K={cfg.TAIL_K})\")\n",
    "        plt.grid(True, axis=\"y\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, \"02_tail_ablation_delta_rmse.png\"), dpi=200, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "    print(\"\\n[Tail ablation] saved: 02_tail_ablation_metrics.csv (+ plot if enabled)\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DONE\n",
    "# ============================================================\n",
    "print(\"\\n[DONE] Trial14 XAI+Diagnosis pack saved to:\")\n",
    "print(\" ->\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fa2fb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] Analyze_Trial14_RootCause\\01_per_file_error_by_seed_ckpt.csv\n",
      "[Saved] Analyze_Trial14_RootCause\\02_raw_minvce_descriptors.csv\n",
      "[Saved] Analyze_Trial14_RootCause\\03_joined_error_plus_raw.csv\n",
      "[Saved] Analyze_Trial14_RootCause\\04_error_group_flags.csv\n",
      "[Saved] Analyze_Trial14_RootCause\\05_persistent_high_error_files.csv\n",
      "[Saved] Analyze_Trial14_RootCause\\06_shift_high_minus_low_ALL.csv\n",
      "[Saved] Analyze_Trial14_RootCause\\07_shift_high_minus_low_by_ckpt.csv\n",
      "[Saved] Analyze_Trial14_RootCause\\08_corr_with_rmse.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\11HOME_AHCI\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\11HOME_AHCI\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\11HOME_AHCI\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\11HOME_AHCI\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] Analyze_Trial14_RootCause\\09_clusters_pca.png\n",
      "[Saved] Analyze_Trial14_RootCause\\09_file_clusters_with_persistency.csv\n",
      "[Saved] Analyze_Trial14_RootCause\\10_top_persistent_high_error.png\n",
      "\n",
      "DONE. Output folder: ./Analyze_Trial14_RootCause\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# optional (clustering)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Config\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    # ---- your raw dataset (same as training) ----\n",
    "    data_dir: str = r\"C:\\Users\\11HOME_AHCI\\Desktop\\PoF\\Winter 2026\\Ref Data4\\100\"\n",
    "\n",
    "    # ---- Trial14 root folder ----\n",
    "    trial_dir: str = r\"./Trial14\"\n",
    "\n",
    "    # seeds you ran\n",
    "    seeds: Tuple[int, ...] = (9819123, 111, 222, 333, 444)\n",
    "\n",
    "    # checkpoints you exported\n",
    "    ckpts: Tuple[str, ...] = (\"best_by_val_norm\", \"last_epoch\")\n",
    "\n",
    "    # which split to analyze (must exist in ckpt folder)\n",
    "    split: str = \"test\"   # \"train\"/\"val\"/\"test\"\n",
    "\n",
    "    # cycle-sequence file name you saved in Trial9-style eval\n",
    "    # (your pipeline writes \"<split>_cycle_sequence_mean.csv\")\n",
    "    cycle_seq_name: str = \"{split}_cycle_sequence_mean.csv\"\n",
    "\n",
    "    # ---- error definition ----\n",
    "    error_metric: str = \"rmse\"  # \"rmse\" or \"mae\"\n",
    "    high_error_top_pct: float = 0.20  # top 20% = high error\n",
    "    low_error_bottom_pct: float = 0.20  # bottom 20% = low error\n",
    "\n",
    "    # ---- feature extraction from raw min_vce ----\n",
    "    tail_k: int = 10      # tail length (last K points)\n",
    "    delta_k: int = 50     # delta over last K steps (v_end - v_end-k)\n",
    "    early_k: int = 10     # early window length (first K points)\n",
    "\n",
    "    # ---- clustering ----\n",
    "    do_clustering: bool = True\n",
    "    k_range: Tuple[int, ...] = (2, 3, 4, 5)\n",
    "\n",
    "    # ---- outputs ----\n",
    "    out_dir: str = r\"./Analyze_Trial14_RootCause\"\n",
    "\n",
    "\n",
    "cfg = Cfg()\n",
    "Path(cfg.out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "def _read_raw_file(data_dir: str, file_name: str) -> Optional[Tuple[np.ndarray, np.ndarray]]:\n",
    "    fp = Path(data_dir) / file_name\n",
    "    if not fp.exists():\n",
    "        return None\n",
    "    df = pd.read_csv(fp, header=None)\n",
    "    if df.shape[1] < 2:\n",
    "        return None\n",
    "    v = df.iloc[:, 0].astype(np.float32).to_numpy()\n",
    "    rul = df.iloc[:, 1].astype(np.float32).to_numpy()\n",
    "    if len(v) == 0 or len(rul) == 0:\n",
    "        return None\n",
    "    return v, rul\n",
    "\n",
    "\n",
    "def _trial_cycle_seq_path(trial_dir: str, seed: int, ckpt: str, split: str, pattern: str) -> Path:\n",
    "    name = pattern.format(split=split)\n",
    "    return Path(trial_dir) / f\"seed_{seed}\" / ckpt / name\n",
    "\n",
    "\n",
    "def _safe_num(x) -> float:\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "\n",
    "def _rmse(a: np.ndarray) -> float:\n",
    "    return float(np.sqrt(np.mean(a * a))) if len(a) else float(\"nan\")\n",
    "\n",
    "\n",
    "def _mae(a: np.ndarray) -> float:\n",
    "    return float(np.mean(np.abs(a))) if len(a) else float(\"nan\")\n",
    "\n",
    "\n",
    "def _linear_slope(y: np.ndarray) -> float:\n",
    "    # slope of y ~ t (least squares)\n",
    "    if len(y) < 2:\n",
    "        return 0.0\n",
    "    t = np.arange(len(y), dtype=np.float32)\n",
    "    vt = np.var(t)\n",
    "    if vt < 1e-12:\n",
    "        return 0.0\n",
    "    # cov(t,y)/var(t)\n",
    "    return float(np.cov(t, y, ddof=0)[0, 1] / (vt + 1e-12))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Load Trial14 per-file errors from cycle_sequence_mean.csv\n",
    "# ============================================================\n",
    "def load_per_file_errors() -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for seed in cfg.seeds:\n",
    "        for ckpt in cfg.ckpts:\n",
    "            p = _trial_cycle_seq_path(cfg.trial_dir, seed, ckpt, cfg.split, cfg.cycle_seq_name)\n",
    "            if not p.exists():\n",
    "                continue\n",
    "            df = pd.read_csv(p)\n",
    "            # expected columns: file, cycle, RUL_true, RUL_pred, (rul0 maybe)\n",
    "            if df.empty or (\"file\" not in df.columns) or (\"RUL_true\" not in df.columns) or (\"RUL_pred\" not in df.columns):\n",
    "                continue\n",
    "\n",
    "            df = df.dropna(subset=[\"file\", \"RUL_true\", \"RUL_pred\"]).copy()\n",
    "            df[\"err\"] = (df[\"RUL_pred\"].astype(float) - df[\"RUL_true\"].astype(float)).values\n",
    "\n",
    "            g = df.groupby(\"file\", as_index=False).agg(\n",
    "                n_points=(\"err\", \"count\"),\n",
    "                mae=(\"err\", lambda x: _mae(x.values.astype(np.float32))),\n",
    "                rmse=(\"err\", lambda x: _rmse(x.values.astype(np.float32))),\n",
    "            )\n",
    "            g[\"seed\"] = int(seed)\n",
    "            g[\"checkpoint\"] = str(ckpt)\n",
    "            g[\"cycle_seq_path\"] = str(p)\n",
    "            rows.append(g)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    out = pd.concat(rows, axis=0, ignore_index=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "df_err = load_per_file_errors()\n",
    "err_out = Path(cfg.out_dir) / \"01_per_file_error_by_seed_ckpt.csv\"\n",
    "df_err.to_csv(err_out, index=False)\n",
    "print(\"[Saved]\", err_out)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Extract raw min_vce-derived descriptors (v0 / tail / deltas)\n",
    "# ============================================================\n",
    "def extract_raw_descriptors(file_list: List[str]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for fn in sorted(set(file_list)):\n",
    "        rr = _read_raw_file(cfg.data_dir, fn)\n",
    "        if rr is None:\n",
    "            continue\n",
    "        v, rul = rr\n",
    "        T = len(v)\n",
    "\n",
    "        # basic points\n",
    "        v0 = float(v[0])\n",
    "        v_end = float(v[-1])\n",
    "\n",
    "        # early/tail stats\n",
    "        ek = min(cfg.early_k, T)\n",
    "        tk = min(cfg.tail_k, T)\n",
    "        early_seg = v[:ek]\n",
    "        tail_seg = v[-tk:]\n",
    "\n",
    "        early_mean = float(np.mean(early_seg))\n",
    "        early_std = float(np.std(early_seg, ddof=0))\n",
    "        tail_mean = float(np.mean(tail_seg))\n",
    "        tail_std = float(np.std(tail_seg, ddof=0))\n",
    "        tail_slope = _linear_slope(tail_seg)\n",
    "\n",
    "        # deltas\n",
    "        dk = int(cfg.delta_k)\n",
    "        if dk <= 0 or T <= dk:\n",
    "            d_end_k = float(v_end - v0)\n",
    "        else:\n",
    "            d_end_k = float(v_end - float(v[-1 - dk]))\n",
    "\n",
    "        drift_total = float(v_end - v0)\n",
    "\n",
    "        # normalized versions (protect divide by 0)\n",
    "        denom = abs(v0) if abs(v0) > 1e-12 else 1.0\n",
    "        drift_total_rel = float(drift_total / denom)\n",
    "        d_end_k_rel = float(d_end_k / denom)\n",
    "        tail_mean_rel = float((tail_mean - v0) / denom)\n",
    "\n",
    "        rows.append({\n",
    "            \"file\": fn,\n",
    "            \"T\": int(T),\n",
    "            \"v0\": v0,\n",
    "            \"v_end\": v_end,\n",
    "            \"early_mean\": early_mean,\n",
    "            \"early_std\": early_std,\n",
    "            \"tail_mean\": tail_mean,\n",
    "            \"tail_std\": tail_std,\n",
    "            \"tail_slope\": tail_slope,\n",
    "            \"drift_total\": drift_total,\n",
    "            \"d_end_k\": d_end_k,\n",
    "            \"drift_total_rel\": drift_total_rel,\n",
    "            \"d_end_k_rel\": d_end_k_rel,\n",
    "            \"tail_mean_rel\": tail_mean_rel,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "if df_err.empty:\n",
    "    print(\"[WARN] No error files found. Check Trial14 paths / split name.\")\n",
    "    raise SystemExit(0)\n",
    "\n",
    "df_raw = extract_raw_descriptors(df_err[\"file\"].tolist())\n",
    "raw_out = Path(cfg.out_dir) / \"02_raw_minvce_descriptors.csv\"\n",
    "df_raw.to_csv(raw_out, index=False)\n",
    "print(\"[Saved]\", raw_out)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Join: error + raw descriptors\n",
    "# ============================================================\n",
    "df = df_err.merge(df_raw, on=\"file\", how=\"left\")\n",
    "join_out = Path(cfg.out_dir) / \"03_joined_error_plus_raw.csv\"\n",
    "df.to_csv(join_out, index=False)\n",
    "print(\"[Saved]\", join_out)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Define high/low error groups per (seed, ckpt), and persistency\n",
    "# ============================================================\n",
    "def mark_groups(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    metric = cfg.error_metric\n",
    "    if metric not in d.columns:\n",
    "        raise ValueError(f\"error_metric '{metric}' not found in columns.\")\n",
    "\n",
    "    d[\"is_high\"] = 0\n",
    "    d[\"is_low\"] = 0\n",
    "\n",
    "    for (seed, ckpt), sub in d.groupby([\"seed\", \"checkpoint\"]):\n",
    "        sub = sub.sort_values(metric, ascending=False).reset_index()\n",
    "        n = len(sub)\n",
    "        if n == 0:\n",
    "            continue\n",
    "        hi_n = max(1, int(np.floor(cfg.high_error_top_pct * n)))\n",
    "        lo_n = max(1, int(np.floor(cfg.low_error_bottom_pct * n)))\n",
    "\n",
    "        hi_idx = sub.loc[:hi_n - 1, \"index\"].tolist()\n",
    "        lo_idx = sub.loc[n - lo_n:, \"index\"].tolist()  # lowest errors at end if sorted desc? careful:\n",
    "        # since sorted desc, lowest are at bottom\n",
    "        lo_idx = sub.loc[n - lo_n:, \"index\"].tolist()\n",
    "\n",
    "        d.loc[hi_idx, \"is_high\"] = 1\n",
    "        d.loc[lo_idx, \"is_low\"] = 1\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "df_g = mark_groups(df)\n",
    "group_out = Path(cfg.out_dir) / \"04_error_group_flags.csv\"\n",
    "df_g.to_csv(group_out, index=False)\n",
    "print(\"[Saved]\", group_out)\n",
    "\n",
    "# persistency: how often each file is high across runs\n",
    "persist = df_g.groupby(\"file\", as_index=False).agg(\n",
    "    n_runs=(\"is_high\", \"count\"),\n",
    "    high_count=(\"is_high\", \"sum\"),\n",
    "    low_count=(\"is_low\", \"sum\"),\n",
    "    mean_rmse=(\"rmse\", \"mean\"),\n",
    "    mean_mae=(\"mae\", \"mean\"),\n",
    ")\n",
    "persist[\"high_rate\"] = persist[\"high_count\"] / np.maximum(persist[\"n_runs\"], 1)\n",
    "persist = persist.sort_values([\"high_rate\", \"high_count\", \"mean_rmse\"], ascending=False)\n",
    "\n",
    "persist_out = Path(cfg.out_dir) / \"05_persistent_high_error_files.csv\"\n",
    "persist.to_csv(persist_out, index=False)\n",
    "print(\"[Saved]\", persist_out)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) “High vs Low” shift table (overall + per ckpt)\n",
    "# ============================================================\n",
    "SHIFT_FEATURES = [\n",
    "    \"v0\", \"early_mean\", \"early_std\",\n",
    "    \"tail_mean\", \"tail_std\", \"tail_slope\",\n",
    "    \"drift_total\", \"d_end_k\",\n",
    "    \"drift_total_rel\", \"d_end_k_rel\", \"tail_mean_rel\",\n",
    "]\n",
    "\n",
    "def shift_table(df_g: pd.DataFrame, by_ckpt: bool = True) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    group_cols = [\"checkpoint\"] if by_ckpt else []\n",
    "    for keys, sub in df_g.groupby(group_cols) if by_ckpt else [((\"ALL\",), df_g)]:\n",
    "        sub_h = sub[sub[\"is_high\"] == 1]\n",
    "        sub_l = sub[sub[\"is_low\"] == 1]\n",
    "        if sub_h.empty or sub_l.empty:\n",
    "            continue\n",
    "\n",
    "        row_prefix = {}\n",
    "        if by_ckpt:\n",
    "            row_prefix[\"checkpoint\"] = keys if isinstance(keys, str) else keys[0]\n",
    "        else:\n",
    "            row_prefix[\"checkpoint\"] = \"ALL\"\n",
    "\n",
    "        for f in SHIFT_FEATURES:\n",
    "            if f not in sub.columns:\n",
    "                continue\n",
    "            h = pd.to_numeric(sub_h[f], errors=\"coerce\").dropna()\n",
    "            l = pd.to_numeric(sub_l[f], errors=\"coerce\").dropna()\n",
    "            if len(h) == 0 or len(l) == 0:\n",
    "                continue\n",
    "            rows.append({\n",
    "                **row_prefix,\n",
    "                \"feature\": f,\n",
    "                \"high_mean\": float(h.mean()),\n",
    "                \"low_mean\": float(l.mean()),\n",
    "                \"shift(high-low)\": float(h.mean() - l.mean()),\n",
    "                \"high_std\": float(h.std(ddof=0)),\n",
    "                \"low_std\": float(l.std(ddof=0)),\n",
    "                \"n_high\": int(len(h)),\n",
    "                \"n_low\": int(len(l)),\n",
    "            })\n",
    "    out = pd.DataFrame(rows)\n",
    "    if not out.empty:\n",
    "        out = out.sort_values(\"shift(high-low)\", ascending=False)\n",
    "    return out\n",
    "\n",
    "\n",
    "df_shift_all = shift_table(df_g, by_ckpt=False)\n",
    "shift_all_out = Path(cfg.out_dir) / \"06_shift_high_minus_low_ALL.csv\"\n",
    "df_shift_all.to_csv(shift_all_out, index=False)\n",
    "print(\"[Saved]\", shift_all_out)\n",
    "\n",
    "df_shift_ckpt = shift_table(df_g, by_ckpt=True)\n",
    "shift_ckpt_out = Path(cfg.out_dir) / \"07_shift_high_minus_low_by_ckpt.csv\"\n",
    "df_shift_ckpt.to_csv(shift_ckpt_out, index=False)\n",
    "print(\"[Saved]\", shift_ckpt_out)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) Correlation: which raw descriptors correlate with error?\n",
    "# ============================================================\n",
    "def corr_table(df: pd.DataFrame, metric: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    y = pd.to_numeric(df[metric], errors=\"coerce\")\n",
    "    for f in SHIFT_FEATURES:\n",
    "        if f not in df.columns:\n",
    "            continue\n",
    "        x = pd.to_numeric(df[f], errors=\"coerce\")\n",
    "        m = np.isfinite(x.values) & np.isfinite(y.values)\n",
    "        if np.sum(m) < 10:\n",
    "            continue\n",
    "        r = np.corrcoef(x.values[m], y.values[m])[0, 1]\n",
    "        rows.append({\"feature\": f, f\"corr_with_{metric}\": float(r), \"n\": int(np.sum(m))})\n",
    "    out = pd.DataFrame(rows)\n",
    "    if not out.empty:\n",
    "        out = out.reindex(out[f\"corr_with_{metric}\"].abs().sort_values(ascending=False).index)\n",
    "    return out\n",
    "\n",
    "\n",
    "corr_rmse = corr_table(df, \"rmse\")\n",
    "corr_out = Path(cfg.out_dir) / \"08_corr_with_rmse.csv\"\n",
    "corr_rmse.to_csv(corr_out, index=False)\n",
    "print(\"[Saved]\", corr_out)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) Simple clustering of files using raw descriptors (one row per file)\n",
    "# ============================================================\n",
    "def cluster_files(persist: pd.DataFrame, df_raw: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
    "    if not cfg.do_clustering:\n",
    "        return None\n",
    "\n",
    "    # one row per file (merge persistency + raw)\n",
    "    m = persist.merge(df_raw, on=\"file\", how=\"left\")\n",
    "    feats = [c for c in SHIFT_FEATURES if c in m.columns]\n",
    "    # also include \"high_rate\" as label-like output, but not as clustering input\n",
    "    X = m[feats].copy()\n",
    "    X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    # drop rows with too many NaNs\n",
    "    keep = np.isfinite(X.values).all(axis=1)\n",
    "    m2 = m[keep].reset_index(drop=True)\n",
    "    X2 = X[keep].values\n",
    "\n",
    "    if len(m2) < 10:\n",
    "        return None\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Z = scaler.fit_transform(X2)\n",
    "\n",
    "    best_k = None\n",
    "    best_score = -1e9\n",
    "    best_labels = None\n",
    "\n",
    "    for k in cfg.k_range:\n",
    "        if k <= 1 or k >= len(m2):\n",
    "            continue\n",
    "        km = KMeans(n_clusters=k, n_init=20, random_state=0)\n",
    "        labels = km.fit_predict(Z)\n",
    "        # silhouette requires >= 2 clusters and not all same\n",
    "        if len(set(labels)) < 2:\n",
    "            continue\n",
    "        score = silhouette_score(Z, labels)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "            best_labels = labels\n",
    "\n",
    "    if best_labels is None:\n",
    "        return None\n",
    "\n",
    "    m2[\"cluster\"] = best_labels.astype(int)\n",
    "    m2[\"silhouette_best_k\"] = best_k\n",
    "    m2[\"silhouette_score\"] = best_score\n",
    "\n",
    "    # PCA plot (2D)\n",
    "    pca = PCA(n_components=2, random_state=0)\n",
    "    P = pca.fit_transform(Z)\n",
    "    m2[\"pc1\"] = P[:, 0]\n",
    "    m2[\"pc2\"] = P[:, 1]\n",
    "\n",
    "    # save plot\n",
    "    plt.figure()\n",
    "    for cl in sorted(m2[\"cluster\"].unique()):\n",
    "        sub = m2[m2[\"cluster\"] == cl]\n",
    "        plt.scatter(sub[\"pc1\"], sub[\"pc2\"], label=f\"cluster {cl}\", s=25)\n",
    "    plt.title(f\"File clusters (min_vce descriptors) | best_k={best_k} | silhouette={best_score:.3f}\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    p = Path(cfg.out_dir) / \"09_clusters_pca.png\"\n",
    "    plt.savefig(p, dpi=200)\n",
    "    plt.close()\n",
    "    print(\"[Saved]\", p)\n",
    "\n",
    "    return m2\n",
    "\n",
    "\n",
    "df_cluster = cluster_files(persist, df_raw)\n",
    "if df_cluster is not None:\n",
    "    cluster_out = Path(cfg.out_dir) / \"09_file_clusters_with_persistency.csv\"\n",
    "    df_cluster.to_csv(cluster_out, index=False)\n",
    "    print(\"[Saved]\", cluster_out)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) A couple of quick figures (optional but useful)\n",
    "# ============================================================\n",
    "def plot_top_persistent(persist: pd.DataFrame, topn: int = 20):\n",
    "    sub = persist.head(topn).copy()\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(sub[\"file\"].astype(str), sub[\"high_rate\"].astype(float))\n",
    "    plt.xticks(rotation=70, ha=\"right\")\n",
    "    plt.ylabel(\"High-error rate\")\n",
    "    plt.title(f\"Top {topn} persistent high-error files (across seeds/ckpts)\")\n",
    "    plt.grid(True, axis=\"y\")\n",
    "    plt.tight_layout()\n",
    "    p = Path(cfg.out_dir) / \"10_top_persistent_high_error.png\"\n",
    "    plt.savefig(p, dpi=200)\n",
    "    plt.close()\n",
    "    print(\"[Saved]\", p)\n",
    "\n",
    "plot_top_persistent(persist, topn=20)\n",
    "\n",
    "print(\"\\nDONE. Output folder:\", cfg.out_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igbt_rnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
