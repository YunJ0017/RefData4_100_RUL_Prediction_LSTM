{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "398156ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "[SEED 9819123] device=cuda\n",
      "[SEED 9819123] out=./Trial15\\seed_9819123\n",
      "==============================\n",
      "[SEED 9819123] [001/300] train_mse_norm=0.008266 | val_rmse_norm=0.047939 | val_mae_cycles=514.409 | best_val_rmse_norm=0.047939\n",
      "[SEED 9819123] [010/300] train_mse_norm=0.000445 | val_rmse_norm=0.047868 | val_mae_cycles=509.901 | best_val_rmse_norm=0.039856\n",
      "[SEED 9819123] [020/300] train_mse_norm=0.001485 | val_rmse_norm=0.048434 | val_mae_cycles=446.065 | best_val_rmse_norm=0.039856\n",
      "[SEED 9819123] [030/300] train_mse_norm=0.000377 | val_rmse_norm=0.049758 | val_mae_cycles=473.080 | best_val_rmse_norm=0.039856\n",
      "[SEED 9819123] Early stopping at epoch 33.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_64376\\4123668076.py:1015: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 9819123] best_by_val_norm: TEST mae_cycles=316.786 | rmse_cycles=464.789 | rmse_norm=0.031608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_64376\\4123668076.py:1015: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 9819123] last_epoch: TEST mae_cycles=262.626 | rmse_cycles=367.972 | rmse_norm=0.029553\n",
      "\n",
      "==============================\n",
      "[SEED 111] device=cuda\n",
      "[SEED 111] out=./Trial15\\seed_111\n",
      "==============================\n",
      "[SEED 111] [001/300] train_mse_norm=0.009536 | val_rmse_norm=0.071118 | val_mae_cycles=1669.901 | best_val_rmse_norm=0.071118\n",
      "[SEED 111] [010/300] train_mse_norm=0.000479 | val_rmse_norm=0.056333 | val_mae_cycles=1097.401 | best_val_rmse_norm=0.051437\n",
      "[SEED 111] [020/300] train_mse_norm=0.000339 | val_rmse_norm=0.063417 | val_mae_cycles=1432.290 | best_val_rmse_norm=0.049318\n",
      "[SEED 111] [030/300] train_mse_norm=0.000208 | val_rmse_norm=0.068207 | val_mae_cycles=1423.052 | best_val_rmse_norm=0.049318\n",
      "[SEED 111] [040/300] train_mse_norm=0.000184 | val_rmse_norm=0.066192 | val_mae_cycles=1452.265 | best_val_rmse_norm=0.049318\n",
      "[SEED 111] Early stopping at epoch 41.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_64376\\4123668076.py:1015: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 111] best_by_val_norm: TEST mae_cycles=361.306 | rmse_cycles=525.208 | rmse_norm=0.033007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_64376\\4123668076.py:1015: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 111] last_epoch: TEST mae_cycles=510.841 | rmse_cycles=926.439 | rmse_norm=0.045829\n",
      "\n",
      "==============================\n",
      "[SEED 222] device=cuda\n",
      "[SEED 222] out=./Trial15\\seed_222\n",
      "==============================\n",
      "[SEED 222] [001/300] train_mse_norm=0.006679 | val_rmse_norm=0.033644 | val_mae_cycles=365.506 | best_val_rmse_norm=0.033644\n",
      "[SEED 222] [010/300] train_mse_norm=0.000407 | val_rmse_norm=0.029170 | val_mae_cycles=261.251 | best_val_rmse_norm=0.026865\n",
      "[SEED 222] [020/300] train_mse_norm=0.000257 | val_rmse_norm=0.029609 | val_mae_cycles=309.499 | best_val_rmse_norm=0.026865\n",
      "[SEED 222] [030/300] train_mse_norm=0.000228 | val_rmse_norm=0.037996 | val_mae_cycles=422.356 | best_val_rmse_norm=0.026865\n",
      "[SEED 222] Early stopping at epoch 36.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_64376\\4123668076.py:1015: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 222] best_by_val_norm: TEST mae_cycles=537.977 | rmse_cycles=668.555 | rmse_norm=0.028186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_64376\\4123668076.py:1015: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 222] last_epoch: TEST mae_cycles=753.693 | rmse_cycles=1012.027 | rmse_norm=0.043656\n",
      "\n",
      "==============================\n",
      "[SEED 333] device=cuda\n",
      "[SEED 333] out=./Trial15\\seed_333\n",
      "==============================\n",
      "[SEED 333] [001/300] train_mse_norm=0.007385 | val_rmse_norm=0.050398 | val_mae_cycles=494.371 | best_val_rmse_norm=0.050398\n",
      "[SEED 333] [010/300] train_mse_norm=0.000465 | val_rmse_norm=0.047068 | val_mae_cycles=570.559 | best_val_rmse_norm=0.040674\n",
      "[SEED 333] [020/300] train_mse_norm=0.000250 | val_rmse_norm=0.046578 | val_mae_cycles=547.267 | best_val_rmse_norm=0.040674\n",
      "[SEED 333] [030/300] train_mse_norm=0.000186 | val_rmse_norm=0.048640 | val_mae_cycles=535.462 | best_val_rmse_norm=0.040674\n",
      "[SEED 333] Early stopping at epoch 33.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_64376\\4123668076.py:1015: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 333] best_by_val_norm: TEST mae_cycles=127.608 | rmse_cycles=166.727 | rmse_norm=0.030715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_64376\\4123668076.py:1015: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 333] last_epoch: TEST mae_cycles=134.520 | rmse_cycles=222.559 | rmse_norm=0.032829\n",
      "\n",
      "==============================\n",
      "[SEED 444] device=cuda\n",
      "[SEED 444] out=./Trial15\\seed_444\n",
      "==============================\n",
      "[SEED 444] [001/300] train_mse_norm=0.007062 | val_rmse_norm=0.044023 | val_mae_cycles=695.898 | best_val_rmse_norm=0.044023\n",
      "[SEED 444] [010/300] train_mse_norm=0.000450 | val_rmse_norm=0.028685 | val_mae_cycles=426.315 | best_val_rmse_norm=0.028685\n",
      "[SEED 444] [020/300] train_mse_norm=0.000250 | val_rmse_norm=0.032803 | val_mae_cycles=361.576 | best_val_rmse_norm=0.028338\n",
      "[SEED 444] [030/300] train_mse_norm=0.000197 | val_rmse_norm=0.035868 | val_mae_cycles=484.318 | best_val_rmse_norm=0.028338\n",
      "[SEED 444] [040/300] train_mse_norm=0.000200 | val_rmse_norm=0.031979 | val_mae_cycles=393.769 | best_val_rmse_norm=0.025454\n",
      "[SEED 444] [050/300] train_mse_norm=0.000149 | val_rmse_norm=0.033058 | val_mae_cycles=398.316 | best_val_rmse_norm=0.025454\n",
      "[SEED 444] [060/300] train_mse_norm=0.000167 | val_rmse_norm=0.032897 | val_mae_cycles=403.574 | best_val_rmse_norm=0.025454\n",
      "[SEED 444] Early stopping at epoch 62.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_64376\\4123668076.py:1015: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 444] best_by_val_norm: TEST mae_cycles=423.602 | rmse_cycles=601.255 | rmse_norm=0.032219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11HOME_AHCI\\AppData\\Local\\Temp\\ipykernel_64376\\4123668076.py:1015: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEED 444] last_epoch: TEST mae_cycles=454.816 | rmse_cycles=639.163 | rmse_norm=0.036382\n",
      "=== WIN-RATE SUMMARY (TEST; lower is better) ===\n",
      "- test_mae_cycles: last wins=1, best wins=4, ties=0 | mean(last-best)=69.843346, std(last-best)=98.447024\n",
      "- test_rmse_cycles: last wins=1, best wins=4, ties=0 | mean(last-best)=148.325030, std(last-best)=191.251301\n",
      "- test_mae_norm: last wins=2, best wins=3, ties=0 | mean(last-best)=0.003605, std(last-best)=0.004254\n",
      "- test_rmse_norm: last wins=1, best wins=4, ties=0 | mean(last-best)=0.006503, std(last-best)=0.006608\n",
      "\n",
      "=== MEAN ± STD across seeds (TEST) ===\n",
      "                 test_mae_cycles             test_rmse_cycles              \\\n",
      "                            mean         std             mean         std   \n",
      "checkpoint                                                                  \n",
      "best_by_val_norm      353.455738  151.147132       485.306981  193.991032   \n",
      "last_epoch            423.299084  238.236420       633.632011  342.237377   \n",
      "\n",
      "                 test_mae_norm           test_rmse_norm            \n",
      "                          mean       std           mean       std  \n",
      "checkpoint                                                         \n",
      "best_by_val_norm      0.025304  0.001040       0.031147  0.001856  \n",
      "last_epoch            0.028910  0.005209       0.037650  0.006953  \n",
      "\n",
      "Saved:\n",
      " - ./Trial15\\summary_across_seeds.csv\n",
      " - ./Trial15\\win_rate_summary.csv\n",
      " - ./Trial15\\win_rate_summary.txt\n",
      "\n",
      "DONE. Check Trial15 folder:\n",
      " - per seed results: Trial15/seed_<seed>/...\n",
      " - figures (paper-style): seed_<seed>/<ckpt>/paper_figures/<split>/\n",
      " - cycle sequence mean CSV: <ckpt>/<split>_cycle_sequence_mean.csv\n",
      " - PH/α–λ metrics CSV: <ckpt>/<split>_prognostics_metrics_per_file.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Trial15: Trial14 + (1) Damage-proxy features + (2) Tail-robust training\n",
    "#\n",
    "# 핵심 아이디어:\n",
    "# - Trial14 결과에서 모델이 tail 구간에 과의존( mask_tail 시 RMSE 폭발 )하는 경향이 강함.\n",
    "# - 해결:\n",
    "#   (1) min_vce만으로도 \"누적 손상(damage)\"을 나타내는 proxy feature 추가 (process 정보)\n",
    "#   (2) 훈련에서 tail 일부를 랜덤하게 마스킹/셔플 -> tail shortcut 방지, tail shift에 강해짐\n",
    "#\n",
    "# Keeps Trial9 evaluation pack: PH / α–λ / CRA / convergence + paper figures\n",
    "#\n",
    "# Folder:\n",
    "#   ./Trial15/seed_<seed>/best_by_val_norm/...\n",
    "#   ./Trial15/seed_<seed>/last_epoch/...\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0) Reproducibility\n",
    "# ============================================================\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Config\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    data_dir: str = r\"C:\\Users\\11HOME_AHCI\\Desktop\\PoF\\Winter 2026\\Ref Data4\\100\"\n",
    "    out_dir: str = r\"./Trial15\"\n",
    "\n",
    "    # seeds to sweep\n",
    "    seeds: Tuple[int, ...] = (9819123, 111, 222, 333, 444)\n",
    "\n",
    "    # sliding window\n",
    "    seq_len: int = 100\n",
    "    stride: int = 5\n",
    "    pred_horizon: int = 0\n",
    "\n",
    "    # split by FILE\n",
    "    train_ratio: float = 0.7\n",
    "    val_ratio: float = 0.2\n",
    "    test_ratio: float = 0.1\n",
    "\n",
    "    # training\n",
    "    batch_size: int = 512\n",
    "    epochs: int = 300\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 0.0\n",
    "    patience: int = 30\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # model\n",
    "    hidden_size: int = 512\n",
    "    num_layers: int = 2\n",
    "    dropout: float = 0.2\n",
    "\n",
    "    # data loading\n",
    "    num_workers: int = 0\n",
    "\n",
    "    # output controls\n",
    "    save_figures: bool = True\n",
    "    max_files_to_plot: Optional[int] = None  # None=all\n",
    "\n",
    "    # ===========================\n",
    "    # Trial9-style Evaluation settings\n",
    "    # ===========================\n",
    "    alpha: float = 0.20\n",
    "    ph_consecutive_m: int = 5\n",
    "    rep_method: str = \"mean\"\n",
    "    lambdas: Tuple[float, ...] = (0.2, 0.4, 0.6, 0.8)\n",
    "    lambda_to_plot: float = 0.6\n",
    "    eps_rul: float = 1e-8\n",
    "\n",
    "    # ===========================\n",
    "    # Trial13 base features (min_vce only)\n",
    "    # ===========================\n",
    "    delta_steps: Tuple[int, ...] = (1, 5, 20, 50)\n",
    "    ema_spans: Tuple[int, ...] = (10, 50)\n",
    "    roll_std_window: int = 10\n",
    "    add_window_stats: bool = True\n",
    "\n",
    "    # ===========================\n",
    "    # Trial15-1: Damage-proxy features (min_vce only)\n",
    "    # ===========================\n",
    "    add_damage_proxy: bool = True\n",
    "    # vce_rel = vce - vce0\n",
    "    # cum_pos = cumsum(max(delta_1,0))\n",
    "    # cum_abs = cumsum(abs(delta_1))\n",
    "    # cum_inc = cumsum(max(vce - vce0,0))\n",
    "    # cum_acc = cumsum(abs(d(delta_1)))  (2차 변화량 누적)\n",
    "\n",
    "    # ===========================\n",
    "    # Temporal Pooling (Trial14 유지)\n",
    "    # ===========================\n",
    "    pooling: str = \"mean_last_k\"   # \"last\" | \"mean_last_k\" | \"attn\"\n",
    "    pool_last_k: int = 10\n",
    "    attn_dim: int = 128\n",
    "\n",
    "    # ===========================\n",
    "    # Trial15-2: Tail-robust training (shortcut 방지)\n",
    "    # ===========================\n",
    "    tail_robust_enable: bool = True\n",
    "    tail_k_max: int = 10                # tail_k ~ Uniform(0..tail_k_max)\n",
    "    tail_apply_p: float = 1.0           # 배치마다 tail perturb 적용 확률\n",
    "    tail_mode: str = \"mask\"             # \"mask\" | \"shuffle\"\n",
    "    tail_mask_value: float = 0.0        # scaled space에서 마스킹 값\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Data utils\n",
    "# ============================================================\n",
    "def list_csv_files(data_dir: str) -> List[Path]:\n",
    "    p = Path(data_dir)\n",
    "    files = sorted([f for f in p.glob(\"*.csv\") if f.is_file()])\n",
    "    if len(files) == 0:\n",
    "        raise FileNotFoundError(f\"No CSV files found in: {data_dir}\")\n",
    "    return files\n",
    "\n",
    "\n",
    "def read_one_csv(csv_path: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(f\"{csv_path.name}: expected at least 2 columns, got {df.shape[1]}\")\n",
    "    vce = df.iloc[:, 0].astype(np.float32).to_numpy()\n",
    "    rul = df.iloc[:, 1].astype(np.float32).to_numpy()\n",
    "\n",
    "    if len(vce) != len(rul):\n",
    "        raise ValueError(f\"{csv_path.name}: length mismatch vce={len(vce)}, rul={len(rul)}\")\n",
    "    if len(vce) < 5:\n",
    "        raise ValueError(f\"{csv_path.name}: too short sequence length={len(vce)}\")\n",
    "    return vce, rul\n",
    "\n",
    "\n",
    "def split_files(\n",
    "    files: List[Path],\n",
    "    train_ratio: float,\n",
    "    val_ratio: float,\n",
    "    test_ratio: float,\n",
    "    seed: int\n",
    ") -> Dict[str, List[Path]]:\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-9\n",
    "\n",
    "    rng = random.Random(seed)\n",
    "    files_shuffled = files[:]\n",
    "    rng.shuffle(files_shuffled)\n",
    "\n",
    "    n = len(files_shuffled)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_val = int(n * val_ratio)\n",
    "\n",
    "    train_files = files_shuffled[:n_train]\n",
    "    val_files = files_shuffled[n_train:n_train + n_val]\n",
    "    test_files = files_shuffled[n_train + n_val:]\n",
    "\n",
    "    return {\"train\": train_files, \"val\": val_files, \"test\": test_files}\n",
    "\n",
    "\n",
    "def delta_k(v: np.ndarray, k: int) -> np.ndarray:\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    if k <= 0:\n",
    "        return out\n",
    "    out[k:] = v[k:] - v[:-k]\n",
    "    return out\n",
    "\n",
    "\n",
    "def ema(v: np.ndarray, span: int) -> np.ndarray:\n",
    "    if span <= 1:\n",
    "        return v.astype(np.float32).copy()\n",
    "    a = 2.0 / (float(span) + 1.0)\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    out[0] = v[0]\n",
    "    for i in range(1, len(v)):\n",
    "        out[i] = a * v[i] + (1.0 - a) * out[i - 1]\n",
    "    return out\n",
    "\n",
    "\n",
    "def rolling_std(v: np.ndarray, w: int) -> np.ndarray:\n",
    "    w = int(w)\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    for i in range(len(v)):\n",
    "        j0 = max(0, i - w + 1)\n",
    "        out[i] = float(np.std(v[j0:i + 1], ddof=0))\n",
    "    return out\n",
    "\n",
    "\n",
    "def _window_slope(seg: np.ndarray) -> float:\n",
    "    L = len(seg)\n",
    "    if L <= 1:\n",
    "        return 0.0\n",
    "    t = np.arange(L, dtype=np.float32)\n",
    "    denom = float(np.var(t) + 1e-12)\n",
    "    if denom <= 0:\n",
    "        return 0.0\n",
    "    return float(np.cov(t, seg, ddof=0)[0, 1] / denom)\n",
    "\n",
    "\n",
    "def feature_names(cfg: Config) -> List[str]:\n",
    "    names = [\"min_vce\"]\n",
    "    for k in cfg.delta_steps:\n",
    "        names.append(f\"delta_{k}\")\n",
    "    for s in cfg.ema_spans:\n",
    "        names.append(f\"ema_{s}\")\n",
    "    if cfg.roll_std_window and cfg.roll_std_window > 1:\n",
    "        names.append(f\"rollstd_{cfg.roll_std_window}\")\n",
    "\n",
    "    if cfg.add_damage_proxy:\n",
    "        names += [\"vce_rel\", \"cum_pos\", \"cum_abs\", \"cum_inc\", \"cum_acc\"]\n",
    "\n",
    "    if cfg.add_window_stats:\n",
    "        names += [\"win_mean\", \"win_std\", \"win_slope\"]\n",
    "    return names\n",
    "\n",
    "\n",
    "def build_features_from_min_vce(vce: np.ndarray, cfg: Config) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return X(T,F) features derived ONLY from min_vce.\n",
    "    (window_stats는 Dataset __getitem__에서 window segment 기준으로 append)\n",
    "    \"\"\"\n",
    "    vce = vce.astype(np.float32)\n",
    "    feats = [vce]  # min_vce\n",
    "\n",
    "    # multi-scale deltas\n",
    "    d1 = None\n",
    "    for k in cfg.delta_steps:\n",
    "        dk = delta_k(vce, int(k))\n",
    "        feats.append(dk)\n",
    "        if int(k) == 1:\n",
    "            d1 = dk\n",
    "\n",
    "    # EMA trends\n",
    "    for s in cfg.ema_spans:\n",
    "        feats.append(ema(vce, int(s)))\n",
    "\n",
    "    # rolling std\n",
    "    if cfg.roll_std_window and cfg.roll_std_window > 1:\n",
    "        feats.append(rolling_std(vce, int(cfg.roll_std_window)))\n",
    "\n",
    "    # damage-proxy features\n",
    "    if cfg.add_damage_proxy:\n",
    "        v0 = float(vce[0])\n",
    "        vce_rel = vce - v0\n",
    "\n",
    "        if d1 is None:\n",
    "            d1 = delta_k(vce, 1)\n",
    "\n",
    "        # 누적 (파일 시작부터의 상태)\n",
    "        cum_pos = np.cumsum(np.maximum(d1, 0.0)).astype(np.float32)\n",
    "        cum_abs = np.cumsum(np.abs(d1)).astype(np.float32)\n",
    "        cum_inc = np.cumsum(np.maximum(vce - v0, 0.0)).astype(np.float32)\n",
    "\n",
    "        dd1 = np.zeros_like(d1, dtype=np.float32)\n",
    "        dd1[1:] = d1[1:] - d1[:-1]\n",
    "        cum_acc = np.cumsum(np.abs(dd1)).astype(np.float32)\n",
    "\n",
    "        feats += [vce_rel.astype(np.float32), cum_pos, cum_abs, cum_inc, cum_acc]\n",
    "\n",
    "    X = np.stack(feats, axis=1).astype(np.float32)  # (T, F_base(+damage))\n",
    "    return X\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Tail-robust augmentation (TRAIN only)\n",
    "# ============================================================\n",
    "def apply_tail_robust(x_scaled: np.ndarray, cfg: Config, rng: np.random.RandomState) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    x_scaled: (L, F) already scaled (StandardScaler)\n",
    "    Randomly perturb the last tail_k timesteps.\n",
    "    \"\"\"\n",
    "    if (not cfg.tail_robust_enable) or cfg.tail_k_max <= 0:\n",
    "        return x_scaled\n",
    "    if rng.rand() > float(cfg.tail_apply_p):\n",
    "        return x_scaled\n",
    "\n",
    "    L, F = x_scaled.shape\n",
    "    if L <= 1:\n",
    "        return x_scaled\n",
    "\n",
    "    k = int(rng.randint(0, int(cfg.tail_k_max) + 1))\n",
    "    if k <= 0:\n",
    "        return x_scaled\n",
    "\n",
    "    x2 = x_scaled.copy()\n",
    "    tail = slice(max(0, L - k), L)\n",
    "\n",
    "    if cfg.tail_mode == \"mask\":\n",
    "        x2[tail, :] = float(cfg.tail_mask_value)\n",
    "    elif cfg.tail_mode == \"shuffle\":\n",
    "        # shuffle within the tail along time (keeps distribution, breaks ordering)\n",
    "        perm = rng.permutation(np.arange(tail.start, tail.stop))\n",
    "        x2[tail, :] = x2[perm, :]\n",
    "    else:\n",
    "        x2[tail, :] = float(cfg.tail_mask_value)\n",
    "\n",
    "    return x2\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Dataset\n",
    "# ============================================================\n",
    "class WindowedRULDatasetNormMinVCE_Trial15(Dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x: (seq_len, F)\n",
    "      y_norm: (1,)\n",
    "      name, start_idx, y_cycles, rul0\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_list: List[Path],\n",
    "        cfg: Config,\n",
    "        scaler_x: StandardScaler = None,\n",
    "        fit_scaler: bool = False,\n",
    "        is_train: bool = False,\n",
    "    ):\n",
    "        self.file_list = file_list\n",
    "        self.cfg = cfg\n",
    "        self.seq_len = cfg.seq_len\n",
    "        self.stride = cfg.stride\n",
    "        self.pred_horizon = cfg.pred_horizon\n",
    "        self.scaler_x = scaler_x if scaler_x is not None else StandardScaler()\n",
    "        self.is_train = bool(is_train)\n",
    "\n",
    "        # deterministic rng per dataset instance (for tail augment)\n",
    "        self._rng = np.random.RandomState(1234 if self.is_train else 4321)\n",
    "\n",
    "        # store: (name, Xbase(T,Fbase), vce(T,), rul(T,), rul0)\n",
    "        self.series: List[Tuple[str, np.ndarray, np.ndarray, np.ndarray, float]] = []\n",
    "        for fp in self.file_list:\n",
    "            vce, rul = read_one_csv(fp)\n",
    "            rul0 = float(rul[0])\n",
    "            if rul0 <= 0:\n",
    "                raise ValueError(f\"{fp.name}: RUL0 must be > 0, got {rul0}\")\n",
    "\n",
    "            Xbase = build_features_from_min_vce(vce, cfg).astype(np.float32)\n",
    "            self.series.append((fp.name, Xbase, vce.astype(np.float32), rul.astype(np.float32), rul0))\n",
    "\n",
    "        # Fit scaler (train only)\n",
    "        if fit_scaler:\n",
    "            if not cfg.add_window_stats:\n",
    "                all_x = np.concatenate([Xbase for _, Xbase, _, _, _ in self.series], axis=0)\n",
    "                self.scaler_x.fit(all_x)\n",
    "            else:\n",
    "                rng = np.random.RandomState(0)\n",
    "                rows = []\n",
    "                max_windows_for_scaler = 5000\n",
    "                for (_name, Xbase, vce_raw, _rul, _rul0) in self.series:\n",
    "                    T = Xbase.shape[0]\n",
    "                    last_start = T - (self.seq_len + self.pred_horizon)\n",
    "                    if last_start < 0:\n",
    "                        continue\n",
    "                    starts = list(range(0, last_start + 1, self.stride))\n",
    "                    if len(starts) == 0:\n",
    "                        continue\n",
    "                    if len(starts) > 200:\n",
    "                        starts = rng.choice(starts, size=200, replace=False).tolist()\n",
    "                    for s in starts:\n",
    "                        xw = Xbase[s:s + self.seq_len, :]  # (L,Fbase)\n",
    "                        seg = vce_raw[s:s + self.seq_len]\n",
    "                        wmean = float(np.mean(seg))\n",
    "                        wstd = float(np.std(seg, ddof=0))\n",
    "                        slope = _window_slope(seg)\n",
    "                        stats = np.array([wmean, wstd, slope], dtype=np.float32).reshape(1, 3)\n",
    "                        stats_rep = np.repeat(stats, repeats=self.seq_len, axis=0)\n",
    "                        xfull = np.concatenate([xw, stats_rep], axis=1)  # (L,F)\n",
    "                        rows.append(xfull)\n",
    "                        if len(rows) >= max_windows_for_scaler:\n",
    "                            break\n",
    "                    if len(rows) >= max_windows_for_scaler:\n",
    "                        break\n",
    "                if len(rows) == 0:\n",
    "                    raise ValueError(\"Scaler fitting failed: no windows sampled. Check seq_len/stride.\")\n",
    "                fit_mat = np.concatenate(rows, axis=0)\n",
    "                self.scaler_x.fit(fit_mat)\n",
    "\n",
    "        # window index\n",
    "        self.index: List[Tuple[int, int]] = []\n",
    "        for fi, (_name, Xbase, _vce, _rul, _rul0) in enumerate(self.series):\n",
    "            T = Xbase.shape[0]\n",
    "            last_start = T - (self.seq_len + self.pred_horizon)\n",
    "            if last_start < 0:\n",
    "                continue\n",
    "            for s in range(0, last_start + 1, self.stride):\n",
    "                self.index.append((fi, s))\n",
    "\n",
    "        if len(self.index) == 0:\n",
    "            raise ValueError(\"No windows were created. Check seq_len/pred_horizon vs file lengths.\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fi, s = self.index[idx]\n",
    "        name, Xbase, vce_raw, rul, rul0 = self.series[fi]\n",
    "\n",
    "        x = Xbase[s:s + self.seq_len, :]  # (L,Fbase)\n",
    "\n",
    "        if self.cfg.add_window_stats:\n",
    "            seg = vce_raw[s:s + self.seq_len]\n",
    "            wmean = float(np.mean(seg))\n",
    "            wstd = float(np.std(seg, ddof=0))\n",
    "            slope = _window_slope(seg)\n",
    "            stats = np.array([wmean, wstd, slope], dtype=np.float32).reshape(1, 3)\n",
    "            stats_rep = np.repeat(stats, repeats=self.seq_len, axis=0)\n",
    "            x = np.concatenate([x, stats_rep], axis=1).astype(np.float32)  # (L,F)\n",
    "\n",
    "        y_idx = s + self.seq_len - 1 + self.pred_horizon\n",
    "        y_cycles = float(rul[y_idx])\n",
    "        y_norm = np.array([y_cycles / rul0], dtype=np.float32)\n",
    "\n",
    "        # scale first\n",
    "        x = self.scaler_x.transform(x).astype(np.float32)\n",
    "\n",
    "        # TRAIN only: tail-robust\n",
    "        if self.is_train and self.cfg.tail_robust_enable:\n",
    "            x = apply_tail_robust(x, self.cfg, self._rng).astype(np.float32)\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x),\n",
    "            torch.from_numpy(y_norm),\n",
    "            name,\n",
    "            torch.tensor(s, dtype=torch.long),\n",
    "            torch.tensor(y_cycles, dtype=torch.float32),\n",
    "            torch.tensor(rul0, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Model (Temporal pooling 유지)\n",
    "# ============================================================\n",
    "class TemporalPool(nn.Module):\n",
    "    def __init__(self, mode: str, hidden_size: int, last_k: int = 10, attn_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.mode = str(mode)\n",
    "        self.last_k = int(last_k)\n",
    "        self.attn_dim = int(attn_dim)\n",
    "\n",
    "        if self.mode == \"attn\":\n",
    "            self.proj = nn.Sequential(\n",
    "                nn.Linear(hidden_size, self.attn_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(self.attn_dim, 1),\n",
    "            )\n",
    "\n",
    "    def forward(self, h: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        h: (B, L, H)\n",
    "        returns pooled: (B, H)\n",
    "        \"\"\"\n",
    "        if self.mode == \"last\":\n",
    "            return h[:, -1, :]\n",
    "\n",
    "        if self.mode == \"mean_last_k\":\n",
    "            L = h.size(1)\n",
    "            k = min(self.last_k, L)\n",
    "            return torch.mean(h[:, -k:, :], dim=1)\n",
    "\n",
    "        if self.mode == \"attn\":\n",
    "            scores = self.proj(h)              # (B, L, 1)\n",
    "            w = torch.softmax(scores, dim=1)   # (B, L, 1)\n",
    "            return torch.sum(w * h, dim=1)     # (B, H)\n",
    "\n",
    "        return h[:, -1, :]\n",
    "\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout: float,\n",
    "                 pooling: str, pool_last_k: int, attn_dim: int):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.pool = TemporalPool(pooling, hidden_size, last_k=pool_last_k, attn_dim=attn_dim)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, _ = self.lstm(x)     # (B, L, H)\n",
    "        z = self.pool(h)        # (B, H)\n",
    "        return self.head(z)     # (B, 1) norm-scale\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) Basic Eval + Save window-level predictions\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def evaluate_basic(model, loader, device) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "\n",
    "    mae_norm_list, mse_norm_list = [], []\n",
    "    mae_cyc_list, mse_cyc_list = [], []\n",
    "\n",
    "    for x, y_norm, _name, _s, y_cycles, rul0 in loader:\n",
    "        x = x.to(device)\n",
    "        y_norm = y_norm.to(device)\n",
    "        y_cycles = y_cycles.to(device).view(-1, 1)\n",
    "        rul0 = rul0.to(device).view(-1, 1)\n",
    "\n",
    "        pred_norm = model(x)\n",
    "\n",
    "        err_norm = pred_norm - y_norm\n",
    "        mae_norm_list.append(torch.mean(torch.abs(err_norm)).item())\n",
    "        mse_norm_list.append(torch.mean(err_norm ** 2).item())\n",
    "\n",
    "        pred_cycles = pred_norm * rul0\n",
    "        err_cyc = pred_cycles - y_cycles\n",
    "        mae_cyc_list.append(torch.mean(torch.abs(err_cyc)).item())\n",
    "        mse_cyc_list.append(torch.mean(err_cyc ** 2).item())\n",
    "\n",
    "    return {\n",
    "        \"mae_norm\": float(np.mean(mae_norm_list)) if mae_norm_list else float(\"nan\"),\n",
    "        \"rmse_norm\": float(np.sqrt(np.mean(mse_norm_list))) if mse_norm_list else float(\"nan\"),\n",
    "        \"mae_cycles\": float(np.mean(mae_cyc_list)) if mae_cyc_list else float(\"nan\"),\n",
    "        \"rmse_cycles\": float(np.sqrt(np.mean(mse_cyc_list))) if mse_cyc_list else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_predictions_windows_csv(model, loader, device, out_csv: str, seq_len: int) -> None:\n",
    "    model.eval()\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for x, y_norm, name, s, y_cycles, rul0 in loader:\n",
    "        x = x.to(device)\n",
    "        y_norm = y_norm.to(device)\n",
    "        y_cycles = y_cycles.to(device).view(-1, 1)\n",
    "        rul0 = rul0.to(device).view(-1, 1)\n",
    "\n",
    "        pred_norm = model(x)\n",
    "        pred_cycles = pred_norm * rul0\n",
    "\n",
    "        pred_norm_np = pred_norm.cpu().numpy().reshape(-1)\n",
    "        y_norm_np = y_norm.cpu().numpy().reshape(-1)\n",
    "        pred_cyc_np = pred_cycles.cpu().numpy().reshape(-1)\n",
    "        y_cyc_np = y_cycles.cpu().numpy().reshape(-1)\n",
    "\n",
    "        rul0_np = rul0.cpu().numpy().reshape(-1)\n",
    "        s_np = s.cpu().numpy().reshape(-1)\n",
    "        name_list = list(name)\n",
    "\n",
    "        for i in range(len(pred_norm_np)):\n",
    "            rows.append({\n",
    "                \"file\": name_list[i],\n",
    "                \"start_idx\": int(s_np[i]),\n",
    "                \"cycle\": int(s_np[i] + (seq_len - 1)),\n",
    "                \"rul0\": float(rul0_np[i]),\n",
    "                \"RUL_true\": float(y_cyc_np[i]),\n",
    "                \"RUL_pred\": float(pred_cyc_np[i]),\n",
    "                \"RUL_true_norm\": float(y_norm_np[i]),\n",
    "                \"RUL_pred_norm\": float(pred_norm_np[i]),\n",
    "            })\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) Window -> Cycle sequence (mean representative)\n",
    "# ============================================================\n",
    "def windows_to_cycle_sequence_mean(windows_csv: str) -> pd.DataFrame:\n",
    "    dfw = pd.read_csv(windows_csv)\n",
    "    if dfw.empty:\n",
    "        raise ValueError(f\"Empty windows csv: {windows_csv}\")\n",
    "\n",
    "    g = dfw.groupby([\"file\", \"cycle\"], as_index=False).agg(\n",
    "        rul0=(\"rul0\", \"first\"),\n",
    "        RUL_true=(\"RUL_true\", \"mean\"),\n",
    "        RUL_pred=(\"RUL_pred\", \"mean\"),\n",
    "        n_windows=(\"RUL_pred\", \"count\"),\n",
    "    )\n",
    "    return g\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) Prognostics metrics (same as Trial9)\n",
    "# ============================================================\n",
    "def compute_metrics_for_one_file(\n",
    "    df_seq_one_file: pd.DataFrame,\n",
    "    seq_len: int,\n",
    "    alpha: float,\n",
    "    ph_consecutive_m: int,\n",
    "    lambdas: Tuple[float, ...],\n",
    "    eps_rul: float,\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "\n",
    "    df = df_seq_one_file.sort_values(\"cycle\").reset_index(drop=True).copy()\n",
    "\n",
    "    t_s = seq_len - 1\n",
    "    last_cycle = int(df[\"cycle\"].max())\n",
    "    EOL_true = last_cycle + 1\n",
    "    t_e = EOL_true - 1\n",
    "\n",
    "    df_eval = df[(df[\"cycle\"] >= t_s) & (df[\"cycle\"] <= t_e)].copy()\n",
    "    df_eval.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if df_eval.empty:\n",
    "        summary = {\n",
    "            \"t_s\": t_s, \"t_e\": t_e, \"EOL_true\": EOL_true,\n",
    "            \"PH\": np.nan, \"t_PH_start\": np.nan,\n",
    "            \"CRA\": np.nan, \"Convergence_cycles\": np.nan,\n",
    "        }\n",
    "        for lam in lambdas:\n",
    "            summary[f\"t_lambda_{lam:.2f}\"] = np.nan\n",
    "            summary[f\"alpha_lambda_ok_{lam:.2f}\"] = np.nan\n",
    "        return df_eval, summary\n",
    "\n",
    "    denom = np.maximum(np.abs(df_eval[\"RUL_true\"].values), eps_rul)\n",
    "    rel_err = np.abs(df_eval[\"RUL_true\"].values - df_eval[\"RUL_pred\"].values) / denom\n",
    "    RA = 1.0 - rel_err\n",
    "\n",
    "    df_eval[\"rel_err\"] = rel_err\n",
    "    df_eval[\"RA\"] = RA\n",
    "    df_eval[\"in_alpha\"] = df_eval[\"rel_err\"] <= alpha\n",
    "\n",
    "    CRA = float(np.mean(df_eval[\"RA\"].values))\n",
    "\n",
    "    flags = df_eval[\"in_alpha\"].values.astype(np.int32)\n",
    "    t_PH_start = np.nan\n",
    "    if len(flags) >= ph_consecutive_m:\n",
    "        run = 0\n",
    "        for i, ok in enumerate(flags):\n",
    "            if ok:\n",
    "                run += 1\n",
    "                if run >= ph_consecutive_m:\n",
    "                    start_i = i - ph_consecutive_m + 1\n",
    "                    t_PH_start = int(df_eval.loc[start_i, \"cycle\"])\n",
    "                    break\n",
    "            else:\n",
    "                run = 0\n",
    "\n",
    "    if np.isfinite(t_PH_start):\n",
    "        PH = float(EOL_true - t_PH_start)\n",
    "        Convergence_cycles = float(t_PH_start - t_s)\n",
    "    else:\n",
    "        PH = np.nan\n",
    "        Convergence_cycles = np.nan\n",
    "\n",
    "    rul0 = float(df_eval[\"rul0\"].iloc[0])\n",
    "    lam_results = {}\n",
    "    for lam in lambdas:\n",
    "        target_rul = (1.0 - float(lam)) * rul0\n",
    "        idx = int(np.argmin(np.abs(df_eval[\"RUL_true\"].values - target_rul)))\n",
    "        t_lam = int(df_eval.loc[idx, \"cycle\"])\n",
    "        ok = bool(df_eval.loc[idx, \"rel_err\"] <= alpha)\n",
    "\n",
    "        lam_results[f\"t_lambda_{lam:.2f}\"] = t_lam\n",
    "        lam_results[f\"alpha_lambda_ok_{lam:.2f}\"] = int(ok)\n",
    "\n",
    "    summary = {\n",
    "        \"t_s\": int(t_s),\n",
    "        \"t_e\": int(t_e),\n",
    "        \"EOL_true\": int(EOL_true),\n",
    "        \"alpha\": float(alpha),\n",
    "        \"ph_consecutive_m\": int(ph_consecutive_m),\n",
    "        \"CRA\": CRA,\n",
    "        \"t_PH_start\": t_PH_start if np.isfinite(t_PH_start) else np.nan,\n",
    "        \"PH\": PH,\n",
    "        \"Convergence_cycles\": Convergence_cycles,\n",
    "        **lam_results\n",
    "    }\n",
    "    return df_eval, summary\n",
    "\n",
    "\n",
    "def compute_metrics_from_windows_csv(\n",
    "    windows_csv: str,\n",
    "    seq_len: int,\n",
    "    alpha: float,\n",
    "    ph_consecutive_m: int,\n",
    "    lambdas: Tuple[float, ...],\n",
    "    eps_rul: float,\n",
    "    out_dir: str,\n",
    "    split_name: str,\n",
    ") -> Tuple[str, str]:\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    df_seq = windows_to_cycle_sequence_mean(windows_csv)\n",
    "    seq_path = os.path.join(out_dir, f\"{split_name}_cycle_sequence_mean.csv\")\n",
    "    df_seq.to_csv(seq_path, index=False)\n",
    "\n",
    "    rows = []\n",
    "    for f in df_seq[\"file\"].unique():\n",
    "        sub = df_seq[df_seq[\"file\"] == f].copy()\n",
    "        _df_eval, summary = compute_metrics_for_one_file(\n",
    "            df_seq_one_file=sub,\n",
    "            seq_len=seq_len,\n",
    "            alpha=alpha,\n",
    "            ph_consecutive_m=ph_consecutive_m,\n",
    "            lambdas=lambdas,\n",
    "            eps_rul=eps_rul,\n",
    "        )\n",
    "        summary[\"file\"] = f\n",
    "        rows.append(summary)\n",
    "\n",
    "    dfm = pd.DataFrame(rows)\n",
    "    metrics_path = os.path.join(out_dir, f\"{split_name}_prognostics_metrics_per_file.csv\")\n",
    "    dfm.to_csv(metrics_path, index=False)\n",
    "\n",
    "    return seq_path, metrics_path\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) Plotters (same as Trial14)\n",
    "# ============================================================\n",
    "def _safe_name(s: str) -> str:\n",
    "    return s.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\")\n",
    "\n",
    "\n",
    "def plot_alpha_ph(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    title: str,\n",
    "    alpha: float,\n",
    "    PH_start: Optional[float],\n",
    "    out_path: str,\n",
    "    dpi: int = 200,\n",
    ") -> None:\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    plt.figure()\n",
    "\n",
    "    x = df_eval[\"cycle\"].values\n",
    "    y_true = df_eval[\"RUL_true\"].values\n",
    "    y_pred = df_eval[\"RUL_pred\"].values\n",
    "\n",
    "    upper = y_true * (1.0 + alpha)\n",
    "    lower = y_true * (1.0 - alpha)\n",
    "\n",
    "    plt.plot(x, y_true, color=\"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, color=\"r\", label=\"Prediction (cycles)\")\n",
    "    plt.plot(x, upper, color=\"b\", linestyle=\"--\", label=f\"+{alpha:.2f} alpha accuracy zone\")\n",
    "    plt.plot(x, lower, color=\"b\", linestyle=\"--\", label=f\"-{alpha:.2f} alpha accuracy zone\")\n",
    "\n",
    "    if PH_start is not None and np.isfinite(PH_start):\n",
    "        plt.axvline(int(PH_start), color=\"g\", linestyle=\"-.\", label=\"PH start\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title}\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_alpha_lambda(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    title: str,\n",
    "    alpha: float,\n",
    "    lambda_to_plot: float,\n",
    "    t_lambda: Optional[int],\n",
    "    out_path: str,\n",
    "    dpi: int = 200,\n",
    ") -> None:\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    x = df_eval[\"cycle\"].values\n",
    "    y_true = df_eval[\"RUL_true\"].values\n",
    "    y_pred = df_eval[\"RUL_pred\"].values\n",
    "\n",
    "    upper = y_true * (1.0 + alpha)\n",
    "    lower = y_true * (1.0 - alpha)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y_true, color=\"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, color=\"r\", label=\"Prediction (cycles)\")\n",
    "\n",
    "    if t_lambda is not None and np.isfinite(t_lambda):\n",
    "        t_lambda = int(t_lambda)\n",
    "        plt.axvline(t_lambda, linestyle=\":\", color=\"g\", label=f\"t_λ (λ={lambda_to_plot:.2f})\")\n",
    "        mask = x >= t_lambda\n",
    "        if np.any(mask):\n",
    "            plt.plot(x[mask], upper[mask], color=\"b\", linestyle=\"--\", label=f\"+{alpha:.2f} alpha–lambda zone\")\n",
    "            plt.plot(x[mask], lower[mask], color=\"b\", linestyle=\"--\", label=f\"-{alpha:.2f} alpha–lambda zone\")\n",
    "        else:\n",
    "            plt.plot(x, upper, color=\"b\", linestyle=\"--\", label=f\"+{alpha:.2f} α zone\")\n",
    "            plt.plot(x, lower, color=\"b\", linestyle=\"--\", label=f\"-{alpha:.2f} α zone\")\n",
    "    else:\n",
    "        plt.plot(x, upper, color=\"b\", linestyle=\"--\", label=f\"+{alpha:.2f} α zone\")\n",
    "        plt.plot(x, lower, color=\"b\", linestyle=\"--\", label=f\"-{alpha:.2f} α zone\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title}\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def make_paper_figures_for_split(\n",
    "    cycle_seq_csv: str,\n",
    "    metrics_per_file_csv: str,\n",
    "    out_fig_dir: str,\n",
    "    title_prefix: str,\n",
    "    alpha: float,\n",
    "    lambda_to_plot: float,\n",
    "    max_files: Optional[int] = None,\n",
    "    dpi: int = 200,\n",
    ") -> None:\n",
    "    df_seq = pd.read_csv(cycle_seq_csv)\n",
    "    dfm = pd.read_csv(metrics_per_file_csv)\n",
    "\n",
    "    files = df_seq[\"file\"].unique().tolist()\n",
    "    if max_files is not None:\n",
    "        files = files[:max_files]\n",
    "\n",
    "    os.makedirs(out_fig_dir, exist_ok=True)\n",
    "\n",
    "    lam_key = f\"t_lambda_{lambda_to_plot:.2f}\"\n",
    "\n",
    "    for f in files:\n",
    "        sub = df_seq[df_seq[\"file\"] == f].sort_values(\"cycle\").copy()\n",
    "        mrow = dfm[dfm[\"file\"] == f]\n",
    "        if mrow.empty:\n",
    "            continue\n",
    "        mrow = mrow.iloc[0].to_dict()\n",
    "\n",
    "        t_s = int(mrow[\"t_s\"])\n",
    "        t_e = int(mrow[\"t_e\"])\n",
    "        PH_start = mrow.get(\"t_PH_start\", np.nan)\n",
    "\n",
    "        df_eval = sub[(sub[\"cycle\"] >= t_s) & (sub[\"cycle\"] <= t_e)].copy()\n",
    "        if df_eval.empty:\n",
    "            continue\n",
    "\n",
    "        t_lambda = None\n",
    "        if lam_key in mrow and np.isfinite(mrow[lam_key]):\n",
    "            t_lambda = int(mrow[lam_key])\n",
    "\n",
    "        safe = _safe_name(f)\n",
    "\n",
    "        out1 = os.path.join(out_fig_dir, f\"FIG1_alpha_PH__{safe}.png\")\n",
    "        plot_alpha_ph(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            title=f\"{title_prefix} | α+PH\",\n",
    "            alpha=alpha,\n",
    "            PH_start=PH_start if np.isfinite(PH_start) else None,\n",
    "            out_path=out1,\n",
    "            dpi=dpi,\n",
    "        )\n",
    "\n",
    "        out2 = os.path.join(out_fig_dir, f\"FIG2_alpha_lambda__lam{lambda_to_plot:.2f}__{safe}.png\")\n",
    "        plot_alpha_lambda(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            title=f\"{title_prefix} | α–λ (λ={lambda_to_plot:.2f})\",\n",
    "            alpha=alpha,\n",
    "            lambda_to_plot=lambda_to_plot,\n",
    "            t_lambda=t_lambda,\n",
    "            out_path=out2,\n",
    "            dpi=dpi,\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10) One seed run\n",
    "# ============================================================\n",
    "def run_one_seed(cfg: Config, seed: int) -> Dict[str, Any]:\n",
    "    set_seed(seed)\n",
    "\n",
    "    seed_dir = os.path.join(cfg.out_dir, f\"seed_{seed}\")\n",
    "    os.makedirs(seed_dir, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"[SEED {seed}] device={device}\")\n",
    "    print(f\"[SEED {seed}] out={seed_dir}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    # split\n",
    "    files = list_csv_files(cfg.data_dir)\n",
    "    splits = split_files(files, cfg.train_ratio, cfg.val_ratio, cfg.test_ratio, seed)\n",
    "\n",
    "    # save split lists\n",
    "    for k in [\"train\", \"val\", \"test\"]:\n",
    "        pd.Series([p.name for p in splits[k]]).to_csv(\n",
    "            os.path.join(seed_dir, f\"{k}_files.csv\"), index=False, header=False\n",
    "        )\n",
    "\n",
    "    # datasets (fit scaler on train only)\n",
    "    scaler_x = StandardScaler()\n",
    "    train_ds = WindowedRULDatasetNormMinVCE_Trial15(\n",
    "        splits[\"train\"], cfg, scaler_x=scaler_x, fit_scaler=True, is_train=True\n",
    "    )\n",
    "    val_ds = WindowedRULDatasetNormMinVCE_Trial15(\n",
    "        splits[\"val\"], cfg, scaler_x=train_ds.scaler_x, fit_scaler=False, is_train=False\n",
    "    )\n",
    "    test_ds = WindowedRULDatasetNormMinVCE_Trial15(\n",
    "        splits[\"test\"], cfg, scaler_x=train_ds.scaler_x, fit_scaler=False, is_train=False\n",
    "    )\n",
    "\n",
    "    feat_list = feature_names(cfg)\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feat_list,\n",
    "        \"mean\": train_ds.scaler_x.mean_.ravel(),\n",
    "        \"std\": np.sqrt(train_ds.scaler_x.var_).ravel(),\n",
    "    }).to_csv(os.path.join(seed_dir, \"scaler_x_mean_std.csv\"), index=False)\n",
    "\n",
    "    # loaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "\n",
    "    train_eval = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "    val_eval = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "    test_eval = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "\n",
    "    # model\n",
    "    input_size = len(feat_list)\n",
    "    model = LSTMRegressor(\n",
    "        input_size=input_size,\n",
    "        hidden_size=cfg.hidden_size,\n",
    "        num_layers=cfg.num_layers,\n",
    "        dropout=cfg.dropout,\n",
    "        pooling=cfg.pooling,\n",
    "        pool_last_k=cfg.pool_last_k,\n",
    "        attn_dim=cfg.attn_dim,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    best_by_val_norm = float(\"inf\")\n",
    "    best_path = os.path.join(seed_dir, \"best_by_val_norm.pt\")\n",
    "    last_path = os.path.join(seed_dir, \"last_epoch.pt\")\n",
    "\n",
    "    history: List[Dict[str, Any]] = []\n",
    "    bad_epochs = 0\n",
    "\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        model.train()\n",
    "        losses = []\n",
    "\n",
    "        for x, y_norm, *_ in train_loader:\n",
    "            x = x.to(device)\n",
    "            y_norm = y_norm.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred_norm = model(x)\n",
    "            loss = criterion(pred_norm, y_norm)\n",
    "            loss.backward()\n",
    "\n",
    "            if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        train_mse_norm = float(np.mean(losses)) if losses else float(\"nan\")\n",
    "        val_metrics = evaluate_basic(model, val_loader, device)\n",
    "\n",
    "        history.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_mse_norm\": train_mse_norm,\n",
    "            \"val_rmse_norm\": val_metrics[\"rmse_norm\"],\n",
    "            \"val_mae_norm\": val_metrics[\"mae_norm\"],\n",
    "            \"val_rmse_cycles\": val_metrics[\"rmse_cycles\"],\n",
    "            \"val_mae_cycles\": val_metrics[\"mae_cycles\"],\n",
    "        })\n",
    "\n",
    "        if val_metrics[\"rmse_norm\"] < best_by_val_norm:\n",
    "            best_by_val_norm = val_metrics[\"rmse_norm\"]\n",
    "            bad_epochs = 0\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(\n",
    "                f\"[SEED {seed}] [{epoch:03d}/{cfg.epochs}] \"\n",
    "                f\"train_mse_norm={train_mse_norm:.6f} | \"\n",
    "                f\"val_rmse_norm={val_metrics['rmse_norm']:.6f} | \"\n",
    "                f\"val_mae_cycles={val_metrics['mae_cycles']:.3f} | \"\n",
    "                f\"best_val_rmse_norm={best_by_val_norm:.6f}\"\n",
    "            )\n",
    "\n",
    "        if bad_epochs >= cfg.patience:\n",
    "            print(f\"[SEED {seed}] Early stopping at epoch {epoch}.\")\n",
    "            break\n",
    "\n",
    "    pd.DataFrame(history).to_csv(os.path.join(seed_dir, \"history.csv\"), index=False)\n",
    "    torch.save(model.state_dict(), last_path)\n",
    "\n",
    "    def export_ckpt(tag: str, ckpt_path: str) -> Dict[str, Any]:\n",
    "        sub_dir = os.path.join(seed_dir, tag)\n",
    "        os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "        model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "        model.eval()\n",
    "\n",
    "        tr = evaluate_basic(model, train_eval, device)\n",
    "        va = evaluate_basic(model, val_eval, device)\n",
    "        te = evaluate_basic(model, test_eval, device)\n",
    "\n",
    "        for split_name, loader in [(\"train\", train_eval), (\"val\", val_eval), (\"test\", test_eval)]:\n",
    "            win_csv = os.path.join(sub_dir, f\"{split_name}_predictions_windows.csv\")\n",
    "            save_predictions_windows_csv(model, loader, device, win_csv, seq_len=cfg.seq_len)\n",
    "\n",
    "            seq_csv, metrics_csv = compute_metrics_from_windows_csv(\n",
    "                windows_csv=win_csv,\n",
    "                seq_len=cfg.seq_len,\n",
    "                alpha=cfg.alpha,\n",
    "                ph_consecutive_m=cfg.ph_consecutive_m,\n",
    "                lambdas=cfg.lambdas,\n",
    "                eps_rul=cfg.eps_rul,\n",
    "                out_dir=sub_dir,\n",
    "                split_name=split_name,\n",
    "            )\n",
    "\n",
    "            if cfg.save_figures:\n",
    "                fig_dir = os.path.join(sub_dir, \"paper_figures\", split_name)\n",
    "                make_paper_figures_for_split(\n",
    "                    cycle_seq_csv=seq_csv,\n",
    "                    metrics_per_file_csv=metrics_csv,\n",
    "                    out_fig_dir=fig_dir,\n",
    "                    title_prefix=f\"SEED {seed} | {tag.upper()} | {split_name}\",\n",
    "                    alpha=cfg.alpha,\n",
    "                    lambda_to_plot=cfg.lambda_to_plot,\n",
    "                    max_files=cfg.max_files_to_plot,\n",
    "                )\n",
    "\n",
    "        ms = {\n",
    "            \"seed\": seed,\n",
    "            \"checkpoint\": tag,\n",
    "\n",
    "            \"train_rmse_cycles\": tr[\"rmse_cycles\"],\n",
    "            \"train_mae_cycles\": tr[\"mae_cycles\"],\n",
    "            \"train_rmse_norm\": tr[\"rmse_norm\"],\n",
    "            \"train_mae_norm\": tr[\"mae_norm\"],\n",
    "\n",
    "            \"val_rmse_cycles\": va[\"rmse_cycles\"],\n",
    "            \"val_mae_cycles\": va[\"mae_cycles\"],\n",
    "            \"val_rmse_norm\": va[\"rmse_norm\"],\n",
    "            \"val_mae_norm\": va[\"mae_norm\"],\n",
    "\n",
    "            \"test_rmse_cycles\": te[\"rmse_cycles\"],\n",
    "            \"test_mae_cycles\": te[\"mae_cycles\"],\n",
    "            \"test_rmse_norm\": te[\"rmse_norm\"],\n",
    "            \"test_mae_norm\": te[\"mae_norm\"],\n",
    "\n",
    "            \"stopped_epoch\": history[-1][\"epoch\"] if len(history) else None,\n",
    "            \"best_val_rmse_norm\": best_by_val_norm,\n",
    "\n",
    "            \"alpha\": cfg.alpha,\n",
    "            \"ph_consecutive_m\": cfg.ph_consecutive_m,\n",
    "            \"rep_method\": cfg.rep_method,\n",
    "            \"lambdas\": str(cfg.lambdas),\n",
    "            \"lambda_to_plot\": cfg.lambda_to_plot,\n",
    "\n",
    "            \"feature_dim\": input_size,\n",
    "            \"features\": \",\".join(feature_names(cfg)),\n",
    "\n",
    "            # Trial15 extras\n",
    "            \"pooling\": cfg.pooling,\n",
    "            \"pool_last_k\": cfg.pool_last_k,\n",
    "            \"tail_robust_enable\": int(cfg.tail_robust_enable),\n",
    "            \"tail_k_max\": cfg.tail_k_max,\n",
    "            \"tail_apply_p\": cfg.tail_apply_p,\n",
    "            \"tail_mode\": cfg.tail_mode,\n",
    "        }\n",
    "        pd.DataFrame([ms]).to_csv(os.path.join(sub_dir, \"metrics_summary.csv\"), index=False)\n",
    "\n",
    "        print(\n",
    "            f\"[SEED {seed}] {tag}: TEST mae_cycles={te['mae_cycles']:.3f} | \"\n",
    "            f\"rmse_cycles={te['rmse_cycles']:.3f} | rmse_norm={te['rmse_norm']:.6f}\"\n",
    "        )\n",
    "        return ms\n",
    "\n",
    "    ms_best = export_ckpt(\"best_by_val_norm\", best_path)\n",
    "    ms_last = export_ckpt(\"last_epoch\", last_path)\n",
    "\n",
    "    return {\"seed\": seed, \"seed_dir\": seed_dir, \"best\": ms_best, \"last\": ms_last}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 11) Seed sweep + global comparison\n",
    "# ============================================================\n",
    "def summarize_across_seeds(cfg: Config, results: List[Dict[str, Any]]) -> None:\n",
    "    rows = []\n",
    "    for r in results:\n",
    "        rows.append(r[\"best\"])\n",
    "        rows.append(r[\"last\"])\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(os.path.join(cfg.out_dir, \"summary_across_seeds.csv\"), index=False)\n",
    "\n",
    "    def _isfinite(x: Any) -> bool:\n",
    "        try:\n",
    "            return bool(np.isfinite(float(x)))\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def win_rate(metric: str) -> Dict[str, Any]:\n",
    "        wins_last = 0\n",
    "        wins_best = 0\n",
    "        ties = 0\n",
    "        diffs = []\n",
    "\n",
    "        for r in results:\n",
    "            b = r[\"best\"][metric]\n",
    "            l = r[\"last\"][metric]\n",
    "            if _isfinite(b) and _isfinite(l):\n",
    "                diffs.append(float(l) - float(b))\n",
    "                if float(l) < float(b):\n",
    "                    wins_last += 1\n",
    "                elif float(b) < float(l):\n",
    "                    wins_best += 1\n",
    "                else:\n",
    "                    ties += 1\n",
    "\n",
    "        return {\n",
    "            \"metric\": metric,\n",
    "            \"wins_last\": wins_last,\n",
    "            \"wins_best\": wins_best,\n",
    "            \"ties\": ties,\n",
    "            \"mean(last-best)\": float(np.mean(diffs)) if diffs else float(\"nan\"),\n",
    "            \"std(last-best)\": float(np.std(diffs, ddof=0)) if diffs else float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    metrics = [\"test_mae_cycles\", \"test_rmse_cycles\", \"test_mae_norm\", \"test_rmse_norm\"]\n",
    "    wr = [win_rate(m) for m in metrics]\n",
    "    pd.DataFrame(wr).to_csv(os.path.join(cfg.out_dir, \"win_rate_summary.csv\"), index=False)\n",
    "\n",
    "    lines = []\n",
    "    lines.append(\"=== WIN-RATE SUMMARY (TEST; lower is better) ===\")\n",
    "    for row in wr:\n",
    "        lines.append(\n",
    "            f\"- {row['metric']}: last wins={row['wins_last']}, best wins={row['wins_best']}, ties={row['ties']} | \"\n",
    "            f\"mean(last-best)={row['mean(last-best)']:.6f}, std(last-best)={row['std(last-best)']:.6f}\"\n",
    "        )\n",
    "\n",
    "    agg = df.groupby(\"checkpoint\")[metrics].agg([\"mean\", \"std\"])\n",
    "    lines.append(\"\\n=== MEAN ± STD across seeds (TEST) ===\")\n",
    "    lines.append(str(agg))\n",
    "\n",
    "    with open(os.path.join(cfg.out_dir, \"win_rate_summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nSaved:\")\n",
    "    print(\" -\", os.path.join(cfg.out_dir, \"summary_across_seeds.csv\"))\n",
    "    print(\" -\", os.path.join(cfg.out_dir, \"win_rate_summary.csv\"))\n",
    "    print(\" -\", os.path.join(cfg.out_dir, \"win_rate_summary.txt\"))\n",
    "\n",
    "\n",
    "def run_trial15_seed_sweep(cfg: Config) -> None:\n",
    "    os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "\n",
    "    results = []\n",
    "    for seed in cfg.seeds:\n",
    "        res = run_one_seed(cfg, seed)\n",
    "        results.append(res)\n",
    "\n",
    "    summarize_across_seeds(cfg, results)\n",
    "\n",
    "    print(\"\\nDONE. Check Trial15 folder:\")\n",
    "    print(\" - per seed results: Trial15/seed_<seed>/...\")\n",
    "    print(\" - figures (paper-style): seed_<seed>/<ckpt>/paper_figures/<split>/\")\n",
    "    print(\" - cycle sequence mean CSV: <ckpt>/<split>_cycle_sequence_mean.csv\")\n",
    "    print(\" - PH/α–λ metrics CSV: <ckpt>/<split>_prognostics_metrics_per_file.csv\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 12) Run\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = Config(\n",
    "        data_dir=r\"C:\\Users\\11HOME_AHCI\\Desktop\\PoF\\Winter 2026\\Ref Data4\\100\",\n",
    "        out_dir=r\"./Trial15\",\n",
    "\n",
    "        seeds=(9819123, 111, 222, 333, 444),\n",
    "\n",
    "        seq_len=100,\n",
    "        stride=5,\n",
    "        pred_horizon=0,\n",
    "\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.2,\n",
    "        test_ratio=0.1,\n",
    "\n",
    "        batch_size=512,\n",
    "        epochs=300,\n",
    "        lr=1e-3,\n",
    "        weight_decay=0.0,\n",
    "        patience=30,\n",
    "        grad_clip=1.0,\n",
    "\n",
    "        hidden_size=512,\n",
    "        num_layers=2,\n",
    "        dropout=0.2,\n",
    "\n",
    "        save_figures=True,\n",
    "        max_files_to_plot=None,\n",
    "        num_workers=0,\n",
    "\n",
    "        alpha=0.20,\n",
    "        ph_consecutive_m=5,\n",
    "        rep_method=\"mean\",\n",
    "        lambdas=(0.2, 0.4, 0.6, 0.8),\n",
    "        lambda_to_plot=0.6,\n",
    "\n",
    "        # base features\n",
    "        delta_steps=(1, 5, 20, 50),\n",
    "        ema_spans=(10, 50),\n",
    "        roll_std_window=10,\n",
    "        add_window_stats=True,\n",
    "\n",
    "        # Trial15-1\n",
    "        add_damage_proxy=True,\n",
    "\n",
    "        # pooling (keep Trial14 default)\n",
    "        pooling=\"mean_last_k\",\n",
    "        pool_last_k=10,\n",
    "        attn_dim=128,\n",
    "\n",
    "        # Trial15-2\n",
    "        tail_robust_enable=True,\n",
    "        tail_k_max=10,\n",
    "        tail_apply_p=1.0,\n",
    "        tail_mode=\"mask\",         # 추천 시작값: \"mask\"\n",
    "        tail_mask_value=0.0,\n",
    "    )\n",
    "\n",
    "    run_trial15_seed_sweep(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c8cc386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ BEST MODEL (Trial15) ================\n",
      "[SELECTED BY VAL | NORM] (recommended; matches best_by_val_norm saving rule)\n",
      "  Trial dir         : ./Trial15\n",
      "  Seed              : 444\n",
      "  Checkpoint        : best_by_val_norm\n",
      "  VAL  RMSE (norm)  : 0.025454\n",
      "  VAL  MAE  (norm)  : 0.020033\n",
      "  VAL  RMSE (cyc)   : 536.334\n",
      "  VAL  MAE  (cyc)   : 335.769\n",
      "  TEST RMSE (norm)  : 0.032219\n",
      "  TEST MAE  (norm)  : 0.026100\n",
      "  TEST RMSE (cyc)   : 601.255\n",
      "  TEST MAE  (cyc)   : 423.602\n",
      "\n",
      "  [Trial15 config snapshot from summary row]\n",
      "   - feature_dim: 16\n",
      "   - pooling: mean_last_k\n",
      "   - pool_last_k: 10\n",
      "   - tail_robust_enable: 1\n",
      "   - tail_k_max: 10\n",
      "   - tail_apply_p: 1.0\n",
      "   - tail_mode: mask\n",
      "\n",
      "[SELECTED BY TEST | NORM] (reporting only; not for tuning)\n",
      "  Seed              : 222\n",
      "  Checkpoint        : best_by_val_norm\n",
      "  TEST RMSE (norm)  : 0.028186\n",
      "  TEST MAE  (norm)  : 0.024751\n",
      "  TEST RMSE (cyc)   : 668.555\n",
      "  TEST MAE  (cyc)   : 537.977\n",
      "  VAL  RMSE (norm)  : 0.026865\n",
      "  VAL  MAE  (norm)  : 0.021343\n",
      "\n",
      "[REFERENCE ONLY] Best by CYCLES (VAL / TEST)\n",
      "  VAL  best cycles -> seed=222, ckpt=best_by_val_norm, val_rmse_cycles=403.750\n",
      "  TEST best cycles -> seed=333, ckpt=best_by_val_norm, test_rmse_cycles=166.727\n",
      "\n",
      "---------------- WIN-RATE (last_epoch vs best_by_val_norm) ----------------\n",
      "- val_rmse_norm: last wins=0, best wins=5, ties=0 | mean(last-best)=0.015131\n",
      "- test_rmse_norm: last wins=1, best wins=4, ties=0 | mean(last-best)=0.006503\n",
      "- val_rmse_cycles: last wins=0, best wins=5, ties=0 | mean(last-best)=402.947369\n",
      "- test_rmse_cycles: last wins=1, best wins=4, ties=0 | mean(last-best)=148.325030\n",
      "========================================================\n",
      "\n",
      "Saved -> ./Trial15\\BEST_MODEL_BY_VAL_NORM.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================\n",
    "# Trial15-compatible paths\n",
    "# ============================\n",
    "TRIAL_DIR = r\"./Trial15\"  # ✅ Trial15 루트 폴더로 변경\n",
    "SUMMARY_CSV = os.path.join(TRIAL_DIR, \"summary_across_seeds.csv\")\n",
    "\n",
    "BEST_TAG = \"best_by_val_norm\"\n",
    "LAST_TAG = \"last_epoch\"\n",
    "\n",
    "\n",
    "def _require_cols(df: pd.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in summary CSV: {missing}\")\n",
    "\n",
    "\n",
    "def _get(df_row: pd.Series, key: str, default=np.nan):\n",
    "    \"\"\"Row에서 key가 없으면 default 반환.\"\"\"\n",
    "    return df_row[key] if key in df_row.index else default\n",
    "\n",
    "\n",
    "def _fmt_float(x, fmt: str):\n",
    "    try:\n",
    "        if x is None:\n",
    "            return \"nan\"\n",
    "        x = float(x)\n",
    "        if not np.isfinite(x):\n",
    "            return \"nan\"\n",
    "        return format(x, fmt)\n",
    "    except Exception:\n",
    "        return \"nan\"\n",
    "\n",
    "\n",
    "def pick_best_row(\n",
    "    df: pd.DataFrame,\n",
    "    metric_prefix: str = \"val\",\n",
    "    sort_space: str = \"norm\",   # \"norm\" or \"cycles\"\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Trial15 selection rule alignment:\n",
    "\n",
    "    - best checkpoint saved by: val_rmse_norm (training loop)\n",
    "      => default sort_space=\"norm\"\n",
    "\n",
    "    metric_prefix: \"val\" or \"test\"\n",
    "    sort_space:\n",
    "      - \"norm\"  : sort by <prefix>_rmse_norm, then <prefix>_mae_norm\n",
    "      - \"cycles\": sort by <prefix>_rmse_cycles, then <prefix>_mae_cycles\n",
    "    \"\"\"\n",
    "    if sort_space not in (\"norm\", \"cycles\"):\n",
    "        raise ValueError(\"sort_space must be 'norm' or 'cycles'\")\n",
    "\n",
    "    rmse_col = f\"{metric_prefix}_rmse_{sort_space}\"\n",
    "    mae_col = f\"{metric_prefix}_mae_{sort_space}\"\n",
    "\n",
    "    _require_cols(df, [\"seed\", \"checkpoint\", rmse_col, mae_col])\n",
    "\n",
    "    df_sorted = df.sort_values(\n",
    "        by=[rmse_col, mae_col],\n",
    "        ascending=[True, True]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return df_sorted.iloc[0]\n",
    "\n",
    "\n",
    "def win_rate(df: pd.DataFrame, metric: str) -> dict:\n",
    "    \"\"\"\n",
    "    Compare BEST_TAG vs LAST_TAG within each seed on the given metric (lower is better).\n",
    "    Returns wins for last, wins for best, ties, and mean(last-best).\n",
    "    \"\"\"\n",
    "    _require_cols(df, [\"seed\", \"checkpoint\", metric])\n",
    "\n",
    "    wins_last = 0\n",
    "    wins_best = 0\n",
    "    ties = 0\n",
    "    diffs = []\n",
    "\n",
    "    for seed, g in df.groupby(\"seed\"):\n",
    "        ckpts = set(g[\"checkpoint\"].astype(str).values)\n",
    "        if not ({BEST_TAG, LAST_TAG} <= ckpts):\n",
    "            continue\n",
    "\n",
    "        b = float(g.loc[g[\"checkpoint\"] == BEST_TAG, metric].iloc[0])\n",
    "        l = float(g.loc[g[\"checkpoint\"] == LAST_TAG, metric].iloc[0])\n",
    "\n",
    "        if np.isfinite(b) and np.isfinite(l):\n",
    "            diffs.append(l - b)  # negative => last better\n",
    "            if l < b:\n",
    "                wins_last += 1\n",
    "            elif b < l:\n",
    "                wins_best += 1\n",
    "            else:\n",
    "                ties += 1\n",
    "\n",
    "    return {\n",
    "        \"metric\": metric,\n",
    "        \"wins_last\": wins_last,\n",
    "        \"wins_best\": wins_best,\n",
    "        \"ties\": ties,\n",
    "        \"mean(last-best)\": float(np.mean(diffs)) if diffs else float(\"nan\"),\n",
    "        \"std(last-best)\": float(np.std(diffs)) if diffs else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(SUMMARY_CSV):\n",
    "        raise FileNotFoundError(f\"Not found: {SUMMARY_CSV}\")\n",
    "\n",
    "    df = pd.read_csv(SUMMARY_CSV)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) VAL 기준 best (recommended)\n",
    "    # -----------------------------\n",
    "    best_val = pick_best_row(df, metric_prefix=\"val\", sort_space=\"norm\")\n",
    "    best_val_seed = int(best_val[\"seed\"])\n",
    "    best_val_ckpt = str(best_val[\"checkpoint\"])\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) TEST 기준 best (for reporting only)\n",
    "    # -----------------------------\n",
    "    best_test = pick_best_row(df, metric_prefix=\"test\", sort_space=\"norm\")\n",
    "    best_test_seed = int(best_test[\"seed\"])\n",
    "    best_test_ckpt = str(best_test[\"checkpoint\"])\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) (Optional) cycles 기준 best (reference)\n",
    "    # -----------------------------\n",
    "    best_val_cycles = pick_best_row(df, metric_prefix=\"val\", sort_space=\"cycles\")\n",
    "    best_test_cycles = pick_best_row(df, metric_prefix=\"test\", sort_space=\"cycles\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4) win-rate (seed별 last vs best 비교)\n",
    "    # -----------------------------\n",
    "    wr_val_rmse_norm = win_rate(df, \"val_rmse_norm\")\n",
    "    wr_test_rmse_norm = win_rate(df, \"test_rmse_norm\")\n",
    "    wr_val_rmse_cycles = win_rate(df, \"val_rmse_cycles\")\n",
    "    wr_test_rmse_cycles = win_rate(df, \"test_rmse_cycles\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5) Print (safe, Trial15 fields may vary)\n",
    "    # -----------------------------\n",
    "    print(\"\\n================ BEST MODEL (Trial15) ================\")\n",
    "    print(\"[SELECTED BY VAL | NORM] (recommended; matches best_by_val_norm saving rule)\")\n",
    "    print(f\"  Trial dir         : {TRIAL_DIR}\")\n",
    "    print(f\"  Seed              : {best_val_seed}\")\n",
    "    print(f\"  Checkpoint        : {best_val_ckpt}\")\n",
    "    print(f\"  VAL  RMSE (norm)  : {_fmt_float(_get(best_val,'val_rmse_norm'), '.6f')}\")\n",
    "    print(f\"  VAL  MAE  (norm)  : {_fmt_float(_get(best_val,'val_mae_norm'),  '.6f')}\")\n",
    "    print(f\"  VAL  RMSE (cyc)   : {_fmt_float(_get(best_val,'val_rmse_cycles'), '.3f')}\")\n",
    "    print(f\"  VAL  MAE  (cyc)   : {_fmt_float(_get(best_val,'val_mae_cycles'),  '.3f')}\")\n",
    "    print(f\"  TEST RMSE (norm)  : {_fmt_float(_get(best_val,'test_rmse_norm'), '.6f')}\")\n",
    "    print(f\"  TEST MAE  (norm)  : {_fmt_float(_get(best_val,'test_mae_norm'),  '.6f')}\")\n",
    "    print(f\"  TEST RMSE (cyc)   : {_fmt_float(_get(best_val,'test_rmse_cycles'), '.3f')}\")\n",
    "    print(f\"  TEST MAE  (cyc)   : {_fmt_float(_get(best_val,'test_mae_cycles'),  '.3f')}\")\n",
    "\n",
    "    # Trial15-specific knobs (있으면 같이 보여줌)\n",
    "    extra_keys = [\n",
    "        \"feature_dim\", \"pooling\", \"pool_last_k\",\n",
    "        \"tail_robust_enable\", \"tail_k_max\", \"tail_apply_p\", \"tail_mode\",\n",
    "        \"add_damage_proxy\",\n",
    "    ]\n",
    "    extras = {k: _get(best_val, k, None) for k in extra_keys if k in best_val.index}\n",
    "    if len(extras) > 0:\n",
    "        print(\"\\n  [Trial15 config snapshot from summary row]\")\n",
    "        for k, v in extras.items():\n",
    "            print(f\"   - {k}: {v}\")\n",
    "\n",
    "    print(\"\\n[SELECTED BY TEST | NORM] (reporting only; not for tuning)\")\n",
    "    print(f\"  Seed              : {best_test_seed}\")\n",
    "    print(f\"  Checkpoint        : {best_test_ckpt}\")\n",
    "    print(f\"  TEST RMSE (norm)  : {_fmt_float(_get(best_test,'test_rmse_norm'), '.6f')}\")\n",
    "    print(f\"  TEST MAE  (norm)  : {_fmt_float(_get(best_test,'test_mae_norm'),  '.6f')}\")\n",
    "    print(f\"  TEST RMSE (cyc)   : {_fmt_float(_get(best_test,'test_rmse_cycles'), '.3f')}\")\n",
    "    print(f\"  TEST MAE  (cyc)   : {_fmt_float(_get(best_test,'test_mae_cycles'),  '.3f')}\")\n",
    "    print(f\"  VAL  RMSE (norm)  : {_fmt_float(_get(best_test,'val_rmse_norm'), '.6f')}\")\n",
    "    print(f\"  VAL  MAE  (norm)  : {_fmt_float(_get(best_test,'val_mae_norm'),  '.6f')}\")\n",
    "\n",
    "    print(\"\\n[REFERENCE ONLY] Best by CYCLES (VAL / TEST)\")\n",
    "    print(\n",
    "        f\"  VAL  best cycles -> seed={int(best_val_cycles['seed'])}, ckpt={best_val_cycles['checkpoint']}, \"\n",
    "        f\"val_rmse_cycles={_fmt_float(_get(best_val_cycles,'val_rmse_cycles'), '.3f')}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  TEST best cycles -> seed={int(best_test_cycles['seed'])}, ckpt={best_test_cycles['checkpoint']}, \"\n",
    "        f\"test_rmse_cycles={_fmt_float(_get(best_test_cycles,'test_rmse_cycles'), '.3f')}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n---------------- WIN-RATE (last_epoch vs best_by_val_norm) ----------------\")\n",
    "    for wr in [wr_val_rmse_norm, wr_test_rmse_norm, wr_val_rmse_cycles, wr_test_rmse_cycles]:\n",
    "        print(\n",
    "            f\"- {wr['metric']}: last wins={wr['wins_last']}, best wins={wr['wins_best']}, ties={wr['ties']} | \"\n",
    "            f\"mean(last-best)={wr['mean(last-best)']:.6f}\"\n",
    "        )\n",
    "    print(\"========================================================\\n\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 6) Save record (VAL best, norm)\n",
    "    # -----------------------------\n",
    "    out_txt = os.path.join(TRIAL_DIR, \"BEST_MODEL_BY_VAL_NORM.txt\")\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"BEST MODEL (Trial15) - Selected by VAL (NORM)\\n\")\n",
    "        f.write(f\"trial_dir={TRIAL_DIR}\\n\")\n",
    "        f.write(f\"seed={best_val_seed}\\n\")\n",
    "        f.write(f\"checkpoint={best_val_ckpt}\\n\")\n",
    "\n",
    "        # core metrics\n",
    "        for k in [\n",
    "            \"val_rmse_norm\", \"val_mae_norm\", \"val_rmse_cycles\", \"val_mae_cycles\",\n",
    "            \"test_rmse_norm\", \"test_mae_norm\", \"test_rmse_cycles\", \"test_mae_cycles\",\n",
    "        ]:\n",
    "            if k in best_val.index:\n",
    "                f.write(f\"{k}={best_val[k]}\\n\")\n",
    "\n",
    "        # extras if present\n",
    "        for k in extra_keys:\n",
    "            if k in best_val.index:\n",
    "                f.write(f\"{k}={best_val[k]}\\n\")\n",
    "\n",
    "    print(f\"Saved -> {out_txt}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d4d51ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] train: λ=0.20:1.000, λ=0.40:1.000, λ=0.60:1.000, λ=0.80:0.900 | mean_all=0.975\n",
      "[OK] val: λ=0.20:1.000, λ=0.40:1.000, λ=0.60:1.000, λ=0.80:0.850 | mean_all=0.963\n",
      "[OK] test: λ=0.20:1.000, λ=0.40:1.000, λ=0.60:1.000, λ=0.80:0.900 | mean_all=0.975\n",
      "\n",
      "==================== DONE ====================\n",
      "Trial: Trial15 | seed=444 | ckpt=best_by_val_norm\n",
      "Saved:\n",
      " - ./Trial15\\seed_444\\best_by_val_norm\\alpha_lambda_eval\\alpha_lambda_summary_seed444_best_by_val_norm.csv\n",
      " - ./Trial15\\seed_444\\best_by_val_norm\\alpha_lambda_eval\\alpha_lambda_per_file_seed444_best_by_val_norm.csv\n",
      "==============================================\n",
      "\n",
      "--- Quick view ---\n",
      "split  n_files       lambdas_found  rate_0.20  rate_0.40  rate_0.60  rate_0.80  rate_mean_all\n",
      "train       70 0.20,0.40,0.60,0.80        1.0        1.0        1.0       0.90         0.9750\n",
      "  val       20 0.20,0.40,0.60,0.80        1.0        1.0        1.0       0.85         0.9625\n",
      " test       10 0.20,0.40,0.60,0.80        1.0        1.0        1.0       0.90         0.9750\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# USER CONFIG (Trial15)\n",
    "# =========================\n",
    "TRIAL_DIR = r\"./Trial15\"                 # ✅ Trial15 루트\n",
    "CKPT = \"best_by_val_norm\"                # default (txt가 있으면 txt 우선)\n",
    "SPLITS = [\"train\", \"val\", \"test\"]        # 평가할 split\n",
    "\n",
    "# (선택) 후반 λ를 더 중요하게 보고 싶으면 가중치 사용\n",
    "# 예: {\"0.20\":1, \"0.40\":1, \"0.60\":2, \"0.80\":3}\n",
    "LAMBDA_WEIGHTS = None\n",
    "\n",
    "# (권장) best seed/ckpt 자동 로드\n",
    "AUTO_USE_BEST_TXT = True\n",
    "BEST_TXT = os.path.join(TRIAL_DIR, \"BEST_MODEL_BY_VAL_NORM.txt\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def _require_cols(df: pd.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "\n",
    "def _list_dir(path: str):\n",
    "    if not os.path.isdir(path):\n",
    "        return []\n",
    "    return sorted([p for p in os.listdir(path)])\n",
    "\n",
    "\n",
    "def _parse_best_txt(best_txt_path: str):\n",
    "    \"\"\"\n",
    "    Reads Trial15/BEST_MODEL_BY_VAL_NORM.txt produced by the previous script.\n",
    "    Expected lines include:\n",
    "      seed=444\n",
    "      checkpoint=best_by_val_norm\n",
    "    \"\"\"\n",
    "    seed = None\n",
    "    ckpt = None\n",
    "    if not os.path.exists(best_txt_path):\n",
    "        return seed, ckpt\n",
    "\n",
    "    with open(best_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"seed=\"):\n",
    "                try:\n",
    "                    seed = int(line.split(\"=\", 1)[1].strip())\n",
    "                except Exception:\n",
    "                    pass\n",
    "            elif line.startswith(\"checkpoint=\"):\n",
    "                ckpt = line.split(\"=\", 1)[1].strip()\n",
    "\n",
    "    return seed, ckpt\n",
    "\n",
    "\n",
    "def discover_lambdas_from_columns(dfm: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    compute_metrics_for_one_file()가 생성하는 컬럼:\n",
    "      - alpha_lambda_ok_{lam:.2f}\n",
    "      - t_lambda_{lam:.2f}\n",
    "    여기서 lam 문자열(\"0.20\")을 자동 추출.\n",
    "    \"\"\"\n",
    "    lam = []\n",
    "    pat = re.compile(r\"^alpha_lambda_ok_(\\d+\\.\\d+)$\")\n",
    "    for c in dfm.columns:\n",
    "        m = pat.match(c)\n",
    "        if m:\n",
    "            lam.append(m.group(1))\n",
    "    lam = sorted(set(lam), key=lambda s: float(s))\n",
    "    return lam\n",
    "\n",
    "\n",
    "def compute_alpha_lambda_rates(dfm: pd.DataFrame, lam_strs: list, weights=None) -> dict:\n",
    "    \"\"\"\n",
    "    dfm: <split>_prognostics_metrics_per_file.csv (per-file summary)\n",
    "    Returns:\n",
    "      - per-lambda success rate (mean of alpha_lambda_ok_{lam})\n",
    "      - overall mean rate (simple mean or weighted mean)\n",
    "    \"\"\"\n",
    "    rates = {}\n",
    "\n",
    "    for ls in lam_strs:\n",
    "        col = f\"alpha_lambda_ok_{ls}\"\n",
    "        if col in dfm.columns:\n",
    "            rates[f\"rate_{ls}\"] = float(dfm[col].mean())  # 0/1 평균 = 성공률\n",
    "        else:\n",
    "            rates[f\"rate_{ls}\"] = np.nan\n",
    "\n",
    "    # overall score\n",
    "    if weights is None:\n",
    "        vals = [rates[f\"rate_{ls}\"] for ls in lam_strs if np.isfinite(rates[f\"rate_{ls}\"])]\n",
    "        rates[\"rate_mean_all\"] = float(np.mean(vals)) if vals else np.nan\n",
    "    else:\n",
    "        num = 0.0\n",
    "        den = 0.0\n",
    "        for ls in lam_strs:\n",
    "            v = rates[f\"rate_{ls}\"]\n",
    "            w = float(weights.get(ls, 0.0))\n",
    "            if np.isfinite(v) and w > 0:\n",
    "                num += w * v\n",
    "                den += w\n",
    "        rates[\"rate_weighted_all\"] = (num / den) if den > 0 else np.nan\n",
    "\n",
    "    return rates\n",
    "\n",
    "\n",
    "def main():\n",
    "    # -----------------------------\n",
    "    # 0) auto seed/ckpt from BEST_MODEL_BY_VAL_NORM.txt\n",
    "    # -----------------------------\n",
    "    seed_from_txt, ckpt_from_txt = _parse_best_txt(BEST_TXT)\n",
    "\n",
    "    if AUTO_USE_BEST_TXT:\n",
    "        if seed_from_txt is None:\n",
    "            raise FileNotFoundError(\n",
    "                f\"AUTO_USE_BEST_TXT=True인데 seed를 못 읽었습니다.\\n\"\n",
    "                f\"- 기대 파일: {BEST_TXT}\\n\"\n",
    "                f\"- 먼저 'best 모델 선택 스크립트'를 돌려 BEST_MODEL_BY_VAL_NORM.txt를 생성하세요.\"\n",
    "            )\n",
    "        SEED = int(seed_from_txt)\n",
    "        if ckpt_from_txt is not None and len(str(ckpt_from_txt)) > 0:\n",
    "            ckpt = str(ckpt_from_txt)\n",
    "        else:\n",
    "            ckpt = CKPT\n",
    "    else:\n",
    "        # 수동 모드면 여기서 직접 지정하도록 변경 가능\n",
    "        SEED = 444\n",
    "        ckpt = CKPT\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) Path resolve (Trial15 표준)\n",
    "    #    ./Trial15/seed_<seed>/<ckpt>/\n",
    "    # -----------------------------\n",
    "    seed_dir = os.path.join(TRIAL_DIR, f\"seed_{SEED}\", ckpt)\n",
    "    if not os.path.isdir(seed_dir):\n",
    "        seed_root = os.path.join(TRIAL_DIR, f\"seed_{SEED}\")\n",
    "        msg = [\n",
    "            f\"Not found: {seed_dir}\",\n",
    "            f\"Available under {TRIAL_DIR}: {_list_dir(TRIAL_DIR)}\",\n",
    "            f\"Available ckpts under {seed_root}: {_list_dir(seed_root)}\",\n",
    "        ]\n",
    "        raise FileNotFoundError(\"\\n\".join(msg))\n",
    "\n",
    "    out_dir = os.path.join(seed_dir, \"alpha_lambda_eval\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    summary_rows = []\n",
    "    per_file_rows = []\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) Load each split metrics + compute α–λ success rates\n",
    "    # -----------------------------\n",
    "    for split in SPLITS:\n",
    "        # Trial15 export_ckpt() 저장 위치:\n",
    "        #   seed_<seed>/<ckpt>/<split>_prognostics_metrics_per_file.csv\n",
    "        mpath = os.path.join(seed_dir, f\"{split}_prognostics_metrics_per_file.csv\")\n",
    "        if not os.path.exists(mpath):\n",
    "            print(f\"[SKIP] Missing: {mpath}\")\n",
    "            continue\n",
    "\n",
    "        dfm = pd.read_csv(mpath)\n",
    "        _require_cols(dfm, [\"file\"])\n",
    "\n",
    "        lam_strs = discover_lambdas_from_columns(dfm)\n",
    "        if not lam_strs:\n",
    "            print(f\"[WARN] No alpha_lambda_ok_* columns found in: {mpath}\")\n",
    "            continue\n",
    "\n",
    "        rates = compute_alpha_lambda_rates(dfm, lam_strs, weights=LAMBDA_WEIGHTS)\n",
    "\n",
    "        row = {\n",
    "            \"trial\": \"Trial15\",\n",
    "            \"seed\": SEED,\n",
    "            \"checkpoint\": ckpt,\n",
    "            \"split\": split,\n",
    "            \"n_files\": int(len(dfm)),\n",
    "            \"lambdas_found\": \",\".join(lam_strs),\n",
    "            **rates,\n",
    "        }\n",
    "        summary_rows.append(row)\n",
    "\n",
    "        # 파일별 pass/fail + t_lambda 저장\n",
    "        keep_cols = [\"file\"]\n",
    "        for ls in lam_strs:\n",
    "            c_ok = f\"alpha_lambda_ok_{ls}\"\n",
    "            c_tl = f\"t_lambda_{ls}\"\n",
    "            if c_ok in dfm.columns:\n",
    "                keep_cols.append(c_ok)\n",
    "            if c_tl in dfm.columns:\n",
    "                keep_cols.append(c_tl)\n",
    "\n",
    "        sub = dfm[keep_cols].copy()\n",
    "        sub.insert(0, \"split\", split)\n",
    "        sub.insert(0, \"checkpoint\", ckpt)\n",
    "        sub.insert(0, \"seed\", SEED)\n",
    "        sub.insert(0, \"trial\", \"Trial15\")\n",
    "        per_file_rows.append(sub)\n",
    "\n",
    "        # 콘솔 출력\n",
    "        msg_parts = []\n",
    "        for ls in lam_strs:\n",
    "            v = row.get(f\"rate_{ls}\", np.nan)\n",
    "            if np.isfinite(v):\n",
    "                msg_parts.append(f\"λ={ls}:{v:.3f}\")\n",
    "        msg = \", \".join(msg_parts) if msg_parts else \"no lambda columns found\"\n",
    "\n",
    "        tail = \"\"\n",
    "        if \"rate_weighted_all\" in row and np.isfinite(row[\"rate_weighted_all\"]):\n",
    "            tail = f\" | weighted_all={row['rate_weighted_all']:.3f}\"\n",
    "        elif \"rate_mean_all\" in row and np.isfinite(row[\"rate_mean_all\"]):\n",
    "            tail = f\" | mean_all={row['rate_mean_all']:.3f}\"\n",
    "\n",
    "        print(f\"[OK] {split}: {msg}{tail}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) Save\n",
    "    # -----------------------------\n",
    "    df_summary = pd.DataFrame(summary_rows)\n",
    "    out_summary = os.path.join(out_dir, f\"alpha_lambda_summary_seed{SEED}_{ckpt}.csv\")\n",
    "    df_summary.to_csv(out_summary, index=False)\n",
    "\n",
    "    out_pf = None\n",
    "    if per_file_rows:\n",
    "        df_pf = pd.concat(per_file_rows, axis=0, ignore_index=True)\n",
    "        out_pf = os.path.join(out_dir, f\"alpha_lambda_per_file_seed{SEED}_{ckpt}.csv\")\n",
    "        df_pf.to_csv(out_pf, index=False)\n",
    "\n",
    "    print(\"\\n==================== DONE ====================\")\n",
    "    print(f\"Trial: Trial15 | seed={SEED} | ckpt={ckpt}\")\n",
    "    print(\"Saved:\")\n",
    "    print(\" -\", out_summary)\n",
    "    if out_pf:\n",
    "        print(\" -\", out_pf)\n",
    "    print(\"==============================================\")\n",
    "\n",
    "    # Quick view\n",
    "    if not df_summary.empty:\n",
    "        base = [\"split\", \"n_files\", \"lambdas_found\"]\n",
    "        extra = [c for c in df_summary.columns if c.startswith(\"rate_\")]\n",
    "        show_cols = [c for c in (base + extra) if c in df_summary.columns]\n",
    "\n",
    "        print(\"\\n--- Quick view ---\")\n",
    "        print(df_summary[show_cols].to_string(index=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0150d5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] ./Trial15\\seed_444\\best_by_val_norm\\alpha_lambda_eval\\alpha_lambda_summary_seed444_best_by_val_norm.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] ./Trial15\\seed_444\\best_by_val_norm\\alpha_lambda_eval\\alpha_lambda_curve_seed444_best_by_val_norm.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# USER CONFIG (Trial15)\n",
    "# =========================\n",
    "TRIAL_DIR = r\"./Trial15\"\n",
    "\n",
    "# 고정 λ를 쓸지(기존 방식) vs CSV에서 자동 추출할지 선택\n",
    "USE_FIXED_LAMBDAS = True\n",
    "LAM_STRS_FIXED = [\"0.20\", \"0.40\", \"0.60\", \"0.80\"]\n",
    "\n",
    "SPLITS_ORDER = [\"train\", \"val\", \"test\"]\n",
    "DEFAULT_CKPT = \"best_by_val_norm\"\n",
    "\n",
    "# best seed/ckpt 자동 로드 (권장)\n",
    "AUTO_USE_BEST_TXT = True\n",
    "BEST_TXT = os.path.join(TRIAL_DIR, \"BEST_MODEL_BY_VAL_NORM.txt\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def _parse_best_txt(best_txt_path: str):\n",
    "    seed = None\n",
    "    ckpt = None\n",
    "    if not os.path.exists(best_txt_path):\n",
    "        return seed, ckpt\n",
    "    with open(best_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"seed=\"):\n",
    "                try:\n",
    "                    seed = int(line.split(\"=\", 1)[1].strip())\n",
    "                except Exception:\n",
    "                    pass\n",
    "            elif line.startswith(\"checkpoint=\"):\n",
    "                ckpt = line.split(\"=\", 1)[1].strip()\n",
    "    return seed, ckpt\n",
    "\n",
    "\n",
    "def discover_lambdas_from_columns(dfm: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    alpha_lambda_ok_{lam:.2f} 컬럼에서 lam 문자열(\"0.20\") 자동 추출\n",
    "    \"\"\"\n",
    "    lam = []\n",
    "    pat = re.compile(r\"^alpha_lambda_ok_(\\d+\\.\\d+)$\")\n",
    "    for c in dfm.columns:\n",
    "        m = pat.match(c)\n",
    "        if m:\n",
    "            lam.append(m.group(1))\n",
    "    lam = sorted(set(lam), key=lambda s: float(s))\n",
    "    return lam\n",
    "\n",
    "\n",
    "def compute_alpha_lambda_rates_df(dfm: pd.DataFrame, lam_strs):\n",
    "    \"\"\"\n",
    "    dfm: <split>_prognostics_metrics_per_file.csv 로드된 DF\n",
    "    Returns:\n",
    "      dict: {\"rate_0.20\":..., ... , \"n_files\":..., \"n_valid_0.20\":...}\n",
    "    \"\"\"\n",
    "    if dfm.empty:\n",
    "        return {f\"rate_{ls}\": np.nan for ls in lam_strs} | {\"n_files\": 0}\n",
    "\n",
    "    out = {\"n_files\": int(len(dfm))}\n",
    "    for ls in lam_strs:\n",
    "        col = f\"alpha_lambda_ok_{ls}\"\n",
    "        if col not in dfm.columns:\n",
    "            out[f\"rate_{ls}\"] = np.nan\n",
    "            out[f\"n_valid_{ls}\"] = 0\n",
    "            continue\n",
    "\n",
    "        v = pd.to_numeric(dfm[col], errors=\"coerce\")  # 0/1 or NaN\n",
    "        v_valid = v.dropna()\n",
    "        out[f\"n_valid_{ls}\"] = int(len(v_valid))\n",
    "        out[f\"rate_{ls}\"] = float(v_valid.mean()) if len(v_valid) > 0 else np.nan\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Resolve seed/ckpt (Trial15 best)\n",
    "# =========================\n",
    "seed_from_txt, ckpt_from_txt = _parse_best_txt(BEST_TXT)\n",
    "\n",
    "if AUTO_USE_BEST_TXT:\n",
    "    if seed_from_txt is None:\n",
    "        raise FileNotFoundError(\n",
    "            f\"AUTO_USE_BEST_TXT=True인데 seed를 못 읽었습니다.\\n\"\n",
    "            f\"- 기대 파일: {BEST_TXT}\\n\"\n",
    "            f\"- 먼저 best model 선택 스크립트를 돌려 BEST_MODEL_BY_VAL_NORM.txt를 생성하세요.\"\n",
    "        )\n",
    "    SEED = int(seed_from_txt)\n",
    "    CKPT = str(ckpt_from_txt) if ckpt_from_txt else DEFAULT_CKPT\n",
    "else:\n",
    "    # 수동 모드\n",
    "    SEED = 444\n",
    "    CKPT = DEFAULT_CKPT\n",
    "\n",
    "# =========================\n",
    "# Paths (Trial15 export_ckpt output structure)\n",
    "#   <TRIAL_DIR>/seed_<seed>/<CKPT>/<split>_prognostics_metrics_per_file.csv\n",
    "# =========================\n",
    "CKPT_DIR = os.path.join(TRIAL_DIR, f\"seed_{SEED}\", CKPT)\n",
    "\n",
    "AL_DIR = os.path.join(CKPT_DIR, \"alpha_lambda_eval\")\n",
    "SUMMARY_CSV = os.path.join(AL_DIR, f\"alpha_lambda_summary_seed{SEED}_{CKPT}.csv\")\n",
    "OUT_PNG = os.path.join(AL_DIR, f\"alpha_lambda_curve_seed{SEED}_{CKPT}.png\")\n",
    "\n",
    "if not os.path.isdir(CKPT_DIR):\n",
    "    raise FileNotFoundError(f\"Not found CKPT_DIR: {CKPT_DIR}\")\n",
    "\n",
    "# =========================\n",
    "# Load per-split metrics & build summary\n",
    "# =========================\n",
    "rows = []\n",
    "lam_strs_used = None\n",
    "\n",
    "for split in SPLITS_ORDER:\n",
    "    metrics_csv = os.path.join(CKPT_DIR, f\"{split}_prognostics_metrics_per_file.csv\")\n",
    "    if not os.path.exists(metrics_csv):\n",
    "        print(f\"[SKIP] not found for split={split}: {metrics_csv}\")\n",
    "        continue\n",
    "\n",
    "    dfm = pd.read_csv(metrics_csv)\n",
    "\n",
    "    # λ 리스트 결정 (1회만 결정해서 모든 split에서 같은 λ로 그림)\n",
    "    if lam_strs_used is None:\n",
    "        if USE_FIXED_LAMBDAS:\n",
    "            lam_strs_used = list(LAM_STRS_FIXED)\n",
    "        else:\n",
    "            lam_strs_used = discover_lambdas_from_columns(dfm)\n",
    "\n",
    "        if not lam_strs_used:\n",
    "            raise ValueError(f\"No alpha_lambda_ok_* columns found in: {metrics_csv}\")\n",
    "\n",
    "    rates = compute_alpha_lambda_rates_df(dfm, lam_strs_used)\n",
    "    row = {\"split\": split, \"seed\": SEED, \"ckpt\": CKPT, \"lambdas\": \",\".join(lam_strs_used), **rates}\n",
    "    rows.append(row)\n",
    "\n",
    "if len(rows) == 0:\n",
    "    raise FileNotFoundError(\n",
    "        \"No per-split metrics CSVs were found.\\n\"\n",
    "        f\"Expected like: {os.path.join(CKPT_DIR, 'test_prognostics_metrics_per_file.csv')}\\n\"\n",
    "        \"Check TRIAL_DIR/SEED/CKPT.\"\n",
    "    )\n",
    "\n",
    "df_sum = pd.DataFrame(rows)\n",
    "\n",
    "# 저장\n",
    "os.makedirs(AL_DIR, exist_ok=True)\n",
    "df_sum.to_csv(SUMMARY_CSV, index=False)\n",
    "print(f\"[SAVE] {SUMMARY_CSV}\")\n",
    "\n",
    "# =========================\n",
    "# Plot\n",
    "# =========================\n",
    "LAM = [float(x) for x in lam_strs_used]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "for split in SPLITS_ORDER:\n",
    "    sub = df_sum[df_sum[\"split\"] == split]\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    row = sub.iloc[0]\n",
    "\n",
    "    rates = []\n",
    "    for ls in lam_strs_used:\n",
    "        v = row.get(f\"rate_{ls}\", np.nan)\n",
    "        rates.append(float(v) if np.isfinite(v) else np.nan)\n",
    "\n",
    "    plt.plot(LAM, rates, marker=\"o\", linewidth=2, label=split)\n",
    "\n",
    "plt.xticks(LAM, [f\"{x:.2f}\" for x in LAM])\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.xlabel(\"λ (life fraction)\")\n",
    "plt.ylabel(\"α–λ success rate\")\n",
    "plt.title(f\"α–λ Success Rate Curve\\nTrial15 | Seed {SEED} | {CKPT}\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(OUT_PNG, dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"[SAVE] {OUT_PNG}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c118e538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] DONE -> ./Trial15\\seed_444\\best_by_val_norm\\paper_figures_bookstyle\\train\n",
      "[val] DONE -> ./Trial15\\seed_444\\best_by_val_norm\\paper_figures_bookstyle\\val\n",
      "[test] DONE -> ./Trial15\\seed_444\\best_by_val_norm\\paper_figures_bookstyle\\test\n",
      "\n",
      "ALL DONE.\n",
      "Saved under: ./Trial15\\seed_444\\best_by_val_norm\\paper_figures_bookstyle\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# USER CONFIG (Trial15)\n",
    "# =========================\n",
    "TRIAL_DIR = r\"./Trial15\"                 # ✅ Trial15 폴더\n",
    "SPLITS = [\"train\", \"val\", \"test\"]        # ✅ 여러 split 한 번에\n",
    "\n",
    "ALPHA = 0.20\n",
    "LAMBDA_TO_PLOT = 0.60                   # α–λ 그림에 표시할 λ\n",
    "\n",
    "MAX_FILES = None                        # None=모두, 아니면 예: 10\n",
    "\n",
    "# best seed/ckpt 자동 로드 (권장)\n",
    "AUTO_USE_BEST_TXT = True\n",
    "BEST_TXT = os.path.join(TRIAL_DIR, \"BEST_MODEL_BY_VAL_NORM.txt\")\n",
    "DEFAULT_CKPT = \"best_by_val_norm\"\n",
    "\n",
    "# (수동 모드에서만 사용)\n",
    "MANUAL_SEED = 444\n",
    "MANUAL_CKPT = \"best_by_val_norm\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "def safe_name(s: str) -> str:\n",
    "    return s.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\")\n",
    "\n",
    "\n",
    "def _parse_best_txt(best_txt_path: str) -> Tuple[Optional[int], Optional[str]]:\n",
    "    seed = None\n",
    "    ckpt = None\n",
    "    if not os.path.exists(best_txt_path):\n",
    "        return seed, ckpt\n",
    "\n",
    "    with open(best_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"seed=\"):\n",
    "                try:\n",
    "                    seed = int(line.split(\"=\", 1)[1].strip())\n",
    "                except Exception:\n",
    "                    pass\n",
    "            elif line.startswith(\"checkpoint=\"):\n",
    "                ckpt = line.split(\"=\", 1)[1].strip()\n",
    "\n",
    "    return seed, ckpt\n",
    "\n",
    "\n",
    "def load_cycle_seq_and_metrics(seed_ckpt_dir: str, split: str):\n",
    "    \"\"\"\n",
    "    Trial15 export_ckpt()가 만들어 둔 파일들:\n",
    "      - <split>_cycle_sequence_mean.csv\n",
    "      - <split>_prognostics_metrics_per_file.csv\n",
    "    위치:\n",
    "      <TRIAL_DIR>/seed_<seed>/<ckpt>/\n",
    "    \"\"\"\n",
    "    seq_csv = os.path.join(seed_ckpt_dir, f\"{split}_cycle_sequence_mean.csv\")\n",
    "    met_csv = os.path.join(seed_ckpt_dir, f\"{split}_prognostics_metrics_per_file.csv\")\n",
    "\n",
    "    if not os.path.exists(seq_csv):\n",
    "        raise FileNotFoundError(f\"Missing: {seq_csv}\")\n",
    "    if not os.path.exists(met_csv):\n",
    "        raise FileNotFoundError(f\"Missing: {met_csv}\")\n",
    "\n",
    "    df_seq = pd.read_csv(seq_csv)\n",
    "    df_met = pd.read_csv(met_csv)\n",
    "    return df_seq, df_met\n",
    "\n",
    "\n",
    "def get_eval_segment(df_one_file: pd.DataFrame, t_s: int, t_e: int) -> pd.DataFrame:\n",
    "    df = df_one_file.sort_values(\"cycle\").copy()\n",
    "    df = df[(df[\"cycle\"] >= t_s) & (df[\"cycle\"] <= t_e)].copy()\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Plotters\n",
    "# ============================================================\n",
    "def plot_ph_alpha_absolute_band(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    alpha: float,\n",
    "    out_path: str,\n",
    "    ph_start: Optional[float] = None,\n",
    "    title_prefix: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    BOOK Fig.2.9(a) 스타일: PH용 α-zone은 '절대 폭(평행 밴드)'\n",
    "      alphaZone = alpha * EOL_true\n",
    "      zone = RUL_true ± alphaZone\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    x = df_eval[\"cycle\"].to_numpy()\n",
    "    y_true = df_eval[\"RUL_true\"].to_numpy()\n",
    "    y_pred = df_eval[\"RUL_pred\"].to_numpy()\n",
    "\n",
    "    last_cycle = int(df_eval[\"cycle\"].max())\n",
    "    eol_true = last_cycle + 1\n",
    "\n",
    "    alpha_zone = alpha * float(eol_true)  # ✅ book-style 핵심 (평행 밴드)\n",
    "    upper = y_true + alpha_zone\n",
    "    lower = y_true - alpha_zone\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y_true, \"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, \"r\", label=\"Prediction (cycles)\")\n",
    "    plt.plot(x, upper, \"b--\", label=f\"+{alpha:.2f} α-zone (±α·EOL)\")\n",
    "    plt.plot(x, lower, \"b--\", label=f\"-{alpha:.2f} α-zone (±α·EOL)\")\n",
    "\n",
    "    if ph_start is not None and np.isfinite(ph_start):\n",
    "        plt.axvline(int(ph_start), color=\"g\", linestyle=\"-.\", label=\"PH start\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title_prefix} | BOOK-STYLE α+PH (absolute band)\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_alpha_lambda_relative_band(\n",
    "    df_eval: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    alpha: float,\n",
    "    lambda_to_plot: float,\n",
    "    t_lambda: Optional[int],\n",
    "    out_path: str,\n",
    "    title_prefix: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    BOOK Fig.2.9(b) 스타일: α–λ는 '상대 폭(수렴 밴드)'\n",
    "      zone = RUL_true*(1±alpha), 그리고 t >= t_lambda 구간만 표시\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    x = df_eval[\"cycle\"].to_numpy()\n",
    "    y_true = df_eval[\"RUL_true\"].to_numpy()\n",
    "    y_pred = df_eval[\"RUL_pred\"].to_numpy()\n",
    "\n",
    "    upper = y_true * (1.0 + alpha)\n",
    "    lower = y_true * (1.0 - alpha)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y_true, \"k\", label=\"True (cycles)\")\n",
    "    plt.plot(x, y_pred, \"r\", label=\"Prediction (cycles)\")\n",
    "\n",
    "    if t_lambda is not None and np.isfinite(t_lambda):\n",
    "        t_lambda = int(t_lambda)\n",
    "        plt.axvline(t_lambda, color=\"g\", linestyle=\":\", label=f\"t_λ (λ={lambda_to_plot:.2f})\")\n",
    "\n",
    "        mask = x >= t_lambda\n",
    "        if np.any(mask):\n",
    "            plt.plot(x[mask], upper[mask], \"b--\", label=f\"+{alpha:.2f} α–λ zone\")\n",
    "            plt.plot(x[mask], lower[mask], \"b--\", label=f\"-{alpha:.2f} α–λ zone\")\n",
    "        else:\n",
    "            plt.plot(x, upper, \"b--\", label=f\"+{alpha:.2f} α zone\")\n",
    "            plt.plot(x, lower, \"b--\", label=f\"-{alpha:.2f} α zone\")\n",
    "    else:\n",
    "        plt.plot(x, upper, \"b--\", label=f\"+{alpha:.2f} α zone\")\n",
    "        plt.plot(x, lower, \"b--\", label=f\"-{alpha:.2f} α zone\")\n",
    "\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"RUL (cycles)\")\n",
    "    plt.title(f\"{title_prefix} | BOOK-STYLE α–λ (relative band)\\n{file_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Main (multi-split)\n",
    "# ============================================================\n",
    "def run_for_one_split(seed_ckpt_dir: str, out_root: str, split: str, seed: int, ckpt: str):\n",
    "    df_seq, df_met = load_cycle_seq_and_metrics(seed_ckpt_dir, split)\n",
    "\n",
    "    files = df_seq[\"file\"].unique().tolist()\n",
    "    if MAX_FILES is not None:\n",
    "        files = files[:MAX_FILES]\n",
    "\n",
    "    out_dir = os.path.join(out_root, split)  # ✅ split별 폴더\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    lam_key = f\"t_lambda_{LAMBDA_TO_PLOT:.2f}\"\n",
    "    title_prefix = f\"Trial15 | SEED {seed} | {ckpt.upper()} | {split}\"\n",
    "\n",
    "    for f in files:\n",
    "        sub = df_seq[df_seq[\"file\"] == f].copy()\n",
    "        mrow = df_met[df_met[\"file\"] == f]\n",
    "        if mrow.empty:\n",
    "            continue\n",
    "        mrow = mrow.iloc[0].to_dict()\n",
    "\n",
    "        # 필수\n",
    "        t_s = int(mrow[\"t_s\"])\n",
    "        t_e = int(mrow[\"t_e\"])\n",
    "\n",
    "        ph_start = mrow.get(\"t_PH_start\", np.nan)\n",
    "        t_lambda = mrow.get(lam_key, np.nan)\n",
    "\n",
    "        df_eval = get_eval_segment(sub, t_s, t_e)\n",
    "        if df_eval.empty:\n",
    "            continue\n",
    "\n",
    "        sname = safe_name(f)\n",
    "\n",
    "        out_a = os.path.join(out_dir, f\"FIG_A_BOOKSTYLE_alpha_PH__{sname}.png\")\n",
    "        plot_ph_alpha_absolute_band(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            alpha=ALPHA,\n",
    "            out_path=out_a,\n",
    "            ph_start=ph_start if np.isfinite(ph_start) else None,\n",
    "            title_prefix=title_prefix,\n",
    "        )\n",
    "\n",
    "        out_b = os.path.join(out_dir, f\"FIG_B_BOOKSTYLE_alpha_lambda__lam{LAMBDA_TO_PLOT:.2f}__{sname}.png\")\n",
    "        plot_alpha_lambda_relative_band(\n",
    "            df_eval=df_eval,\n",
    "            file_name=f,\n",
    "            alpha=ALPHA,\n",
    "            lambda_to_plot=LAMBDA_TO_PLOT,\n",
    "            t_lambda=int(t_lambda) if np.isfinite(t_lambda) else None,\n",
    "            out_path=out_b,\n",
    "            title_prefix=title_prefix,\n",
    "        )\n",
    "\n",
    "    print(f\"[{split}] DONE -> {out_dir}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # -----------------------------\n",
    "    # Resolve best seed/ckpt\n",
    "    # -----------------------------\n",
    "    seed_from_txt, ckpt_from_txt = _parse_best_txt(BEST_TXT)\n",
    "\n",
    "    if AUTO_USE_BEST_TXT:\n",
    "        if seed_from_txt is None:\n",
    "            raise FileNotFoundError(\n",
    "                f\"AUTO_USE_BEST_TXT=True인데 seed를 못 읽었습니다.\\n\"\n",
    "                f\"- 기대 파일: {BEST_TXT}\\n\"\n",
    "                f\"- 먼저 best model 선택 스크립트를 돌려 BEST_MODEL_BY_VAL_NORM.txt를 생성하세요.\"\n",
    "            )\n",
    "        seed = int(seed_from_txt)\n",
    "        ckpt = str(ckpt_from_txt) if ckpt_from_txt else DEFAULT_CKPT\n",
    "    else:\n",
    "        seed = int(MANUAL_SEED)\n",
    "        ckpt = str(MANUAL_CKPT)\n",
    "\n",
    "    seed_ckpt_dir = os.path.join(TRIAL_DIR, f\"seed_{seed}\", ckpt)\n",
    "    if not os.path.isdir(seed_ckpt_dir):\n",
    "        raise FileNotFoundError(f\"Not found: {seed_ckpt_dir}\")\n",
    "\n",
    "    # 저장 폴더 루트 (Trial15 표준)\n",
    "    out_root = os.path.join(seed_ckpt_dir, \"paper_figures_bookstyle\")\n",
    "\n",
    "    for split in SPLITS:\n",
    "        run_for_one_split(seed_ckpt_dir, out_root, split, seed, ckpt)\n",
    "\n",
    "    print(\"\\nALL DONE.\")\n",
    "    print(\"Saved under:\", out_root)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f859d520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] persistent files: 0 (min_hits=3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\11HOME_AHCI\\anaconda3\\envs\\igbt_rnn\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SAVED]\n",
      " - Trial15_rootcause_analysis\\02_persistent_high_error_files.csv\n",
      " - Trial15_rootcause_analysis\\03_file_level_features_damage_baseline_tail.csv\n",
      " - Trial15_rootcause_analysis\\06_cluster_summary.csv\n",
      " - Trial15_rootcause_analysis\\08_pca_clusters.png\n",
      " - Trial15_rootcause_analysis\\11_persistent_minus_others_mean_shift_z.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# USER CONFIG\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    data_dir: str = r\"C:\\Users\\11HOME_AHCI\\Desktop\\PoF\\Winter 2026\\Ref Data4\\100\"\n",
    "\n",
    "    # Trial 결과 루트 (Trial15)\n",
    "    trial_dir: str = r\"./Trial15\"\n",
    "    seed: int = 333\n",
    "\n",
    "    # 어떤 ckpt를 볼지: [\"best_by_val_norm\", \"last_epoch\"] 또는 하나만\n",
    "    ckpts: Tuple[str, ...] = (\"best_by_val_norm\", \"last_epoch\")\n",
    "\n",
    "    # 어떤 split을 볼지: [\"train\",\"val\",\"test\"] 중 선택\n",
    "    splits: Tuple[str, ...] = (\"train\", \"val\", \"test\")\n",
    "\n",
    "    # high-error 정의 (per file metric 기준)\n",
    "    # 방법1: 상위 q 분위수\n",
    "    high_quantile: float = 0.80\n",
    "    # 방법2: 상위 k개 (q 대신 사용하려면 k를 int로 설정)\n",
    "    topk: Optional[int] = None\n",
    "\n",
    "    # persistent 조건: (ckpt, split) 조합 중 최소 몇 번 high에 걸리면 persistent로 볼지\n",
    "    persistent_min_hits: int = 3\n",
    "\n",
    "    # 클러스터 수 (KMeans)\n",
    "    n_clusters: int = 4\n",
    "\n",
    "    # 출력 폴더\n",
    "    out_dir: str = r\"./Trial15_rootcause_analysis\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helpers: raw data read + feature extraction (damage/baseline/delta/tail)\n",
    "# ============================================================\n",
    "def read_raw_minvce(csv_path: Path) -> np.ndarray:\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    if df.shape[1] < 1:\n",
    "        raise ValueError(f\"{csv_path}: needs at least 1 col\")\n",
    "    v = df.iloc[:, 0].astype(np.float32).to_numpy()\n",
    "    return v\n",
    "\n",
    "\n",
    "def _delta(v: np.ndarray, k: int = 1) -> np.ndarray:\n",
    "    out = np.zeros_like(v, dtype=np.float32)\n",
    "    if k <= 0:\n",
    "        return out\n",
    "    out[k:] = v[k:] - v[:-k]\n",
    "    return out\n",
    "\n",
    "\n",
    "def extract_file_level_features(v: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    파일 전체 시퀀스에서 file-level feature를 뽑음.\n",
    "    - baseline(v0) / tail level / delta stats / damage proxies (cum)\n",
    "    \"\"\"\n",
    "    v = v.astype(np.float32)\n",
    "    T = len(v)\n",
    "    if T < 5:\n",
    "        return {}\n",
    "\n",
    "    v0 = float(v[0])\n",
    "    v_end = float(v[-1])\n",
    "    v_mean = float(np.mean(v))\n",
    "    v_std = float(np.std(v, ddof=0))\n",
    "\n",
    "    d1 = _delta(v, 1)\n",
    "    dd1 = _delta(d1, 1)\n",
    "\n",
    "    # delta stats\n",
    "    d1_mean = float(np.mean(d1))\n",
    "    d1_std = float(np.std(d1, ddof=0))\n",
    "    d1_pos_rate = float(np.mean((d1 > 0).astype(np.float32)))\n",
    "    d1_abs_mean = float(np.mean(np.abs(d1)))\n",
    "\n",
    "    # tail stats\n",
    "    tail_k = max(5, int(0.05 * T))  # 마지막 5% 또는 최소 5개\n",
    "    tail = v[-tail_k:]\n",
    "    tail_mean = float(np.mean(tail))\n",
    "    tail_std = float(np.std(tail, ddof=0))\n",
    "\n",
    "    # baseline-relative\n",
    "    v_rel_end = float(v_end - v0)\n",
    "    tail_rel_mean = float(tail_mean - v0)\n",
    "\n",
    "    # damage proxies (누적량)\n",
    "    cum_pos = float(np.sum(np.maximum(d1, 0.0)))\n",
    "    cum_abs = float(np.sum(np.abs(d1)))\n",
    "    cum_inc = float(np.sum(np.maximum(v - v0, 0.0)))\n",
    "    cum_acc = float(np.sum(np.abs(dd1)))\n",
    "\n",
    "    # \"속도\" 개념을 위해 길이로 정규화한 버전도 추가\n",
    "    denom = float(max(T - 1, 1))\n",
    "    cum_pos_rate = cum_pos / denom\n",
    "    cum_abs_rate = cum_abs / denom\n",
    "    cum_inc_rate = cum_inc / denom\n",
    "    cum_acc_rate = cum_acc / denom\n",
    "\n",
    "    return {\n",
    "        \"T\": float(T),\n",
    "        \"v0\": v0,\n",
    "        \"v_end\": v_end,\n",
    "        \"v_mean\": v_mean,\n",
    "        \"v_std\": v_std,\n",
    "\n",
    "        \"tail_k\": float(tail_k),\n",
    "        \"tail_mean\": tail_mean,\n",
    "        \"tail_std\": tail_std,\n",
    "        \"v_rel_end\": v_rel_end,\n",
    "        \"tail_rel_mean\": tail_rel_mean,\n",
    "\n",
    "        \"d1_mean\": d1_mean,\n",
    "        \"d1_std\": d1_std,\n",
    "        \"d1_abs_mean\": d1_abs_mean,\n",
    "        \"d1_pos_rate\": d1_pos_rate,\n",
    "\n",
    "        \"cum_pos\": cum_pos,\n",
    "        \"cum_abs\": cum_abs,\n",
    "        \"cum_inc\": cum_inc,\n",
    "        \"cum_acc\": cum_acc,\n",
    "\n",
    "        \"cum_pos_rate\": cum_pos_rate,\n",
    "        \"cum_abs_rate\": cum_abs_rate,\n",
    "        \"cum_inc_rate\": cum_inc_rate,\n",
    "        \"cum_acc_rate\": cum_acc_rate,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helpers: per-file error (from windows predictions csv)\n",
    "# ============================================================\n",
    "def compute_per_file_rmse_from_windows(windows_csv: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    windows_csv columns expected:\n",
    "      file, cycle, RUL_true, RUL_pred, ...\n",
    "    Returns per-file RMSE/MAE + count.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(windows_csv)\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # file-level error computed on window-level rows (간단/일관)\n",
    "    df[\"err\"] = df[\"RUL_pred\"] - df[\"RUL_true\"]\n",
    "    g = df.groupby(\"file\", as_index=False).agg(\n",
    "        n=(\"err\", \"count\"),\n",
    "        mae=(\"err\", lambda x: float(np.mean(np.abs(x)))),\n",
    "        rmse=(\"err\", lambda x: float(np.sqrt(np.mean(np.square(x))))),\n",
    "        bias=(\"err\", \"mean\"),\n",
    "    )\n",
    "    return g\n",
    "\n",
    "\n",
    "def pick_high_error_files(per_file_df: pd.DataFrame, q: float, topk: Optional[int] = None) -> List[str]:\n",
    "    if per_file_df.empty:\n",
    "        return []\n",
    "    d = per_file_df.sort_values(\"rmse\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    if topk is not None and int(topk) > 0:\n",
    "        return d.head(int(topk))[\"file\"].tolist()\n",
    "\n",
    "    thr = float(d[\"rmse\"].quantile(q))\n",
    "    return d[d[\"rmse\"] >= thr][\"file\"].tolist()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Main\n",
    "# ============================================================\n",
    "def run_rootcause(cfg: Cfg) -> None:\n",
    "    out_dir = Path(cfg.out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    base = Path(cfg.trial_dir) / f\"seed_{cfg.seed}\"\n",
    "\n",
    "    # 1) 모든 (ckpt, split) 조합에서 per-file RMSE 계산 + high-error 선정\n",
    "    hits: Dict[str, int] = {}\n",
    "    records = []\n",
    "\n",
    "    for ckpt in cfg.ckpts:\n",
    "        for split in cfg.splits:\n",
    "            win_csv = base / ckpt / f\"{split}_predictions_windows.csv\"\n",
    "            if not win_csv.exists():\n",
    "                print(f\"[WARN] missing: {win_csv}\")\n",
    "                continue\n",
    "\n",
    "            per_file = compute_per_file_rmse_from_windows(win_csv)\n",
    "            per_file[\"ckpt\"] = ckpt\n",
    "            per_file[\"split\"] = split\n",
    "            records.append(per_file)\n",
    "\n",
    "            high_files = pick_high_error_files(per_file, q=cfg.high_quantile, topk=cfg.topk)\n",
    "            for f in high_files:\n",
    "                hits[f] = hits.get(f, 0) + 1\n",
    "\n",
    "    if len(records) == 0:\n",
    "        raise FileNotFoundError(\"No *_predictions_windows.csv found. Check cfg.trial_dir/seed/ckpt/split paths.\")\n",
    "\n",
    "    df_err_all = pd.concat(records, axis=0, ignore_index=True)\n",
    "    df_err_all.to_csv(out_dir / \"00_per_file_error_all_ckpt_split.csv\", index=False)\n",
    "\n",
    "    # 2) persistent high-error 파일 선정\n",
    "    rows_hits = [{\"file\": f, \"high_hits\": c} for f, c in sorted(hits.items(), key=lambda x: -x[1])]\n",
    "    df_hits = pd.DataFrame(rows_hits)\n",
    "    df_hits.to_csv(out_dir / \"01_high_error_hit_counts.csv\", index=False)\n",
    "\n",
    "    persistent = df_hits[df_hits[\"high_hits\"] >= int(cfg.persistent_min_hits)][\"file\"].tolist()\n",
    "    pd.DataFrame({\"file\": persistent}).to_csv(out_dir / \"02_persistent_high_error_files.csv\", index=False)\n",
    "\n",
    "    print(f\"[INFO] persistent files: {len(persistent)} (min_hits={cfg.persistent_min_hits})\")\n",
    "\n",
    "    # 3) 원본 min_vce 기반 file-level feature 추출 (전체 파일 + persistent 표시)\n",
    "    raw_dir = Path(cfg.data_dir)\n",
    "    raw_files = sorted([p for p in raw_dir.glob(\"*.csv\") if p.is_file()])\n",
    "\n",
    "    feats_rows = []\n",
    "    for fp in raw_files:\n",
    "        v = read_raw_minvce(fp)\n",
    "        feats = extract_file_level_features(v)\n",
    "        if not feats:\n",
    "            continue\n",
    "        feats[\"file\"] = fp.name\n",
    "        feats[\"is_persistent_high\"] = int(fp.name in set(persistent))\n",
    "        feats_rows.append(feats)\n",
    "\n",
    "    df_feat = pd.DataFrame(feats_rows)\n",
    "    df_feat.to_csv(out_dir / \"03_file_level_features_damage_baseline_tail.csv\", index=False)\n",
    "\n",
    "    # 4) clustering (KMeans) on selected axes\n",
    "    #    - baseline(v0), delta(d1_*), tail(tail_*), damage(cum_*)\n",
    "    feature_cols = [\n",
    "        \"v0\",\n",
    "        \"tail_rel_mean\", \"v_rel_end\",\n",
    "        \"d1_mean\", \"d1_std\", \"d1_abs_mean\", \"d1_pos_rate\",\n",
    "        \"cum_pos_rate\", \"cum_abs_rate\", \"cum_inc_rate\", \"cum_acc_rate\",\n",
    "    ]\n",
    "    df_use = df_feat.dropna(subset=feature_cols).copy()\n",
    "    X = df_use[feature_cols].to_numpy(dtype=np.float32)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X)\n",
    "\n",
    "    km = KMeans(n_clusters=int(cfg.n_clusters), random_state=0, n_init=10)\n",
    "    labels = km.fit_predict(Xs)\n",
    "\n",
    "    df_use[\"cluster\"] = labels.astype(int)\n",
    "    df_use.to_csv(out_dir / \"04_features_with_clusters.csv\", index=False)\n",
    "\n",
    "    # 5) cluster summary: persistent 비율/에러 평균 (에러는 df_err_all과 merge)\n",
    "    #   - 대표로 test/best_by_val_norm 기준 RMSE도 붙여줌(있으면)\n",
    "    df_err_ref = df_err_all[(df_err_all[\"ckpt\"] == cfg.ckpts[0]) & (df_err_all[\"split\"] == \"test\")][\n",
    "        [\"file\", \"rmse\", \"mae\", \"bias\", \"n\"]\n",
    "    ].rename(columns={\"rmse\": \"rmse_ref\", \"mae\": \"mae_ref\", \"bias\": \"bias_ref\", \"n\": \"n_ref\"})\n",
    "\n",
    "    df_join = df_use.merge(df_err_ref, on=\"file\", how=\"left\")\n",
    "    df_join.to_csv(out_dir / \"05_clustered_with_ref_error.csv\", index=False)\n",
    "\n",
    "    summ = df_join.groupby(\"cluster\", as_index=False).agg(\n",
    "        n_files=(\"file\", \"count\"),\n",
    "        persistent_rate=(\"is_persistent_high\", \"mean\"),\n",
    "        rmse_ref_mean=(\"rmse_ref\", \"mean\"),\n",
    "        rmse_ref_std=(\"rmse_ref\", \"std\"),\n",
    "        v0_mean=(\"v0\", \"mean\"),\n",
    "        tail_rel_mean_mean=(\"tail_rel_mean\", \"mean\"),\n",
    "        cum_abs_rate_mean=(\"cum_abs_rate\", \"mean\"),\n",
    "        cum_inc_rate_mean=(\"cum_inc_rate\", \"mean\"),\n",
    "        d1_std_mean=(\"d1_std\", \"mean\"),\n",
    "    )\n",
    "    summ.to_csv(out_dir / \"06_cluster_summary.csv\", index=False)\n",
    "\n",
    "    # 6) PCA 2D plot\n",
    "    pca = PCA(n_components=2, random_state=0)\n",
    "    Z = pca.fit_transform(Xs)\n",
    "    df_join[\"pca1\"] = Z[:, 0]\n",
    "    df_join[\"pca2\"] = Z[:, 1]\n",
    "    df_join.to_csv(out_dir / \"07_clustered_with_pca.csv\", index=False)\n",
    "\n",
    "    plt.figure()\n",
    "    for c in sorted(df_join[\"cluster\"].unique()):\n",
    "        sub = df_join[df_join[\"cluster\"] == c]\n",
    "        plt.scatter(sub[\"pca1\"], sub[\"pca2\"], s=20, label=f\"cluster {c}\", alpha=0.7)\n",
    "    # persistent 표시(테두리 강조)\n",
    "    subp = df_join[df_join[\"is_persistent_high\"] == 1]\n",
    "    if len(subp) > 0:\n",
    "        plt.scatter(subp[\"pca1\"], subp[\"pca2\"], s=80, facecolors=\"none\", edgecolors=\"k\", linewidths=1.5, label=\"persistent high\")\n",
    "\n",
    "    plt.xlabel(\"PCA-1\")\n",
    "    plt.ylabel(\"PCA-2\")\n",
    "    plt.title(\"File clusters on baseline/Δv/tail/damage-proxy axes\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_dir / \"08_pca_clusters.png\", dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 7) persistent 파일만 따로: 어떤 cluster에 몰리는지 + 상위 특징\n",
    "    df_p = df_join[df_join[\"is_persistent_high\"] == 1].copy()\n",
    "    df_p.to_csv(out_dir / \"09_persistent_only_with_features.csv\", index=False)\n",
    "\n",
    "    if len(df_p) > 0:\n",
    "        dist = df_p[\"cluster\"].value_counts().sort_index()\n",
    "        dist.to_csv(out_dir / \"10_persistent_cluster_distribution.csv\", header=[\"count\"])\n",
    "\n",
    "        # persistent vs others mean shift (표준화된 공간에서)\n",
    "        Xs_all = pd.DataFrame(Xs, columns=[f\"z_{c}\" for c in feature_cols])\n",
    "        Xs_all[\"is_persistent_high\"] = df_use[\"is_persistent_high\"].values\n",
    "        mu_p = Xs_all[Xs_all[\"is_persistent_high\"] == 1].mean(numeric_only=True)\n",
    "        mu_o = Xs_all[Xs_all[\"is_persistent_high\"] == 0].mean(numeric_only=True)\n",
    "        shift = (mu_p - mu_o).sort_values(ascending=False)\n",
    "        shift.to_csv(out_dir / \"11_persistent_minus_others_mean_shift_z.csv\", header=[\"mean_shift_z\"])\n",
    "\n",
    "    print(\"\\n[SAVED]\")\n",
    "    print(\" -\", out_dir / \"02_persistent_high_error_files.csv\")\n",
    "    print(\" -\", out_dir / \"03_file_level_features_damage_baseline_tail.csv\")\n",
    "    print(\" -\", out_dir / \"06_cluster_summary.csv\")\n",
    "    print(\" -\", out_dir / \"08_pca_clusters.png\")\n",
    "    print(\" -\", out_dir / \"11_persistent_minus_others_mean_shift_z.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = Cfg(\n",
    "        data_dir=r\"C:\\Users\\11HOME_AHCI\\Desktop\\PoF\\Winter 2026\\Ref Data4\\100\",\n",
    "        trial_dir=r\"./Trial15\",\n",
    "        seed=333,\n",
    "        ckpts=(\"best_by_val_norm\", \"last_epoch\"),\n",
    "        splits=(\"train\", \"val\", \"test\"),\n",
    "        high_quantile=0.80,\n",
    "        topk=None,\n",
    "        persistent_min_hits=3,\n",
    "        n_clusters=4,\n",
    "        out_dir=r\"./Trial15_rootcause_analysis\",\n",
    "    )\n",
    "    run_rootcause(cfg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igbt_rnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
